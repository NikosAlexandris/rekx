{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"rekx \ud83e\udd96","text":"<p><sup>1</sup> </p> <p>Experimental</p> <p>Everything is under heavy development and subject to change! Interested ? Peek over at the To Do list !</p>","tags":["NetCDF","Time Series","Kerchunk","Python"]},{"location":"#what","title":"What ?","text":"<p><code>rekx</code> seamlessly interfaces the Kerchunk library <sup>2</sup> in an interactive way through the command line. It assists in creating virtual aggregate datasets, also known as Kerchunk reference sets, which allows for an efficient, parallel and cloud-friendly way to access data in-situ without duplicating the original datasets.</p> <p>More than a functional tool, <code>rekx</code> serves an educational purpose on matters around chunking, compression and efficient data reading from common scientific file formats such as NetCDF used extensively to store large time-series. While there is abundant documentation on such topics, it is often highly technical and oriented towards developers, <code>rekx</code> tries to simplify these concepts through practical examples.</p>","tags":["NetCDF","Time Series","Kerchunk","Python"]},{"location":"#why","title":"Why ?","text":"<p>Similarly, existing tools for managing HDF and NetCDF data, such as <code>cdo</code>, <code>nco</code>, and others, often have overlapping functionalities and present a steep learning curve for non-experts. <code>rekx</code> focuses on practical aspects of efficient data access trying to simplify these processes.</p> <p>It features simple command line tools to:</p> <ul> <li>diagnose data structures</li> <li>validate uniform chunking across files</li> <li>suggest good chunking shapes</li> <li>parameterise the rechunking of datasets.</li> <li>create and aggregate Kerchunk reference sets</li> <li>time data read operations for performance analysis</li> </ul> <p><code>rekx</code> dedicates to practicality, simplicity, and essence.</p> <p>The Zen of Chunking</p> <p>Chunks are equal-sized data blocks.</p> <p>Chunks are required for compression, extendible data and subsetting large datasets.</p> <p>Small chunks lead to a large number of chunks.</p> <p>Large chunks lead to a small number of chunks.</p> <p>Appropriately sized chunks can improve performance.</p> <p>Unthoughtfully sized chunks will decrease performance.</p> <p>Good chunk sizes depend on data access patterns.</p> <p>Good chunk sizes balance read/write operations and computational efficiency.</p> <p>There is no one size fits all.</p> <ol> <li> <p>Original T-Rex drawn by pikisuperstar on Freepik\u00a0\u21a9</p> </li> <li> <p>Martin Durant, Max Jones, Ryan Abernathey, David Hoese, and James Bednar. Pangeo-ML Augmentation - Enabling Cloud-native access to archival data with Kerchunk. 3 2023. URL: https://figshare.com/articles/preprint/Pangeo-ML_Augmentation_-_Enabling_Cloud-native_access_to_archival_data_with_Kerchunk/22266433, doi:10.6084/m9.figshare.22266433.v1.\u00a0\u21a9</p> </li> </ol>","tags":["NetCDF","Time Series","Kerchunk","Python"]},{"location":"license/","title":"European Union Public Licence v. 1.2","text":"","tags":["License","EUPL-1.2"]},{"location":"license/#keywords","title":"Keywords","text":"<ul> <li> <p>Open Source (or free software) -- providing source code, with permission to use, modify and redistribute it</p> </li> <li> <p>Reciprocal (or copyleft) \u2013- distributed derivatives must remain shared under the EUPL</p> </li> <li> <p>Compatible -- when merged with another work covered by GPL-2.0, GPL-3.0, LGPL, AGPL, CeCILL, OSL, EPL or MPL, the combined work can be distributed under these licences;</p> </li> <li> <p>Interoperable -- interfaces, APIs and data structures may be freely copied in order to link with other components;</p> </li> <li> <p>Multilingual -- the sole licence with 23 original versions with the same legal value;</p> </li> <li> <p>Complete -- covers the use of patents, various \u2018works\u2019 and distribution methods including \u2018services\u2019;</p> </li> <li> <p>Compliant -- with EU and Member State laws</p> </li> </ul> <p>Source :  European Commission, Directorate-General for Digital Services, Schmitz, P., European Union Public Licence (EUPL) \u2013 Guidelines July 2021, Publications Office, 2021, https://data.europa.eu/doi/10.2799/77160</p>","tags":["License","EUPL-1.2"]},{"location":"license/#full-text","title":"Full text","text":"LICENSE.txt<pre><code>                      EUROPEAN UNION PUBLIC LICENCE v. 1.2\n                      EUPL \u00a9 the European Union 2007, 2016\n\nThis European Union Public Licence (the \u2018EUPL\u2019) applies to the Work (as defined\nbelow) which is provided under the terms of this Licence. Any use of the Work,\nother than as authorised under this Licence is prohibited (to the extent such\nuse is covered by a right of the copyright holder of the Work).\n\nThe Work is provided under the terms of this Licence when the Licensor (as\ndefined below) has placed the following notice immediately following the\ncopyright notice for the Work:\n\n        Licensed under the EUPL\n\nor has expressed by any other means his willingness to license under the EUPL.\n\n1. Definitions\n\nIn this Licence, the following terms have the following meaning:\n\n- \u2018The Licence\u2019: this Licence.\n\n- \u2018The Original Work\u2019: the work or software distributed or communicated by the\n  Licensor under this Licence, available as Source Code and also as Executable\n  Code as the case may be.\n\n- \u2018Derivative Works\u2019: the works or software that could be created by the\n  Licensee, based upon the Original Work or modifications thereof. This Licence\n  does not define the extent of modification or dependence on the Original Work\n  required in order to classify a work as a Derivative Work; this extent is\n  determined by copyright law applicable in the country mentioned in Article 15.\n\n- \u2018The Work\u2019: the Original Work or its Derivative Works.\n\n- \u2018The Source Code\u2019: the human-readable form of the Work which is the most\n  convenient for people to study and modify.\n\n- \u2018The Executable Code\u2019: any code which has generally been compiled and which is\n  meant to be interpreted by a computer as a program.\n\n- \u2018The Licensor\u2019: the natural or legal person that distributes or communicates\n  the Work under the Licence.\n\n- \u2018Contributor(s)\u2019: any natural or legal person who modifies the Work under the\n  Licence, or otherwise contributes to the creation of a Derivative Work.\n\n- \u2018The Licensee\u2019 or \u2018You\u2019: any natural or legal person who makes any usage of\n  the Work under the terms of the Licence.\n\n- \u2018Distribution\u2019 or \u2018Communication\u2019: any act of selling, giving, lending,\n  renting, distributing, communicating, transmitting, or otherwise making\n  available, online or offline, copies of the Work or providing access to its\n  essential functionalities at the disposal of any other natural or legal\n  person.\n\n2. Scope of the rights granted by the Licence\n\nThe Licensor hereby grants You a worldwide, royalty-free, non-exclusive,\nsublicensable licence to do the following, for the duration of copyright vested\nin the Original Work:\n\n- use the Work in any circumstance and for all usage,\n- reproduce the Work,\n- modify the Work, and make Derivative Works based upon the Work,\n- communicate to the public, including the right to make available or display\n  the Work or copies thereof to the public and perform publicly, as the case may\n  be, the Work,\n- distribute the Work or copies thereof,\n- lend and rent the Work or copies thereof,\n- sublicense rights in the Work or copies thereof.\n\nThose rights can be exercised on any media, supports and formats, whether now\nknown or later invented, as far as the applicable law permits so.\n\nIn the countries where moral rights apply, the Licensor waives his right to\nexercise his moral right to the extent allowed by law in order to make effective\nthe licence of the economic rights here above listed.\n\nThe Licensor grants to the Licensee royalty-free, non-exclusive usage rights to\nany patents held by the Licensor, to the extent necessary to make use of the\nrights granted on the Work under this Licence.\n\n3. Communication of the Source Code\n\nThe Licensor may provide the Work either in its Source Code form, or as\nExecutable Code. If the Work is provided as Executable Code, the Licensor\nprovides in addition a machine-readable copy of the Source Code of the Work\nalong with each copy of the Work that the Licensor distributes or indicates, in\na notice following the copyright notice attached to the Work, a repository where\nthe Source Code is easily and freely accessible for as long as the Licensor\ncontinues to distribute or communicate the Work.\n\n4. Limitations on copyright\n\nNothing in this Licence is intended to deprive the Licensee of the benefits from\nany exception or limitation to the exclusive rights of the rights owners in the\nWork, of the exhaustion of those rights or of other applicable limitations\nthereto.\n\n5. Obligations of the Licensee\n\nThe grant of the rights mentioned above is subject to some restrictions and\nobligations imposed on the Licensee. Those obligations are the following:\n\nAttribution right: The Licensee shall keep intact all copyright, patent or\ntrademarks notices and all notices that refer to the Licence and to the\ndisclaimer of warranties. The Licensee must include a copy of such notices and a\ncopy of the Licence with every copy of the Work he/she distributes or\ncommunicates. The Licensee must cause any Derivative Work to carry prominent\nnotices stating that the Work has been modified and the date of modification.\n\nCopyleft clause: If the Licensee distributes or communicates copies of the\nOriginal Works or Derivative Works, this Distribution or Communication will be\ndone under the terms of this Licence or of a later version of this Licence\nunless the Original Work is expressly distributed only under this version of the\nLicence \u2014 for example by communicating \u2018EUPL v. 1.2 only\u2019. The Licensee\n(becoming Licensor) cannot offer or impose any additional terms or conditions on\nthe Work or Derivative Work that alter or restrict the terms of the Licence.\n\nCompatibility clause: If the Licensee Distributes or Communicates Derivative\nWorks or copies thereof based upon both the Work and another work licensed under\na Compatible Licence, this Distribution or Communication can be done under the\nterms of this Compatible Licence. For the sake of this clause, \u2018Compatible\nLicence\u2019 refers to the licences listed in the appendix attached to this Licence.\nShould the Licensee's obligations under the Compatible Licence conflict with\nhis/her obligations under this Licence, the obligations of the Compatible\nLicence shall prevail.\n\nProvision of Source Code: When distributing or communicating copies of the Work,\nthe Licensee will provide a machine-readable copy of the Source Code or indicate\na repository where this Source will be easily and freely available for as long\nas the Licensee continues to distribute or communicate the Work.\n\nLegal Protection: This Licence does not grant permission to use the trade names,\ntrademarks, service marks, or names of the Licensor, except as required for\nreasonable and customary use in describing the origin of the Work and\nreproducing the content of the copyright notice.\n\n6. Chain of Authorship\n\nThe original Licensor warrants that the copyright in the Original Work granted\nhereunder is owned by him/her or licensed to him/her and that he/she has the\npower and authority to grant the Licence.\n\nEach Contributor warrants that the copyright in the modifications he/she brings\nto the Work are owned by him/her or licensed to him/her and that he/she has the\npower and authority to grant the Licence.\n\nEach time You accept the Licence, the original Licensor and subsequent\nContributors grant You a licence to their contributions to the Work, under the\nterms of this Licence.\n\n7. Disclaimer of Warranty\n\nThe Work is a work in progress, which is continuously improved by numerous\nContributors. It is not a finished work and may therefore contain defects or\n\u2018bugs\u2019 inherent to this type of development.\n\nFor the above reason, the Work is provided under the Licence on an \u2018as is\u2019 basis\nand without warranties of any kind concerning the Work, including without\nlimitation merchantability, fitness for a particular purpose, absence of defects\nor errors, accuracy, non-infringement of intellectual property rights other than\ncopyright as stated in Article 6 of this Licence.\n\nThis disclaimer of warranty is an essential part of the Licence and a condition\nfor the grant of any rights to the Work.\n\n8. Disclaimer of Liability\n\nExcept in the cases of wilful misconduct or damages directly caused to natural\npersons, the Licensor will in no event be liable for any direct or indirect,\nmaterial or moral, damages of any kind, arising out of the Licence or of the use\nof the Work, including without limitation, damages for loss of goodwill, work\nstoppage, computer failure or malfunction, loss of data or any commercial\ndamage, even if the Licensor has been advised of the possibility of such damage.\nHowever, the Licensor will be liable under statutory product liability laws as\nfar such laws apply to the Work.\n\n9. Additional agreements\n\nWhile distributing the Work, You may choose to conclude an additional agreement,\ndefining obligations or services consistent with this Licence. However, if\naccepting obligations, You may act only on your own behalf and on your sole\nresponsibility, not on behalf of the original Licensor or any other Contributor,\nand only if You agree to indemnify, defend, and hold each Contributor harmless\nfor any liability incurred by, or claims asserted against such Contributor by\nthe fact You have accepted any warranty or additional liability.\n\n10. Acceptance of the Licence\n\nThe provisions of this Licence can be accepted by clicking on an icon \u2018I agree\u2019\nplaced under the bottom of a window displaying the text of this Licence or by\naffirming consent in any other similar way, in accordance with the rules of\napplicable law. Clicking on that icon indicates your clear and irrevocable\nacceptance of this Licence and all of its terms and conditions.\n\nSimilarly, you irrevocably accept this Licence and all of its terms and\nconditions by exercising any rights granted to You by Article 2 of this Licence,\nsuch as the use of the Work, the creation by You of a Derivative Work or the\nDistribution or Communication by You of the Work or copies thereof.\n\n11. Information to the public\n\nIn case of any Distribution or Communication of the Work by means of electronic\ncommunication by You (for example, by offering to download the Work from a\nremote location) the distribution channel or media (for example, a website) must\nat least provide to the public the information requested by the applicable law\nregarding the Licensor, the Licence and the way it may be accessible, concluded,\nstored and reproduced by the Licensee.\n\n12. Termination of the Licence\n\nThe Licence and the rights granted hereunder will terminate automatically upon\nany breach by the Licensee of the terms of the Licence.\n\nSuch a termination will not terminate the licences of any person who has\nreceived the Work from the Licensee under the Licence, provided such persons\nremain in full compliance with the Licence.\n\n13. Miscellaneous\n\nWithout prejudice of Article 9 above, the Licence represents the complete\nagreement between the Parties as to the Work.\n\nIf any provision of the Licence is invalid or unenforceable under applicable\nlaw, this will not affect the validity or enforceability of the Licence as a\nwhole. Such provision will be construed or reformed so as necessary to make it\nvalid and enforceable.\n\nThe European Commission may publish other linguistic versions or new versions of\nthis Licence or updated versions of the Appendix, so far this is required and\nreasonable, without reducing the scope of the rights granted by the Licence. New\nversions of the Licence will be published with a unique version number.\n\nAll linguistic versions of this Licence, approved by the European Commission,\nhave identical value. Parties can take advantage of the linguistic version of\ntheir choice.\n\n14. Jurisdiction\n\nWithout prejudice to specific agreement between parties,\n\n- any litigation resulting from the interpretation of this License, arising\n  between the European Union institutions, bodies, offices or agencies, as a\n  Licensor, and any Licensee, will be subject to the jurisdiction of the Court\n  of Justice of the European Union, as laid down in article 272 of the Treaty on\n  the Functioning of the European Union,\n\n- any litigation arising between other parties and resulting from the\n  interpretation of this License, will be subject to the exclusive jurisdiction\n  of the competent court where the Licensor resides or conducts its primary\n  business.\n\n15. Applicable Law\n\nWithout prejudice to specific agreement between parties,\n\n- this Licence shall be governed by the law of the European Union Member State\n  where the Licensor has his seat, resides or has his registered office,\n\n- this licence shall be governed by Belgian law if the Licensor has no seat,\n  residence or registered office inside a European Union Member State.\n\nAppendix\n\n\u2018Compatible Licences\u2019 according to Article 5 EUPL are:\n\n- GNU General Public License (GPL) v. 2, v. 3\n- GNU Affero General Public License (AGPL) v. 3\n- Open Software License (OSL) v. 2.1, v. 3.0\n- Eclipse Public License (EPL) v. 1.0\n- CeCILL v. 2.0, v. 2.1\n- Mozilla Public Licence (MPL) v. 2\n- GNU Lesser General Public Licence (LGPL) v. 2.1, v. 3\n- Creative Commons Attribution-ShareAlike v. 3.0 Unported (CC BY-SA 3.0) for\n  works other than software\n- European Union Public Licence (EUPL) v. 1.1, v. 1.2\n- Qu\u00e9bec Free and Open-Source Licence \u2014 Reciprocity (LiLiQ-R) or Strong\n  Reciprocity (LiLiQ-R+).\n\nThe European Commission may update this Appendix to later versions of the above\nlicences without producing a new version of the EUPL, as long as they provide\nthe rights granted in Article 2 of this Licence and protect the covered Source\nCode from exclusive appropriation.\n\nAll other changes or additions to this Appendix require the production of a new\nEUPL version.\n</code></pre>","tags":["License","EUPL-1.2"]},{"location":"to_do/","title":"To Do","text":"<p>Warning</p> <p>While functional, <code>rekx</code> needs some  to be able to properly crunch more data chunks!</p> <ul> <li> Complete backend for rechunking, support for <ul> <li> NetCDF4</li> <li> Xarray</li> <li> <code>nccopy</code></li> </ul> </li> <li> Simplify command line interface<ul> <li> merge \"multi\" commands to single/simple ones ?</li> <li> make <code>common-shape</code> and <code>validate</code> options to <code>shapes</code> ?</li> <li> clean non-sense <code>suggest-alternative</code> command or merge to <code>suggest</code></li> <li> merge <code>reference-parquet</code> to <code>reference</code></li> <li> as above, same for/with <code>combine</code> commands</li> <li> does a sepatate <code>select-fast</code> make sense ?</li> <li> review various select/read commands</li> </ul> </li> <li> Go through :<ul> <li> https://peps.python.org/pep-0314/</li> <li> ?</li> </ul> </li> <li> Write clean and meaningful docstrings for each and every function</li> <li> Pytest each and every (?) function</li> <li> Packaging</li> <li> Documentation<ul> <li> Use https://squidfunk.github.io/mkdocs-material/</li> <li> Simple examples<ul> <li> Diagnose</li> <li> Suggest</li> <li> Rechunk</li> <li> Kerchunk<ul> <li> JSON<ul> <li> Create references</li> <li> Combine references</li> <li> Read data from aggregated reference and load in memory</li> </ul> </li> <li> Parquet<ul> <li> Create references</li> <li> Combine references</li> <li> Read data from aggregated reference and load in memory</li> <li> Pending issue https://github.com/fsspec/kerchunk/issues/345#issuecomment-1807349725</li> </ul> </li> </ul> </li> <li> Select (aka read)<ul> <li> From Xarray-supported datasets</li> <li> From Kerchunk references</li> </ul> </li> </ul> </li> <li> Tutorial<ul> <li> Rechunking and Kerchunking SARAH3 products</li> </ul> </li> <li> Add visuals to Concepts</li> </ul> </li> </ul>","tags":["Development","Programing","Design","Documentation","CLI","Ideas"]},{"location":"cli/","title":"Index","text":"<p>Bug</p> <p>Under Heavy Development ! Review Me !</p>","tags":["rekx","CLI","Reference","Tools","CLI","Source Code"]},{"location":"cli/#tools","title":"Tools","text":"<p><code>rekx</code> strives to follow a logical sequence to support crunching chunked datasets, i.e. to :</p> <ol> <li> <p>diagnose data structures and chunking shapes</p> </li> <li> <p>suggest eventually good chunking shapes</p> </li> <li> <p>rechunk data</p> </li> <li> <p>create Kerchunk reference strives</p> </li> <li> <p>select data from normal datasets or Kerchunk reference sets</p> </li> <li> <p>measure the time required to read data </p> </li> </ol>","tags":["rekx","CLI","Reference","Tools","CLI","Source Code"]},{"location":"cli/#command-line-interface","title":"Command Line Interface","text":"<p>Documentation for all command line tools.</p>","tags":["rekx","CLI","Reference","Tools","CLI","Source Code"]},{"location":"cli/#logging","title":"Logging","text":"<p>Logging and debugging</p>","tags":["rekx","CLI","Reference","Tools","CLI","Source Code"]},{"location":"cli/inspect/","title":"Inspect","text":"","tags":["rekx","Reference","CLI","Tools","inspect"]},{"location":"cli/inspect/#rekx.inspect","title":"inspect","text":"<p>Functions:</p> Name Description <code>inspect_netcdf_data</code> <p>Collect the metadata of a single or multiple NetCDF files.</p>","tags":["rekx","Reference","CLI","Tools","inspect"]},{"location":"cli/inspect/#rekx.inspect.inspect_netcdf_data","title":"inspect_netcdf_data","text":"<pre><code>inspect_netcdf_data(\n    input_path: Path = \".\",\n    pattern: str = \"*.nc\",\n    variable: str = None,\n    variable_set: XarrayVariableSet = all,\n    long_table: Optional[bool] = True,\n    group_metadata: Optional[bool] = False,\n    longitude: float = 8,\n    latitude: float = 45,\n    repetitions: int = REPETITIONS_DEFAULT,\n    humanize: bool = False,\n    csv: Path = None,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n) -&gt; None\n</code></pre> <p>Collect the metadata of a single or multiple NetCDF files.</p> <p>Scan the <code>source_directory</code> for files that match the given <code>pattern</code>, and collect their metadata, including : file name, file size, dimensions, shape, chunks, cache, type, scale, offset, compression, shuffling and lastly measure the time required to retrieve and load data variables (only) in memory.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Path</code> <p>A singe path or a list of paths to the input NetCDF data</p> <code>'.'</code> <code>variable</code> <code>str</code> <p>Name of the variable to query</p> <code>None</code> <code>variable_set</code> <code>XarrayVariableSet</code> <p>Name of the set of variables to query. See also docstring of XarrayVariableSet.</p> <code>all</code> <code>longitude</code> <code>float</code> <p>The longitude of the location to read data</p> <code>8</code> <code>latitude</code> <code>float</code> <p>The latitude of the location to read data</p> <code>45</code> <code>group_metadata</code> <code>Optional[bool]</code> <p>Visually group metadata records per input file by using empty lines in-between</p> <code>False</code> <code>repetitions</code> <code>int</code> <p>Number of repetitions for read operation</p> <code>REPETITIONS_DEFAULT</code> <code>humanize</code> <code>bool</code> <p>Humanize measured quantities of bytes</p> <code>False</code> <code>csv</code> <code>Path</code> <p>Output file name for comma-separated values</p> <code>None</code> <code>verbose</code> <code>int</code> <p>Verbosity level</p> <code>VERBOSE_LEVEL_DEFAULT</code> <p>Returns:</p> Type Description <code>None</code> <p>This function does not return anything. It either prints out the results in the terminal or writes then in a CSV file if requested.</p> Source code in <code>rekx/inspect.py</code> <pre><code>def inspect_netcdf_data(\n    input_path: Annotated[Path, typer_argument_source_path] = \".\",\n    pattern: Annotated[str, typer_option_filename_pattern] = \"*.nc\",\n    variable: str = None,\n    variable_set: Annotated[\n        XarrayVariableSet, typer.Option(help=\"Set of Xarray variables to diagnose\")\n    ] = XarrayVariableSet.all,\n    long_table: Annotated[\n        Optional[bool],\n        \"Group rows of metadata per input NetCDF file and variable in a long table\",\n    ] = True,\n    group_metadata: Annotated[\n        Optional[bool],\n        \"Visually cluster rows of metadata per input NetCDF file and variable\",\n    ] = False,\n    longitude: Annotated[float, typer_argument_longitude_in_degrees] = 8,\n    latitude: Annotated[float, typer_argument_latitude_in_degrees] = 45,\n    repetitions: Annotated[int, typer_option_repetitions] = REPETITIONS_DEFAULT,\n    humanize: Annotated[bool, typer_option_humanize] = False,\n    csv: Annotated[Path, typer_option_csv] = None,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n) -&gt; None:\n    \"\"\"Collect the metadata of a single or multiple NetCDF files.\n\n    Scan the `source_directory` for files that match the given `pattern`,\n    and collect their metadata, including : file name, file size, dimensions,\n    shape, chunks, cache, type, scale, offset, compression, shuffling and\n    lastly measure the time required to retrieve and load data variables (only)\n    in memory.\n\n    Parameters\n    ----------\n    input_path: Path\n        A singe path or a list of paths to the input NetCDF data\n    variable: str\n        Name of the variable to query\n    variable_set: XarrayVariableSet\n        Name of the set of variables to query. See also docstring of\n        XarrayVariableSet.\n    longitude: float\n        The longitude of the location to read data\n    latitude: float\n        The latitude of the location to read data\n    group_metadata: bool\n        Visually group metadata records per input file by using empty lines\n        in-between\n    repetitions: int\n        Number of repetitions for read operation\n    humanize: bool\n        Humanize measured quantities of bytes\n    csv: Path\n        Output file name for comma-separated values\n    verbose: int\n        Verbosity level\n\n    Returns\n    -------\n    None\n        This function does not return anything. It either prints out the\n        results in the terminal or writes then in a CSV file if requested.\n\n    \"\"\"\n    if input_path.is_file():\n        metadata, _ = get_netcdf_metadata(\n            input_netcdf_path=input_path,\n            variable=variable,\n            variable_set=variable_set,\n            longitude=longitude,\n            latitude=latitude,\n            repetitions=repetitions,\n            humanize=humanize,\n        )\n        if not csv:\n            from .print import print_metadata_table\n\n            print_metadata_table(metadata)\n        if csv:\n            write_metadata_dictionary_to_csv(\n                dictionary=metadata,\n                output_filename=csv,\n            )\n        return\n\n    if input_path.is_dir():\n        source_directory = Path(input_path)\n        if not any(source_directory.iterdir()):\n            print(f\"[red]The directory [code]{source_directory}[/code] is empty[/red].\")\n            return\n        file_paths = list(source_directory.glob(pattern))\n        if not file_paths:\n            print(\n                f\"No files matching the pattern [code]{pattern}[/code] found in [code]{source_directory}[/code]!\"\n            )\n            return\n        mode = DisplayMode(verbose)\n        with display_context[mode]:\n            try:\n                metadata_series = get_multiple_netcdf_metadata(\n                    file_paths=file_paths,\n                    variable_set=variable_set,\n                    longitude=longitude,\n                    latitude=latitude,\n                    repetitions=repetitions,\n                    humanize=humanize,\n                )\n            except TypeError as e:\n                raise ValueError(\"Error occurred:\", e)\n\n        if csv:\n            write_nested_dictionary_to_csv(\n                nested_dictionary=metadata_series,\n                output_filename=csv,\n            )\n            return\n\n        if not long_table:\n            from .print import print_metadata_series_table\n\n            print_metadata_series_table(\n                metadata_series=metadata_series,\n                group_metadata=group_metadata,\n            )\n        else:\n            from .print import print_metadata_series_long_table\n\n            print_metadata_series_long_table(\n                metadata_series=metadata_series,\n                group_metadata=group_metadata,\n            )\n</code></pre>","tags":["rekx","Reference","CLI","Tools","inspect"]},{"location":"cli/interface/","title":"Command Line Interface","text":"","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.cli","title":"cli","text":"<p>Rekx is a command line interface to Kerchunk</p>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.constants","title":"constants","text":"","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.hardcodings","title":"hardcodings","text":"<p>Hardcodings</p>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.typer_parameters","title":"typer_parameters","text":"<p>Classes:</p> Name Description <code>OrderCommands</code> <p>Functions:</p> Name Description <code>callback_source_directory</code> <code>callback_source_path</code> <code>callback_source_path_with_pattern</code>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.typer_parameters.OrderCommands","title":"OrderCommands","text":"<p>               Bases: <code>TyperGroup</code></p> <p>Methods:</p> Name Description <code>list_commands</code> <p>Return list of commands in the order they appear.</p>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.typer_parameters.OrderCommands.list_commands","title":"list_commands","text":"<pre><code>list_commands(ctx: Context)\n</code></pre> <p>Return list of commands in the order they appear.</p> See also <ul> <li>https://github.com/tiangolo/typer/issues/428#issuecomment-1238866548</li> </ul> Source code in <code>rekx/typer_parameters.py</code> <pre><code>def list_commands(self, ctx: typer.Context):\n    \"\"\"Return list of commands in the order they appear.\n\n    See also\n    --------\n    - https://github.com/tiangolo/typer/issues/428#issuecomment-1238866548\n\n    \"\"\"\n    return list(self.commands)\n</code></pre>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.typer_parameters.callback_source_directory","title":"callback_source_directory","text":"<pre><code>callback_source_directory(directory: Path)\n</code></pre> Source code in <code>rekx/typer_parameters.py</code> <pre><code>def callback_source_directory(directory: Path):\n    \"\"\" \"\"\"\n    if not directory.exists() or not any(directory.iterdir()):\n        print(\n            f\"[red]The directory [code]{directory}[/code] does not exist or is empty[/red].\"\n        )\n    return directory\n</code></pre>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.typer_parameters.callback_source_path","title":"callback_source_path","text":"<pre><code>callback_source_path(input_path: Path)\n</code></pre> Source code in <code>rekx/typer_parameters.py</code> <pre><code>def callback_source_path(input_path: Path):\n    \"\"\" \"\"\"\n    from rich import print\n\n    if not input_path.exists():\n        print(f\"[red]The path [code]{input_path}[/code] does not exist[/red].\")\n        raise typer.Exit()\n\n    if not input_path.is_file() and not input_path.is_dir():\n        print(f\"[red]The path [code]{input_path}[/code] is not valid[/red].\")\n        raise typer.Exit()\n\n    return input_path\n</code></pre>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.typer_parameters.callback_source_path_with_pattern","title":"callback_source_path_with_pattern","text":"<pre><code>callback_source_path_with_pattern(path: Path, ctx: Context)\n</code></pre> Source code in <code>rekx/typer_parameters.py</code> <pre><code>def callback_source_path_with_pattern(path: Path, ctx: typer.Context):\n    \"\"\" \"\"\"\n    if not path.exists():\n        raise typer.BadParameter(\"Path does not exist.\")\n\n    if not path.is_file() and not path.is_dir():\n        raise typer.BadParameter(\"Path is not a file.\")\n\n    # if not path.is_readable():\n    #     raise typer.BadParameter(\"File is not readable.\")\n\n    return path.resolve()\n</code></pre>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.rich_help_panel_names","title":"rich_help_panel_names","text":"","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.models","title":"models","text":"<p>Functions:</p> Name Description <code>get_file_format</code> <p>Get the format from the filename extension.</p> <code>select_netcdf_variable_set_from_dataset</code> <p>The same Enum model for both : netcdf4_variable_set and xarray_variable_set</p> <code>select_xarray_variable_set_from_dataset</code> <p>Select user-requested set of variables from an Xarray dataset.</p>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.models.get_file_format","title":"get_file_format","text":"<pre><code>get_file_format(file_path: Path) -&gt; FileFormat\n</code></pre> <p>Get the format from the filename extension.</p> Source code in <code>rekx/models.py</code> <pre><code>def get_file_format(file_path: Path) -&gt; FileFormat:\n    \"\"\"\n    Get the format from the filename extension.\n    \"\"\"\n    file_extension = file_path.suffix.lower()\n    return FileFormat(file_extension)\n</code></pre>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.models.select_netcdf_variable_set_from_dataset","title":"select_netcdf_variable_set_from_dataset","text":"<pre><code>select_netcdf_variable_set_from_dataset(\n    netcdf4_variable_set: Type[Enum],\n    variable_set: List[Enum],\n    dataset: Dataset,\n)\n</code></pre> <p>The same Enum model for both : netcdf4_variable_set and xarray_variable_set</p> Source code in <code>rekx/models.py</code> <pre><code>def select_netcdf_variable_set_from_dataset(\n    netcdf4_variable_set: Type[enum.Enum],\n    variable_set: List[enum.Enum],\n    dataset: netCDF4.Dataset,\n):\n    \"\"\"\n    The same Enum model for both : netcdf4_variable_set and xarray_variable_set\n    \"\"\"\n    metadata_attributes = {\"record_status\", \"bnds\"}\n    coordinates_data_attributes = {\"lat_bnds\", \"lon_bnds\"}\n    time_coordinate = {\"time\"}\n    dimensions_attributes = set(dataset.dimensions)  # no `coordinates` via netCDF4\n    variables_attributes = set(dataset.variables)\n    data_attributes = (\n        variables_attributes\n        - dimensions_attributes\n        - coordinates_data_attributes\n        - metadata_attributes\n    )\n\n    if variable_set == netcdf4_variable_set.all:\n        return variables_attributes\n\n    elif variable_set == netcdf4_variable_set.coordinates:\n        return dimensions_attributes  # Same as next one ?\n\n    elif variable_set == netcdf4_variable_set.coordinates_without_data:\n        return dimensions_attributes\n\n    elif variable_set == netcdf4_variable_set.data:\n        return data_attributes\n\n    elif variable_set == netcdf4_variable_set.metadata:\n        return metadata_attributes.intersection(variables_attributes)\n\n    elif variable_set == netcdf4_variable_set.time:\n        return time_coordinate\n\n    else:\n        raise ValueError(\"Invalid category\")\n</code></pre>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.models.select_xarray_variable_set_from_dataset","title":"select_xarray_variable_set_from_dataset","text":"<pre><code>select_xarray_variable_set_from_dataset(\n    xarray_variable_set: Type[Enum],\n    variable_set: List[Enum],\n    dataset: Dataset,\n)\n</code></pre> <p>Select user-requested set of variables from an Xarray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>xarray_variable_set</code> <code>Type[Enum]</code> <p>The Enum model to use for selection</p> required <code>variable_set</code> <code>List[Enum]</code> <p>The user-requested sets of variables to select based on the Enum model</p> required <code>dataset</code> <code>Dataset</code> <p>The input Xarray dataset from which to extract the user-requested variables</p> required <p>Returns:</p> Type Description <code>Examples</code> <code>--------</code> Notes <p>Is quasi-identical to the function select_netcdf_variable_set_from_dataset() with differences in terms of the names of attributes. See also docstring of other function.</p> Source code in <code>rekx/models.py</code> <pre><code>def select_xarray_variable_set_from_dataset(\n    xarray_variable_set: Type[enum.Enum],\n    variable_set: List[enum.Enum],\n    dataset: xr.Dataset,\n):\n    \"\"\"\n    Select user-requested set of variables from an Xarray dataset.\n\n    Parameters\n    ----------\n    xarray_variable_set: enum.Enum\n        The Enum model to use for selection\n\n    variable_set: List[enum.Enum]\n        The user-requested sets of variables to select based on the Enum model\n\n    dataset: xr.Dataset\n        The input Xarray dataset from which to extract the user-requested\n        variables\n\n    Returns\n    -------\n\n\n    Examples\n    --------\n\n\n    Notes\n    -----\n    Is quasi-identical to the function\n    select_netcdf_variable_set_from_dataset() with differences in terms of the\n    names of attributes. See also docstring of other function.\n    \"\"\"\n    # Hardcoded ! ---------------------------------------------\n    metadata_attributes = {\"record_status\"}\n    coordinates_data_attributes = {\"lat_bnds\", \"lon_bnds\"}\n    time_coordinate = {\"time\"}\n    variables_attributes = set(dataset.variables)\n    coordinates_attributes = set(dataset.coords)\n    data_attributes = (\n        set(dataset.data_vars) - coordinates_data_attributes - metadata_attributes\n    )\n    # --------------------------------------------- Hardcoded !\n\n    if variable_set == xarray_variable_set.all:\n        return variables_attributes\n\n    elif variable_set == xarray_variable_set.coordinates:\n        return coordinates_attributes\n\n    elif variable_set == xarray_variable_set.coordinates_without_data:\n        return coordinates_attributes - coordinates_data_attributes\n\n    elif variable_set == xarray_variable_set.data:\n        # return data - coordinates_data - metadata\n        return data_attributes - coordinates_data_attributes - metadata_attributes\n\n    elif variable_set == xarray_variable_set.metadata:\n        return metadata_attributes.intersection(variables_attributes)\n\n    elif variable_set == xarray_variable_set.time:\n        return time_coordinate\n\n    else:\n        raise ValueError(\"Invalid category\")\n</code></pre>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.utilities","title":"utilities","text":"<p>Functions:</p> Name Description <code>get_scale_and_offset</code> <p>Get scale and offset values from a netCDF file</p> <code>select_location_time_series</code> <p>Select a location from a time series dataset format supported by</p> <code>set_location_indexers</code> <p>Select single pair of coordinates from a data array</p>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.utilities.get_scale_and_offset","title":"get_scale_and_offset","text":"<pre><code>get_scale_and_offset(netcdf)\n</code></pre> <p>Get scale and offset values from a netCDF file</p> Source code in <code>rekx/utilities.py</code> <pre><code>def get_scale_and_offset(netcdf):\n    \"\"\"Get scale and offset values from a netCDF file\"\"\"\n    dataset = netCDF4.Dataset(netcdf)\n    netcdf_dimensions = set(dataset.dimensions)\n    netcdf_dimensions.update(\n        {\"lon\", \"longitude\", \"lat\", \"latitude\"}\n    )  # all space dimensions?\n    netcdf_variables = set(dataset.variables)\n    variable = str(\n        list(netcdf_variables.difference(netcdf_dimensions))[0]\n    )  # single variable name!\n\n    if \"scale_factor\" in dataset[variable].ncattrs():\n        scale_factor = dataset[variable].scale_factor\n    else:\n        scale_factor = None\n\n    if \"add_offset\" in dataset[variable].ncattrs():\n        add_offset = dataset[variable].add_offset\n    else:\n        add_offset = None\n\n    return (scale_factor, add_offset)\n</code></pre>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.utilities.select_location_time_series","title":"select_location_time_series","text":"<pre><code>select_location_time_series(\n    time_series: Path = None,\n    longitude: float = None,\n    latitude: float = None,\n    mask_and_scale: bool = False,\n    neighbor_lookup: MethodForInexactMatches = nearest,\n    tolerance: float = 0.1,\n    in_memory: bool = False,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n)\n</code></pre> <p>Select a location from a time series dataset format supported by xarray</p> Source code in <code>rekx/utilities.py</code> <pre><code>def select_location_time_series(\n    time_series: Path = None,\n    longitude: float = None,  # Longitude = None,\n    latitude: float = None,  # Latitude = None,\n    mask_and_scale: bool = False,\n    neighbor_lookup: MethodForInexactMatches = MethodForInexactMatches.nearest,\n    tolerance: float = 0.1,\n    in_memory: bool = False,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n):\n    \"\"\"Select a location from a time series dataset format supported by\n    xarray\"\"\"\n    data_array = open_data_array(\n        time_series,\n        mask_and_scale,\n        in_memory,\n    )\n    indexers = set_location_indexers(\n        data_array=data_array,\n        longitude=longitude,\n        latitude=latitude,\n        verbose=verbose,\n    )\n    try:\n        location_time_series = data_array.sel(\n            **indexers,\n            method=neighbor_lookup,\n            tolerance=tolerance,\n        )\n        location_time_series.load()  # load into memory for fast processing\n\n    except Exception as exception:\n        print(f\"{ERROR_IN_SELECTING_DATA} : {exception}\")\n        raise SystemExit(33)\n\n    if verbose == 3:\n        debug(locals())\n\n    return location_time_series\n</code></pre>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.utilities.set_location_indexers","title":"set_location_indexers","text":"<pre><code>set_location_indexers(\n    data_array,\n    longitude: float = None,\n    latitude: float = None,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n)\n</code></pre> <p>Select single pair of coordinates from a data array</p> <p>Will select center coordinates if none of (longitude, latitude) are provided.</p> Source code in <code>rekx/utilities.py</code> <pre><code>def set_location_indexers(\n    data_array,\n    longitude: float = None,  # Longitude = None,\n    latitude: float = None,  # Latitude = None,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n):\n    \"\"\"Select single pair of coordinates from a data array\n\n    Will select center coordinates if none of (longitude, latitude) are\n    provided.\n    \"\"\"\n    # ----------------------------------------------------------- Deduplicate me\n    # Ugly hack for when dimensions 'longitude', 'latitude' are not spelled out!\n    # Use `coords` : a time series of a single pair of coordinates has only a `time` dimension!\n    indexers = {}\n    dimensions = [\n        dimension for dimension in data_array.coords if isinstance(dimension, str)\n    ]\n    if set([\"lon\", \"lat\"]) &amp; set(dimensions):\n        x = \"lon\"\n        y = \"lat\"\n    elif set([\"longitude\", \"latitude\"]) &amp; set(dimensions):\n        x = \"longitude\"\n        y = \"latitude\"\n\n    if x and y:\n        logger.info(f\"Dimensions  : {x}, {y}\")\n\n    if not (longitude and latitude):\n        warning = f\"{exclamation_mark} Coordinates (longitude, latitude) not provided. Selecting center coordinates.\"\n        logger.warning(warning)\n        print(warning)\n\n        center_longitude = float(data_array[x][len(data_array[x]) // 2])\n        center_latitude = float(data_array[y][len(data_array[y]) // 2])\n        indexers[x] = center_longitude\n        indexers[y] = center_latitude\n\n        text_coordinates = f\"{check_mark} Center coordinates (longitude, latitude) : {center_longitude}, {center_latitude}.\"\n\n    else:\n        indexers[x] = longitude\n        indexers[y] = latitude\n        text_coordinates = f\"{check_mark} Coordinates : {longitude}, {latitude}.\"\n\n    logger.info(text_coordinates)\n\n    if verbose &gt; 0:\n        print(text_coordinates)\n\n    if verbose == 3:\n        debug(locals())\n\n    return indexers\n</code></pre>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.conversions","title":"conversions","text":"<p>Functions:</p> Name Description <code>convert_to_radians</code> <p>Convert floating point angular measurement from degrees to radians.</p>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.conversions.convert_to_radians","title":"convert_to_radians","text":"<pre><code>convert_to_radians(\n    ctx: Context, param: CallbackParam, angle: float\n) -&gt; float\n</code></pre> <p>Convert floating point angular measurement from degrees to radians.</p> Source code in <code>rekx/conversions.py</code> <pre><code>def convert_to_radians(\n    ctx: typer.Context, param: typer.CallbackParam, angle: float\n) -&gt; float:\n    \"\"\"Convert floating point angular measurement from degrees to radians.\"\"\"\n    if ctx.resilient_parsing:\n        return\n    if type(angle) != float:\n        raise typer.BadParameter(\"Input should be a float!\")\n\n    return np.radians(angle)\n</code></pre>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.timestamp","title":"timestamp","text":"<p>Functions:</p> Name Description <code>generate_datetime_series</code> <p>Example</p>","tags":["rekx","CLI","Reference"]},{"location":"cli/interface/#rekx.timestamp.generate_datetime_series","title":"generate_datetime_series","text":"<pre><code>generate_datetime_series(\n    start_time: Optional[str] = None,\n    end_time: Optional[str] = None,\n    frequency: Optional[str] = TIMESTAMPS_FREQUENCY_DEFAULT,\n)\n</code></pre> Example <p>start_time = '2010-06-01 06:00:00' end_time = '2010-06-01 08:00:00' frequency = 'h'  # 'h' for hourly generate_datetime_series(start_time, end_time, frequency) array(['2010-06-01T06:00:00', '2010-06-01T07:00:00', '2010-06-01T08:00:00'],       dtype='datetime64[s]')</p> Source code in <code>rekx/timestamp.py</code> <pre><code>def generate_datetime_series(\n    start_time: Optional[str] = None,\n    end_time: Optional[str] = None,\n    frequency: Optional[str] = TIMESTAMPS_FREQUENCY_DEFAULT,\n):\n    \"\"\"\n    Example\n    -------\n    &gt;&gt;&gt; start_time = '2010-06-01 06:00:00'\n    &gt;&gt;&gt; end_time = '2010-06-01 08:00:00'\n    &gt;&gt;&gt; frequency = 'h'  # 'h' for hourly\n    &gt;&gt;&gt; generate_datetime_series(start_time, end_time, frequency)\n    array(['2010-06-01T06:00:00', '2010-06-01T07:00:00', '2010-06-01T08:00:00'],\n          dtype='datetime64[s]')\n    \"\"\"\n    start = np.datetime64(start_time)\n    end = np.datetime64(end_time)\n    freq = np.timedelta64(1, frequency)\n    timestamps = np.arange(start, end + freq, freq)  # +freq to include the end time\n\n    from pandas import DatetimeIndex\n\n    timestamps = DatetimeIndex(timestamps.astype(\"datetime64[ns]\"))\n    return timestamps.astype(\"datetime64[ns]\")\n</code></pre>","tags":["rekx","CLI","Reference"]},{"location":"cli/kerchunking/","title":"Kerchunking","text":"","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/kerchunking/#rekx.reference","title":"reference","text":"<p>Functions:</p> Name Description <code>create_kerchunk_reference</code> <p>Reference local NetCDF files using Kerchunk</p> <code>create_single_reference</code> <p>Helper function for create_kerchunk_reference()</p>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/kerchunking/#rekx.reference.create_kerchunk_reference","title":"create_kerchunk_reference","text":"<pre><code>create_kerchunk_reference(\n    source_directory: Path,\n    output_directory: Path,\n    pattern: str = \"*.nc\",\n    workers: int = 4,\n    dry_run: bool = False,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n)\n</code></pre> <p>Reference local NetCDF files using Kerchunk</p> Source code in <code>rekx/reference.py</code> <pre><code>def create_kerchunk_reference(\n    source_directory: Annotated[Path, typer_argument_source_directory],\n    output_directory: Annotated[Path, typer_argument_output_directory],\n    pattern: Annotated[str, typer_option_filename_pattern] = \"*.nc\",\n    workers: Annotated[int, typer_option_number_of_workers] = 4,\n    dry_run: Annotated[bool, typer_option_dry_run] = False,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n):\n    \"\"\"Reference local NetCDF files using Kerchunk\"\"\"\n    # import cProfile\n    # import pstats\n    # profiler = cProfile.Profile()\n    # profiler.enable()\n\n    file_paths = list(source_directory.glob(pattern))\n    if not file_paths:\n        logger.info(\"No files found in the source directory matching the pattern.\")\n        return\n    if dry_run:\n        print(\n            f\"[bold]Dry run[/bold] of [bold]operations that would be performed[/bold]:\"\n        )\n        print(\n            f\"&gt; Reading files in [code]{source_directory}[/code] matching the pattern [code]{pattern}[/code]\"\n        )\n        print(f\"&gt; Number of files matched: {len(file_paths)}\")\n        print(f\"&gt; Creating single reference files to [code]{output_directory}[/code]\")\n        return  # Exit for a dry run\n    output_directory.mkdir(parents=True, exist_ok=True)\n\n    # Map verbosity level to display mode\n    mode = DisplayMode(verbose)\n    with display_context[mode]:\n        with multiprocessing.Pool(processes=workers) as pool:\n            from functools import partial\n\n            partial_create_single_reference = partial(\n                create_single_reference, output_directory=output_directory\n            )\n            results = pool.map(partial_create_single_reference, file_paths)\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/kerchunking/#rekx.reference.create_single_reference","title":"create_single_reference","text":"<pre><code>create_single_reference(\n    file_path: Path,\n    output_directory: Path,\n    verbose: int = 0,\n)\n</code></pre> <p>Helper function for create_kerchunk_reference()</p> Notes <p>Will create an MD5 Hash for each new reference file in order to avoid regenerating the same file in case of a renewed attempt to reference the same file.  This is useful in the context or epxlorative massive processing.</p> Source code in <code>rekx/reference.py</code> <pre><code>def create_single_reference(\n    file_path: Path,\n    output_directory: Path,\n    # md5: bool = True,\n    verbose: int = 0,\n):\n    \"\"\"Helper function for create_kerchunk_reference()\n\n    Notes\n    -----\n\n    Will create an MD5 Hash for each new reference file in order to avoid\n    regenerating the same file in case of a renewed attempt to reference the\n    same file.  This is useful in the context or epxlorative massive\n    processing.\n\n    \"\"\"\n    filename = file_path.stem\n    output_file = f\"{output_directory}/{filename}.json\"\n    hash_file = output_file + \".hash\"\n    generated_hash = generate_file_md5(file_path)\n    local_fs = fsspec.filesystem(\"file\")\n    if local_fs.exists(output_file) and local_fs.exists(hash_file):\n        logger.debug(f\"Found a reference file '{output_file}' and a hash '{hash_file}'\")\n        with local_fs.open(hash_file, \"r\") as hf:\n            existing_hash = hf.read().strip()\n\n        if existing_hash == generated_hash:\n            pass\n    else:\n        logger.debug(\n            f\"Creating reference file '{output_file}' with hash '{generated_hash}'\"\n        )\n        file_url = f\"file://{file_path}\"\n        with fsspec.open(file_url, mode=\"rb\") as input_file:\n            h5chunks = SingleHdf5ToZarr(input_file, file_url, inline_threshold=0)\n            json = ujson.dumps(h5chunks.translate()).encode()\n            with local_fs.open(output_file, \"wb\") as f:\n                f.write(json)\n            with local_fs.open(hash_file, \"w\") as hf:\n                hf.write(generated_hash)\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/kerchunking/#rekx.combine","title":"combine","text":"<p>Functions:</p> Name Description <code>combine_kerchunk_references</code> <p>Combine multiple JSON references into a single logical aggregate</p> <code>combine_kerchunk_references_to_parquet</code> <p>Combine multiple JSON references into a single Parquet store using Kerchunk's <code>MultiZarrToZarr</code> function</p>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/kerchunking/#rekx.combine.combine_kerchunk_references","title":"combine_kerchunk_references","text":"<pre><code>combine_kerchunk_references(\n    source_directory: Path,\n    pattern: str = \"*.json\",\n    combined_reference: Path = \"combined_kerchunk.json\",\n    dry_run: bool = False,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n)\n</code></pre> <p>Combine multiple JSON references into a single logical aggregate dataset using Kerchunk's <code>MultiZarrToZarr</code> function</p> Source code in <code>rekx/combine.py</code> <pre><code>def combine_kerchunk_references(\n    source_directory: Annotated[Path, typer_argument_source_directory],\n    pattern: Annotated[str, typer_option_filename_pattern] = \"*.json\",\n    combined_reference: Annotated[\n        Path, typer_argument_kerchunk_combined_reference\n    ] = \"combined_kerchunk.json\",\n    dry_run: Annotated[bool, typer_option_dry_run] = False,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n):\n    \"\"\"Combine multiple JSON references into a single logical aggregate\n    dataset using Kerchunk's `MultiZarrToZarr` function\"\"\"\n\n    mode = DisplayMode(verbose)\n    with display_context[mode]:\n        source_directory = Path(source_directory)\n        reference_file_paths = list(source_directory.glob(pattern))\n        reference_file_paths = list(map(str, reference_file_paths))\n\n        if dry_run:\n            print(\n                f\"[bold]Dry run[/bold] of [bold]operations that would be performed[/bold]:\"\n            )\n            print(\n                f\"&gt; Reading files in [code]{source_directory}[/code] matching the pattern [code]{pattern}[/code]\"\n            )\n            print(f\"&gt; Number of files matched: {len(reference_file_paths)}\")\n            print(\n                f\"&gt; Writing combined reference file to [code]{combined_reference}[/code]\"\n            )\n            return  # Exit for a dry run\n\n        from kerchunk.combine import MultiZarrToZarr\n\n        mzz = MultiZarrToZarr(\n            reference_file_paths,\n            concat_dims=[\"time\"],\n            identical_dims=[\"lat\", \"lon\"],\n        )\n        multifile_kerchunk = mzz.translate()\n\n        combined_reference_filename = Path(combined_reference)\n        local_fs = fsspec.filesystem(\"file\")\n        with local_fs.open(combined_reference_filename, \"wb\") as f:\n            f.write(ujson.dumps(multifile_kerchunk).encode())\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/kerchunking/#rekx.combine.combine_kerchunk_references_to_parquet","title":"combine_kerchunk_references_to_parquet","text":"<pre><code>combine_kerchunk_references_to_parquet(\n    source_directory: Path,\n    pattern: str = \"*.json\",\n    combined_reference: Path = \"combined_kerchunk.parq\",\n    dry_run: bool = False,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n)\n</code></pre> <p>Combine multiple JSON references into a single Parquet store using Kerchunk's <code>MultiZarrToZarr</code> function</p> Source code in <code>rekx/combine.py</code> <pre><code>def combine_kerchunk_references_to_parquet(\n    source_directory: Annotated[Path, typer_argument_source_directory],\n    pattern: Annotated[str, typer_option_filename_pattern] = \"*.json\",\n    combined_reference: Annotated[\n        Path, typer_argument_kerchunk_combined_reference\n    ] = \"combined_kerchunk.parq\",\n    dry_run: Annotated[bool, typer_option_dry_run] = False,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n):\n    \"\"\"Combine multiple JSON references into a single Parquet store using Kerchunk's `MultiZarrToZarr` function\"\"\"\n\n    mode = DisplayMode(verbose)\n    with display_context[mode]:\n        source_directory = Path(source_directory)\n        reference_file_paths = list(source_directory.glob(pattern))\n        reference_file_paths = list(map(str, reference_file_paths))\n\n        if dry_run:\n            print(\n                f\"[bold]Dry run[/bold] of [bold]operations that would be performed[/bold]:\"\n            )\n            print(\n                f\"&gt; Reading files in [code]{source_directory}[/code] matching the pattern [code]{pattern}[/code]\"\n            )\n            print(f\"&gt; Number of files matched: {len(reference_file_paths)}\")\n            print(\n                f\"&gt; Writing combined reference file to [code]{combined_reference}[/code]\"\n            )\n            return  # Exit for a dry run\n\n        # Create LazyReferenceMapper to pass to MultiZarrToZarr\n        filesystem = fsspec.filesystem(\"file\")\n        import os\n\n        combined_reference.mkdir(parents=True, exist_ok=True)\n        from fsspec.implementations.reference import LazyReferenceMapper\n\n        output_lazy = LazyReferenceMapper(\n            root=str(combined_reference),\n            fs=filesystem,\n            cache_size=1000,\n        )\n\n        from kerchunk.combine import MultiZarrToZarr\n\n        # Combine single references\n        mzz = MultiZarrToZarr(\n            reference_file_paths,\n            remote_protocol=\"file\",\n            concat_dims=[\"time\"],\n            identical_dims=[\"lat\", \"lon\"],\n            out=output_lazy,\n        )\n        multifile_kerchunk = mzz.translate()\n\n        output_lazy.flush()  # Write all non-full reference batches\n\n        # Read from the Parquet storage\n        kerchunk.df.refs_to_dataframe(multifile_kerchunk, str(combined_reference))\n\n        filesystem = fsspec.implementations.reference.ReferenceFileSystem(\n            fo=str(combined_reference),\n            target_protocol=\"file\",\n            remote_protocol=\"file\",\n            lazy=True,\n        )\n        ds = xr.open_dataset(\n            filesystem.get_mapper(\"\"),\n            engine=\"zarr\",\n            chunks={},\n            backend_kwargs={\"consolidated\": False},\n        )\n        print(ds)\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/kerchunking/#rekx.parquet","title":"parquet","text":"<p>Functions:</p> Name Description <code>combine_parquet_stores_to_parquet</code> <p>Combine multiple Parquet stores into a single aggregate dataset using Kerchunk's <code>MultiZarrToZarr</code> function</p> <code>create_multiple_parquet_stores</code> <code>create_parquet_store</code> <code>create_single_parquet_store</code> <p>Helper function for create_multiple_parquet_stores()</p> <code>parquet_multi_reference</code> <p>Create Parquet references from an HDF5/NetCDF file</p> <code>parquet_reference</code> <p>Create Parquet references from an HDF5/NetCDF file</p> <code>select_from_parquet</code> <p>Select data from a Parquet store</p>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/kerchunking/#rekx.parquet.combine_parquet_stores_to_parquet","title":"combine_parquet_stores_to_parquet","text":"<pre><code>combine_parquet_stores_to_parquet(\n    source_directory: Path,\n    pattern: str = \"*.parquet\",\n    combined_reference: Path = \"combined_kerchunk.parquet\",\n    record_size: int = DEFAULT_RECORD_SIZE,\n    dry_run: bool = False,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n)\n</code></pre> <p>Combine multiple Parquet stores into a single aggregate dataset using Kerchunk's <code>MultiZarrToZarr</code> function</p> Source code in <code>rekx/parquet.py</code> <pre><code>def combine_parquet_stores_to_parquet(\n    source_directory: Annotated[Path, typer_argument_source_directory],\n    pattern: Annotated[str, typer_option_filename_pattern] = \"*.parquet\",\n    combined_reference: Annotated[\n        Path, typer_argument_kerchunk_combined_reference\n    ] = \"combined_kerchunk.parquet\",\n    record_size: int = DEFAULT_RECORD_SIZE,\n    dry_run: Annotated[\n        bool,\n        typer.Option(\"--dry-run\", help=\"Run the command without making any changes.\"),\n    ] = False,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n):\n    \"\"\"Combine multiple Parquet stores into a single aggregate dataset using Kerchunk's `MultiZarrToZarr` function\"\"\"\n\n    mode = DisplayMode(verbose)\n    with display_context[mode]:\n        source_directory = Path(source_directory)\n        reference_file_paths = list(source_directory.glob(pattern))\n        reference_file_paths = list(map(str, reference_file_paths))\n        reference_file_paths.sort()\n\n        if dry_run:\n            print(\n                f\"[bold]Dry run[/bold] of [bold]operations that would be performed[/bold]:\"\n            )\n            print(\n                f\"&gt; Reading files in [code]{source_directory}[/code] matching the pattern [code]{pattern}[/code]\"\n            )\n            print(f\"&gt; Number of files matched: {len(reference_file_paths)}\")\n            print(\n                f\"&gt; Writing combined reference file to [code]{combined_reference}[/code]\"\n            )\n            return  # Exit for a dry run\n\n        try:\n            # Create LazyReferenceMapper to pass to MultiZarrToZarr\n            combined_reference.mkdir(parents=True, exist_ok=True)\n            print(f\"Combined reference name : {combined_reference}\")\n            filesystem = fsspec.filesystem(\"file\")\n            from fsspec.implementations.reference import LazyReferenceMapper\n\n            output_lazy = LazyReferenceMapper.create(\n                root=str(combined_reference),\n                fs=filesystem,\n                record_size=record_size,\n            )\n\n            # Combine single references\n            from kerchunk.combine import MultiZarrToZarr\n\n            multi_zarr_to_zarr = MultiZarrToZarr(\n                reference_file_paths,\n                remote_protocol=\"file\",\n                concat_dims=[\"time\"],\n                identical_dims=[\"lat\", \"lon\"],\n                coo_map={\"time\": \"cf:time\"},\n                out=output_lazy,\n            )\n            multifile_kerchunk = multi_zarr_to_zarr.translate()\n            output_lazy.flush()  # Write all non-full reference batches\n\n        except Exception as e:\n            print(f\"Failed creating the [code]{combined_reference}[/code] : {e}!\")\n            import traceback\n\n            traceback.print_exc()\n\n        if verbose &gt; 1:\n            # Read from the Parquet storage\n            # kerchunk.df.refs_to_dataframe(multifile_kerchunk, str(combined_reference))\n\n            # filesystem = fsspec.implementations.reference.ReferenceFileSystem(\n            #     fo=str(combined_reference),\n            #     target_protocol='file',\n            #     remote_protocol='file',\n            #     lazy=True\n            # )\n            # ds = xr.open_dataset(\n            #     filesystem.get_mapper(''),\n            #     engine=\"zarr\",\n            #     chunks={},\n            #     backend_kwargs={\"consolidated\": False},\n            # )\n            # print(ds)\n            dataset = xr.open_dataset(\n                str(combined_reference),  # does not handle Path\n                engine=\"kerchunk\",\n                storage_options=dict(remote_protocol=\"file\")\n                # storage_options=dict(skip_instance_cache=True, remote_protocol=\"file\"),\n            )\n            print(dataset)\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/kerchunking/#rekx.parquet.create_multiple_parquet_stores","title":"create_multiple_parquet_stores","text":"<pre><code>create_multiple_parquet_stores(\n    source_directory: Path,\n    output_directory: Path,\n    pattern: str = \"*.nc\",\n    record_size: int = DEFAULT_RECORD_SIZE,\n    workers: int = 4,\n    verbose: int = 0,\n)\n</code></pre> Source code in <code>rekx/parquet.py</code> <pre><code>def create_multiple_parquet_stores(\n    source_directory: Path,\n    output_directory: Path,\n    pattern: str = \"*.nc\",\n    record_size: int = DEFAULT_RECORD_SIZE,\n    workers: int = 4,\n    verbose: int = 0,\n):\n    \"\"\" \"\"\"\n    input_file_paths = list(source_directory.glob(pattern))\n    # if verbose:\n    #     print(f'Input file paths : {input_file_paths}')\n    if not input_file_paths:\n        print(\n            \"No files found in [code]{source_directory}[/code] matching the pattern [code]{pattern}[/code]!\"\n        )\n        return\n    output_directory.mkdir(parents=True, exist_ok=True)\n    with multiprocessing.Pool(processes=workers) as pool:\n        print(\n            f\"Creating the following Parquet stores in [code]{output_directory}[/code] : \"\n        )\n        partial_create_parquet_references = partial(\n            create_single_parquet_store,\n            output_directory=output_directory,\n            record_size=record_size,\n            verbose=verbose,\n        )\n        pool.map(partial_create_parquet_references, input_file_paths)\n    if verbose:\n        print(f\"Done!\")\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/kerchunking/#rekx.parquet.create_parquet_store","title":"create_parquet_store","text":"<pre><code>create_parquet_store(\n    input_file: Path,\n    output_parquet_store: Path,\n    record_size: int = DEFAULT_RECORD_SIZE,\n)\n</code></pre> Source code in <code>rekx/parquet.py</code> <pre><code>def create_parquet_store(\n    input_file: Path,\n    output_parquet_store: Path,\n    record_size: int = DEFAULT_RECORD_SIZE,\n):\n    \"\"\" \"\"\"\n    log_messages = []\n    log_messages.append(\"Logging execution of create_parquet_store()\")\n    output_parquet_store.mkdir(parents=True, exist_ok=True)\n\n    try:\n        log_messages.append(f\"Creating a filesystem mapper for {output_parquet_store}\")\n        filesystem = fsspec.filesystem(\"file\")\n        output = LazyReferenceMapper.create(\n            root=str(output_parquet_store),  # does not handle Path\n            fs=filesystem,\n            record_size=record_size,\n        )\n        log_messages.append(f\"Created the filesystem mapper {output}\")\n\n        log_messages.append(f\"Kerchunking the file {input_file}\")\n        single_zarr = SingleHdf5ToZarr(str(input_file), out=output)\n        single_zarr.translate()\n        log_messages.append(f\"Kerchunked the file {input_file}\")\n\n    except Exception as e:\n        print(f\"Failed processing file [code]{input_file}[/code] : {e}\")\n        log_messages.append(f\"Exception occurred: {e}\")\n        log_messages.append(\"Traceback (most recent call last):\")\n\n        tb_lines = traceback.format_exc().splitlines()\n        for line in tb_lines:\n            log_messages.append(line)\n\n        raise\n\n    finally:\n        logger.info(\"\\n\".join(log_messages))\n\n    logger.info(f\"Returning a Parquet store : {output_parquet_store}\")\n    return output_parquet_store\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/kerchunking/#rekx.parquet.create_single_parquet_store","title":"create_single_parquet_store","text":"<pre><code>create_single_parquet_store(\n    input_file_path,\n    output_directory,\n    record_size: int = DEFAULT_RECORD_SIZE,\n    verbose: int = 0,\n)\n</code></pre> <p>Helper function for create_multiple_parquet_stores()</p> Source code in <code>rekx/parquet.py</code> <pre><code>def create_single_parquet_store(\n    input_file_path,\n    output_directory,\n    record_size: int = DEFAULT_RECORD_SIZE,\n    verbose: int = 0,\n):\n    \"\"\"Helper function for create_multiple_parquet_stores()\"\"\"\n    filename = input_file_path.stem\n    single_parquet_store = output_directory / f\"{filename}.parquet\"\n    create_parquet_store(\n        input_file_path,\n        output_parquet_store=single_parquet_store,\n        record_size=record_size,\n    )\n    if verbose &gt; 0:\n        print(f\"  [code]{single_parquet_store}[/code]\")\n\n    if verbose &gt; 1:\n        dataset = xr.open_dataset(\n            str(single_parquet_store),\n            engine=\"kerchunk\",\n            storage_options=dict(remote_protocol=\"file\"),\n        )\n        print(dataset)\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/kerchunking/#rekx.parquet.parquet_multi_reference","title":"parquet_multi_reference","text":"<pre><code>parquet_multi_reference(\n    source_directory: Path,\n    output_directory: Optional[Path] = \".\",\n    pattern: str = \"*.nc\",\n    record_size: int = DEFAULT_RECORD_SIZE,\n    workers: int = 4,\n    dry_run: bool = False,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n)\n</code></pre> <p>Create Parquet references from an HDF5/NetCDF file</p> Source code in <code>rekx/parquet.py</code> <pre><code>def parquet_multi_reference(\n    source_directory: Path,\n    output_directory: Optional[Path] = \".\",\n    pattern: str = \"*.nc\",\n    record_size: int = DEFAULT_RECORD_SIZE,\n    workers: int = 4,\n    dry_run: Annotated[bool, typer_option_dry_run] = False,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n):\n    \"\"\"Create Parquet references from an HDF5/NetCDF file\"\"\"\n    input_file_paths = list(source_directory.glob(pattern))\n\n    if not input_file_paths:\n        print(\"No files found in the source directory matching the pattern.\")\n        return\n\n    if dry_run:\n        print(f\"[bold]Dry running operations that would be performed[/bold]:\")\n        print(\n            f\"&gt; Reading files in [code]{source_directory}[/code] matching the pattern [code]{pattern}[/code]\"\n        )\n        print(f\"&gt; Number of files matched : {len(input_file_paths)}\")\n        print(f\"&gt; Creating Parquet stores in [code]{output_directory}[/code]\")\n        return  # Exit for a dry run\n\n    create_multiple_parquet_stores(\n        source_directory=source_directory,\n        output_directory=output_directory,\n        pattern=pattern,\n        record_size=record_size,\n        workers=workers,\n        verbose=verbose,\n    )\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/kerchunking/#rekx.parquet.parquet_reference","title":"parquet_reference","text":"<pre><code>parquet_reference(\n    input_file: Path,\n    output_directory: Optional[Path] = \".\",\n    record_size: int = DEFAULT_RECORD_SIZE,\n    dry_run: bool = False,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n)\n</code></pre> <p>Create Parquet references from an HDF5/NetCDF file</p> Source code in <code>rekx/parquet.py</code> <pre><code>def parquet_reference(\n    input_file: Path,\n    output_directory: Optional[Path] = \".\",\n    record_size: int = DEFAULT_RECORD_SIZE,\n    dry_run: Annotated[bool, typer_option_dry_run] = False,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n):\n    \"\"\"Create Parquet references from an HDF5/NetCDF file\"\"\"\n    filename = input_file.stem\n    output_parquet_store = output_directory / f\"{filename}.parquet\"\n\n    if dry_run:\n        print(f\"[bold]Dry running operations that would be performed[/bold]:\")\n        print(\n            f\"&gt; Creating Parquet references to [code]{input_file}[/code] in [code]{output_parquet_store}[/code]\"\n        )\n        return  # Exit for a dry run\n\n    create_single_parquet_store(\n        input_file_path=input_file,\n        output_directory=output_directory,\n        record_size=record_size,\n        verbose=verbose,\n    )\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/kerchunking/#rekx.parquet.select_from_parquet","title":"select_from_parquet","text":"<pre><code>select_from_parquet(\n    parquet_store: Path,\n    variable: str,\n    longitude: float,\n    latitude: float,\n    timestamps: Optional[Any] = None,\n    start_time: Optional[datetime] = None,\n    end_time: Optional[datetime] = None,\n    time: Optional[int] = None,\n    lat: Optional[int] = None,\n    lon: Optional[int] = None,\n    mask_and_scale: bool = False,\n    neighbor_lookup: MethodForInexactMatches = None,\n    tolerance: Optional[float] = 0.1,\n    in_memory: bool = False,\n    statistics: bool = False,\n    csv: Path = None,\n    variable_name_as_suffix: bool = True,\n    rounding_places: Optional[\n        int\n    ] = ROUNDING_PLACES_DEFAULT,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n) -&gt; None\n</code></pre> <p>Select data from a Parquet store</p> Source code in <code>rekx/parquet.py</code> <pre><code>def select_from_parquet(\n    parquet_store: Annotated[Path, typer.Argument(..., help=\"Path to Parquet store\")],\n    variable: Annotated[str, typer.Argument(..., help=\"Variable name to select from\")],\n    longitude: Annotated[float, typer_argument_longitude_in_degrees],\n    latitude: Annotated[float, typer_argument_latitude_in_degrees],\n    timestamps: Annotated[Optional[Any], typer_argument_timestamps] = None,\n    start_time: Annotated[Optional[datetime], typer_option_start_time] = None,\n    end_time: Annotated[Optional[datetime], typer_option_end_time] = None,\n    time: Annotated[\n        Optional[int], typer.Option(help=\"New chunk size for the 'time' dimension\")\n    ] = None,\n    lat: Annotated[\n        Optional[int], typer.Option(help=\"New chunk size for the 'lat' dimension\")\n    ] = None,\n    lon: Annotated[\n        Optional[int], typer.Option(help=\"New chunk size for the 'lon' dimension\")\n    ] = None,\n    # convert_longitude_360: Annotated[bool, typer_option_convert_longitude_360] = False,\n    mask_and_scale: Annotated[bool, typer_option_mask_and_scale] = False,\n    neighbor_lookup: Annotated[\n        MethodForInexactMatches, typer_option_neighbor_lookup\n    ] = None,\n    tolerance: Annotated[\n        Optional[float], typer_option_tolerance\n    ] = 0.1,  # Customize default if needed\n    in_memory: Annotated[bool, typer_option_in_memory] = False,\n    statistics: Annotated[bool, typer_option_statistics] = False,\n    csv: Annotated[Path, typer_option_csv] = None,\n    # output_filename: Annotated[Path, typer_option_output_filename] = 'series_in',  #Path(),\n    variable_name_as_suffix: Annotated[\n        bool, typer_option_variable_name_as_suffix\n    ] = True,\n    rounding_places: Annotated[\n        Optional[int], typer_option_rounding_places\n    ] = ROUNDING_PLACES_DEFAULT,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n) -&gt; None:\n    \"\"\"Select data from a Parquet store\"\"\"\n\n    # if convert_longitude_360:\n    #     longitude = longitude % 360\n    # warn_for_negative_longitude(longitude)\n\n    logger.debug(f\"Command context : {typer.Context}\")\n\n    data_retrieval_start_time = timer.time()\n    logger.debug(f\"Starting data retrieval... {data_retrieval_start_time}\")\n\n    # timer_start = timer.time()\n    # mapper = fsspec.get_mapper(\n    #     \"reference://\",\n    #     fo=str(reference_file),\n    #     remote_protocol=\"file\",\n    #     remote_options={\"skip_instance_cache\": True},\n    # )\n    # timer_end = timer.time()\n    # logger.debug(f\"Mapper creation took {timer_end - timer_start:.2f} seconds\")\n    timer_start = timer.perf_counter()\n    dataset = xr.open_dataset(\n        str(parquet_store),  # does not handle Path\n        engine=\"kerchunk\",\n        storage_options=dict(skip_instance_cache=True, remote_protocol=\"file\"),\n        # backend_kwargs={\"consolidated\": False},\n        # chunks=None,\n        # mask_and_scale=mask_and_scale,\n    )\n    timer_end = timer.perf_counter()\n    logger.debug(\n        f\"Dataset opening via Xarray took {timer_end - timer_start:.2f} seconds\"\n    )\n\n    available_variables = list(dataset.data_vars)\n    if not variable in available_variables:\n        print(\n            f\"The requested variable `{variable}` does not exist! Plese select one among the available variables : {available_variables}.\"\n        )\n        raise typer.Exit(code=0)\n    else:\n        timer_start = timer.time()\n        time_series = dataset[variable]\n        timer_end = timer.time()\n        logger.debug(\n            f\"Data array variable selection took {timer_end - timer_start:.2f} seconds\"\n        )\n\n        timer_start = timer.time()\n        chunks = {\"time\": time, \"lat\": lat, \"lon\": lon}\n        time_series.chunk(chunks=chunks)\n        timer_end = timer.time()\n        logger.debug(\n            f\"Data array rechunking took {timer_end - timer_start:.2f} seconds\"\n        )\n\n    timer_start = timer.time()\n    indexers = set_location_indexers(\n        data_array=time_series,\n        longitude=longitude,\n        latitude=latitude,\n        verbose=verbose,\n    )\n    timer_end = timer.time()\n    logger.debug(\n        f\"Data array indexers setting took {timer_end - timer_start:.2f} seconds\"\n    )\n\n    try:\n        timer_start = timer.time()\n        location_time_series = time_series.sel(\n            **indexers,\n            method=neighbor_lookup,\n            tolerance=tolerance,\n        )\n        timer_end = timer.time()\n        logger.debug(f\"Location selection took {timer_end - timer_start:.2f} seconds\")\n\n        if in_memory:\n            timer_start = timer.time()\n            location_time_series.load()  # load into memory for faster ... ?\n            timer_end = timer.time()\n            logger.debug(\n                f\"Location selection loading in memory took {timer_end - timer_start:.2f} seconds\"\n            )\n\n    except Exception as exception:\n        print(f\"{ERROR_IN_SELECTING_DATA} : {exception}\")\n        raise SystemExit(33)\n    # ------------------------------------------------------------------------\n\n    if start_time or end_time:\n        timestamps = None  # we don't need a timestamp anymore!\n\n        if start_time and not end_time:  # set `end_time` to end of series\n            end_time = location_time_series.time.values[-1]\n\n        elif end_time and not start_time:  # set `start_time` to beginning of series\n            start_time = location_time_series.time.values[0]\n\n        else:  # Convert `start_time` &amp; `end_time` to the correct string format\n            start_time = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n            end_time = end_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        timer_start = timer.time()\n        location_time_series = location_time_series.sel(\n            time=slice(start_time, end_time)\n        )\n        timer_end = timer.time()\n        logger.debug(\n            f\"Time slicing with `start_time` and `end_time` took {timer_end - timer_start:.2f} seconds\"\n        )\n\n    if timestamps is not None and not start_time and not end_time:\n        if len(timestamps) == 1:\n            start_time = end_time = timestamps[0]\n\n        try:\n            timer_start = timer.time()\n            location_time_series = location_time_series.sel(\n                time=timestamps, method=neighbor_lookup\n            )\n            timer_end = timer.time()\n            logger.debug(\n                f\"Time selection with `timestamps` took {timer_end - timer_start:.2f} seconds\"\n            )\n\n        except KeyError:\n            print(f\"No data found for one or more of the given {timestamps}.\")\n\n    if location_time_series.size == 1:\n        timer_start = timer.time()\n        single_value = float(location_time_series.values)\n        warning = (\n            f\"{exclamation_mark} The selected timestamp \"\n            + f\"{location_time_series.time.values}\"\n            + f\" matches the single value \"\n            + f\"{single_value}\"\n        )\n        timer_end = timer.time()\n        logger.debug(\n            f\"Single value conversion to float took {timer_end - timer_start:.2f} seconds\"\n        )\n        logger.warning(warning)\n        if verbose &gt; 0:\n            print(warning)\n\n    data_retrieval_end_time = timer.time()\n    logger.debug(\n        f\"Data retrieval took {data_retrieval_end_time - data_retrieval_start_time:.2f} seconds\"\n    )\n\n    timer_start = timer.time()\n    results = {\n        location_time_series.name: location_time_series.to_numpy(),\n    }\n    timer_end = timer.time()\n    logger.debug(\n        f\"Data series conversion to NumPy took {timer_end - timer_start:.2f} seconds\"\n    )\n\n    title = \"Location time series\"\n\n    # special case!\n    if location_time_series is not None and timestamps is None:\n        timer_start = timer.time()\n        timestamps = location_time_series.time.to_numpy()\n        timer_end = timer.time()\n        logger.debug(\n            f\"Timestamps conversion to NumPy from Xarray's _time_ coordinate took {timer_end - timer_start:.2f} seconds\"\n        )\n\n    if not verbose and not (statistics or csv):\n        flat_array = location_time_series.values.flatten()\n        print(*flat_array, sep=\", \")\n    if verbose &gt; 0:\n        print(location_time_series)\n\n    if statistics:  # after echoing series which might be Long!\n        timer_start = timer.time()\n        print_series_statistics(\n            data_array=location_time_series,\n            timestamps=timestamps,\n            title=\"Selected series\",\n        )\n        timer_end = timer.time()\n        logger.debug(\n            f\"Printing statistics in the console took {timer_end - timer_start:.2f} seconds\"\n        )\n\n    if csv:\n        timer_start = timer.time()\n        to_csv(\n            x=location_time_series,\n            path=csv,\n        )\n        timer_end = timer.time()\n        logger.debug(f\"Exporting to CSV took {timer_end - timer_start:.2f} seconds\")\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/logging/","title":"Logging &amp; Verbosity","text":"","tags":["rekx","CLI","Reference","Logging"]},{"location":"cli/logging/#rekx.log","title":"log","text":"<p>Functions:</p> Name Description <code>filter_function</code> <p>Determine which logs to include based on the verbosity level.</p>","tags":["rekx","CLI","Reference","Logging"]},{"location":"cli/logging/#rekx.log.filter_function","title":"filter_function","text":"<pre><code>filter_function(record)\n</code></pre> <p>Determine which logs to include based on the verbosity level.</p> Source code in <code>rekx/log.py</code> <pre><code>def filter_function(record):\n    \"\"\"Determine which logs to include based on the verbosity level.\"\"\"\n    # Assuming record[\"level\"].name gives the log level like \"INFO\", \"DEBUG\", etc.\n    # return verbose\n    return record[\"level\"].name == \"INFO\" if verbosity_level == 1 else True\n</code></pre>","tags":["rekx","CLI","Reference","Logging"]},{"location":"cli/logging/#rekx.progress","title":"progress","text":"","tags":["rekx","CLI","Reference","Logging"]},{"location":"cli/logging/#rekx.messages","title":"messages","text":"","tags":["rekx","CLI","Reference","Logging"]},{"location":"cli/logging/#rekx.print","title":"print","text":"<p>Functions:</p> Name Description <code>print_metadata_series_long_table</code> <code>print_metadata_series_table</code> <code>print_metadata_table</code>","tags":["rekx","CLI","Reference","Logging"]},{"location":"cli/logging/#rekx.print.print_metadata_series_long_table","title":"print_metadata_series_long_table","text":"<pre><code>print_metadata_series_long_table(\n    metadata_series: dict, group_metadata=False\n)\n</code></pre> Source code in <code>rekx/print.py</code> <pre><code>def print_metadata_series_long_table(\n    metadata_series: dict,\n    group_metadata=False,\n):\n    \"\"\" \"\"\"\n    console = Console()\n    columns = []\n    columns.append(\"Name\")\n    columns.append(\"Size\")\n    columns.append(\"Dimensions\")\n    metadata_series_level_one = next(iter(metadata_series.values()))\n    variables_metadata = metadata_series_level_one.get(\"Variables\", {})\n    columns.append(\"Variable\")\n    # Add columns from the first variable's metadata dictionary\n    for key in next(iter(variables_metadata.values())).keys():\n        columns.append(key.replace(\"_\", \" \").title())\n    dimensions = metadata_series_level_one.get(\"Dimensions\", {})\n    dimensions_sort_order = [\"bnds\", \"time\", \"lon\", \"lat\"]\n    dimension_attributes_sorted = {\n        key for key in dimensions_sort_order if key in dimensions\n    }\n    dimension_attributes = \" x \".join(\n        [f\"[bold]{dimension}[/bold]\" for dimension in dimension_attributes_sorted]\n    )\n    caption = f\"Dimensions: {dimension_attributes} | \"\n    caption += f\"Cache [bold]size[/bold] in bytes | \"\n    caption += f\"[bold]Number of elements[/bold] | \"\n    caption += f\"[bold]Preemption strategy[/bold] ranging in [0, 1] | \"\n    repetitions = metadata_series_level_one.get(\"Repetitions\", None)\n    caption += (\n        f\"Average time of [bold]{repetitions}[/bold] reads in [bold]seconds[/bold]\"\n    )\n    table = Table(\n        *columns,\n        caption=caption,\n        show_header=True,\n        header_style=\"bold magenta\",\n        box=SIMPLE_HEAD,\n    )\n\n    # Process each file's metadata\n    for filename, metadata in metadata_series.items():\n        filename = metadata.get(\"File name\", NOT_AVAILABLE)\n        file_size = metadata.get(\"File size\", NOT_AVAILABLE)\n\n        dimensions = metadata.get(\"Dimensions\", {})\n        dimensions_sorted = {\n            key: dimensions[key] for key in dimensions_sort_order if key in dimensions\n        }\n        dimension_shape = \" x \".join(\n            [f\"{size}\" for dimension, size in dimensions_sorted.items()]\n        )\n\n        variables_metadata = metadata.get(\"Variables\", {})\n        # row = []\n        for variable, details in variables_metadata.items():\n            if \"Compression\" in details:\n                compression_details = format_compression(details[\"Compression\"])\n                details[\"Compression\"] = compression_details[\"Filters\"]\n                details[\"Level\"] = compression_details[\"Level\"]\n\n            row = [filename, str(file_size), dimension_shape, variable]\n            row += [str(details.get(key, NOT_AVAILABLE)) for key in details.keys()]\n            table.add_row(*row)\n\n        if group_metadata:\n            table.add_row(\"\")  # Add an empty line between 'files' for clarity\n\n    console.print(table)\n</code></pre>","tags":["rekx","CLI","Reference","Logging"]},{"location":"cli/logging/#rekx.print.print_metadata_series_table","title":"print_metadata_series_table","text":"<pre><code>print_metadata_series_table(\n    metadata_series: dict, group_metadata=False\n)\n</code></pre> Source code in <code>rekx/print.py</code> <pre><code>def print_metadata_series_table(\n    metadata_series: dict,\n    group_metadata=False,\n):\n    \"\"\" \"\"\"\n    for filename, metadata in metadata_series.items():\n        filename = metadata.get(\"File name\", \"N/A\")\n        file_size = metadata.get(\"File size\", \"N/A\")\n        dimensions = metadata.get(\"Dimensions\", {})\n        dimensions_string = \", \".join(\n            [f\"{dimension}: {size}\" for dimension, size in dimensions.items()]\n        )\n        caption = f\"File size: {file_size} bytes, Dimensions: {dimensions_string}\"\n        caption += f\"\\n* Cache: Size in bytes, Number of elements, Preemption ranging in [0, 1]\"\n        variables_metadata = metadata.get(\"Variables\")\n        if variables_metadata:\n            table = Table(\n                title=f\"[bold]{filename}[/bold]\",\n                caption=caption,\n                show_header=True,\n                header_style=\"bold magenta\",\n                box=SIMPLE_HEAD,\n            )\n            # table = Table(caption=caption, show_header=True, header_style=\"bold magenta\", box=SIMPLE_HEAD)\n            table.add_column(\"Variable\", style=\"dim\", no_wrap=True)\n\n            # Expectedly all variables feature the same keys\n            for key in next(iter(variables_metadata.values())).keys():\n                table.add_column(key.replace(\"_\", \" \").title(), no_wrap=True)\n\n            for variable, details in variables_metadata.items():\n                if \"Compression\" in details:\n                    compression_details = format_compression(details[\"Compression\"])\n                    details[\"Compression\"] = compression_details[\"Filters\"]\n                    details[\"Level\"] = compression_details[\"Level\"]\n\n                row = [variable] + [\n                    str(details.get(key, \"\"))\n                    for key in next(iter(variables_metadata.values())).keys()\n                ]\n                table.add_row(*row)\n\n            console = Console()\n            console.print(table)\n            if group_metadata:\n                console.print(\"\\n\")  # Add an empty line between groups for clarity\n</code></pre>","tags":["rekx","CLI","Reference","Logging"]},{"location":"cli/logging/#rekx.print.print_metadata_table","title":"print_metadata_table","text":"<pre><code>print_metadata_table(metadata)\n</code></pre> Source code in <code>rekx/print.py</code> <pre><code>def print_metadata_table(metadata):\n    \"\"\" \"\"\"\n    filename = metadata.get(\"File name\", \"N/A\")\n    file_size = metadata.get(\"File size\", \"N/A\")\n    dimensions = metadata.get(\"Dimensions\", {})\n    dimensions_string = \", \".join(\n        [f\"{dimension}: {size}\" for dimension, size in dimensions.items()]\n    )\n    caption = f\"File size: {file_size} bytes, Dimensions: {dimensions_string}\"\n    caption += (\n        f\"\\n* Cache: Size in bytes, Number of elements, Preemption ranging in [0, 1]\"\n    )\n\n    variables_metadata = metadata.get(\"Variables\")\n    if variables_metadata:\n        table = Table(\n            title=filename,\n            caption=caption,\n            show_header=True,\n            header_style=\"bold magenta\",\n            box=SIMPLE_HEAD,\n        )\n        table.add_column(\"Variable\", style=\"dim\", no_wrap=True)\n\n        # Dynamically add columns based on the keys of the nested dictionaries\n        # Assuming all variables have the same set of keys\n        for key in next(iter(variables_metadata.values())).keys():\n            table.add_column(key.replace(\"_\", \" \").title(), no_wrap=True)\n\n        for variable, details in variables_metadata.items():\n            # Format compression dictionary into a readable string\n            if \"Compression\" in details:\n                compression_details = format_compression(details[\"Compression\"])\n                details[\"Compression\"] = compression_details[\"Filters\"]\n                details[\"Level\"] = compression_details[\"Level\"]\n\n            row = [variable] + [\n                str(details.get(key, \"\"))\n                for key in next(iter(variables_metadata.values())).keys()\n            ]\n            table.add_row(*row)\n\n        console = Console()\n        console.print(table)\n</code></pre>","tags":["rekx","CLI","Reference","Logging"]},{"location":"cli/rechunk/","title":"Rechunk","text":"","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk","title":"rechunk","text":"<p>Classes:</p> Name Description <code>NetCDF4Backend</code> <code>RechunkingBackend</code> <code>XarrayBackend</code> <code>nccopyBackend</code> <p>Functions:</p> Name Description <code>generate_rechunk_commands</code> <p>Generate variations of rechunking commands based on <code>nccopy</code>.</p> <code>generate_rechunk_commands_for_multiple_netcdf</code> <p>Generate variations of rechunking commands based on <code>nccopy</code>.</p> <code>modify_chunk_size</code> <p>Modify the chunk size of a variable in a NetCDF file.</p> <code>rechunk</code> <p>Rechunk a NetCDF4 dataset with options to fine tune the output</p>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk.NetCDF4Backend","title":"NetCDF4Backend","text":"<p>               Bases: <code>RechunkingBackendBase</code></p> <p>Methods:</p> Name Description <code>rechunk</code> <p>Rechunk data stored in a NetCDF4 file.</p>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk.NetCDF4Backend.rechunk","title":"rechunk","text":"<pre><code>rechunk(\n    input_filepath: Path,\n    output_filepath: Path,\n    time: int = None,\n    lat: int = None,\n    lon: int = None,\n) -&gt; None\n</code></pre> <p>Rechunk data stored in a NetCDF4 file.</p> Notes <p>Text partially quoted from</p> <p>https://unidata.github.io/netcdf4-python/#netCDF4.Dataset.createVariable :</p> <p>The function <code>createVariable()</code> available through the <code>netcdf4-python</code> python interface to the netCDF C library, features the optional keyword <code>chunksizes</code> which can be used to manually specify the HDF5 chunk sizes for each dimension of the variable.</p> <p>A detailed discussion of HDF chunking and I/O performance is available at https://support.hdfgroup.org/HDF5/doc/Advanced/Chunking/. The default chunking scheme in the netcdf-c library is discussed at https://docs.unidata.ucar.edu/nug/current/netcdf_perf_chunking.html.</p> <p>Basically, the chunk size for each dimension should match as closely as possible the size of the data block that users will read from the file. <code>chunksizes</code> cannot be set if <code>contiguous=True</code>.</p> Source code in <code>rekx/rechunk.py</code> <pre><code>def rechunk(\n    input_filepath: Path,\n    output_filepath: Path,\n    time: int = None,\n    lat: int = None,\n    lon: int = None,\n) -&gt; None:\n    \"\"\"Rechunk data stored in a NetCDF4 file.\n\n    Notes\n    -----\n    Text partially quoted from\n\n    https://unidata.github.io/netcdf4-python/#netCDF4.Dataset.createVariable :\n\n    The function `createVariable()` available through the `netcdf4-python`\n    python interface to the netCDF C library, features the optional keyword\n    `chunksizes` which can be used to manually specify the HDF5 chunk sizes for\n    each dimension of the variable.\n\n    A detailed discussion of HDF chunking and I/O performance is available at\n    https://support.hdfgroup.org/HDF5/doc/Advanced/Chunking/. The default\n    chunking scheme in the netcdf-c library is discussed at\n    https://docs.unidata.ucar.edu/nug/current/netcdf_perf_chunking.html.\n\n    Basically, the chunk size for each dimension should match as closely as\n    possible the size of the data block that users will read from the file.\n    `chunksizes` cannot be set if `contiguous=True`.\n    \"\"\"\n    # Check if any chunking has been requested\n    if time is None and lat is None and lon is None:\n        logger.info(\n            f\"No chunking requested for {input_filepath}. Exiting function.\"\n        )\n        return\n\n    # logger.info(f\"Rechunking of {input_filepath} with chunk sizes: time={time}, lat={lat}, lon={lon}\")\n    new_chunks = {\"time\": time, \"lat\": lat, \"lon\": lon}\n    with nc.Dataset(input_filepath, mode=\"r\") as input_dataset:\n        with nc.Dataset(output_filepath, mode=\"w\") as output_dataset:\n            for name in input_dataset.ncattrs():\n                output_dataset.setncattr(name, input_dataset.getncattr(name))\n            for name, dimension in input_dataset.dimensions.items():\n                output_dataset.createDimension(\n                    name, (len(dimension) if not dimension.isunlimited() else None)\n                )\n            for name, variable in input_dataset.variables.items():\n                # logger.debug(f\"Processing variable: {name}\")\n                if name in new_chunks:\n                    chunk_size = new_chunks[name]\n                    import dask.array as da\n\n                    if chunk_size is not None:\n                        # logger.debug(f\"Chunking variable `{name}` with chunk sizes: {chunk_size}\")\n                        x = da.from_array(\n                            variable, chunks=(chunk_size,) * len(variable.shape)\n                        )\n                        output_dataset.createVariable(\n                            name,\n                            variable.datatype,\n                            variable.dimensions,\n                            zlib=True,\n                            complevel=4,\n                            chunksizes=(chunk_size,) * len(variable.shape),\n                        )\n                        output_dataset[name].setncatts(input_dataset[name].__dict__)\n                        output_dataset[name][:] = x\n                    else:\n                        # logger.debug(f\"No chunk sizes specified for `{name}`, copying as is.\")\n                        output_dataset.createVariable(\n                            name, variable.datatype, variable.dimensions\n                        )\n                        output_dataset[name].setncatts(input_dataset[name].__dict__)\n                        output_dataset[name][:] = variable[:]\n                else:\n                    # logger.debug(f\"Variable `{name}` not in chunking list, copying as is.\")\n                    output_dataset.createVariable(\n                        name, variable.datatype, variable.dimensions\n                    )\n                    output_dataset[name].setncatts(input_dataset[name].__dict__)\n                    output_dataset[name][:] = variable[:]\n</code></pre>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk.RechunkingBackend","title":"RechunkingBackend","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Methods:</p> Name Description <code>default</code> <p>Default rechunking backend to use</p> <code>get_backend</code> <p>Array type associated to a backend.</p>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk.RechunkingBackend.default","title":"default  <code>classmethod</code>","text":"<pre><code>default() -&gt; RechunkingBackend\n</code></pre> <p>Default rechunking backend to use</p> Source code in <code>rekx/rechunk.py</code> <pre><code>@classmethod\ndef default(cls) -&gt; \"RechunkingBackend\":\n    \"\"\"Default rechunking backend to use\"\"\"\n    return cls.nccopy\n</code></pre>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk.RechunkingBackend.get_backend","title":"get_backend","text":"<pre><code>get_backend() -&gt; RechunkingBackendBase\n</code></pre> <p>Array type associated to a backend.</p> Source code in <code>rekx/rechunk.py</code> <pre><code>def get_backend(self) -&gt; RechunkingBackendBase:\n    \"\"\"Array type associated to a backend.\"\"\"\n\n    if self.name == \"nccopy\":\n        return nccopyBackend()\n\n    elif self.name == \"netcdf4\":\n        return NetCDF4Backend()\n\n    elif self.name == \"xarray\":\n        return XarrayBackend()\n\n    else:\n        raise ValueError(f\"No known backend for {self.name}.\")\n</code></pre>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk.XarrayBackend","title":"XarrayBackend","text":"<p>               Bases: <code>RechunkingBackendBase</code></p> <p>Methods:</p> Name Description <code>rechunk_netcdf_via_xarray</code> <p>Rechunk a NetCDF dataset and save it to a new file.</p>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk.XarrayBackend.rechunk_netcdf_via_xarray","title":"rechunk_netcdf_via_xarray","text":"<pre><code>rechunk_netcdf_via_xarray(\n    input_filepath: Path,\n    output_filepath: Path,\n    time: int = None,\n    latitude: int = None,\n    longitude: int = None,\n) -&gt; None\n</code></pre> <p>Rechunk a NetCDF dataset and save it to a new file.</p> <p>Parameters:</p> Name Type Description Default <code>input_filepath</code> <code>Path</code> <p>The path to the input NetCDF file.</p> required <code>output_filepath</code> <code>Path</code> <p>The path to the output NetCDF file where the rechunked dataset will be saved.</p> required <code>chunks</code> <code>Dict[str, Union[int, None]]</code> <p>A dictionary specifying the new chunk sizes for each dimension. Use <code>None</code> for dimensions that should not be chunked.</p> required <p>Returns:</p> Type Description <code>None</code> <p>The function saves the rechunked dataset to <code>output_filepath</code>.</p> <p>Examples:</p>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk.XarrayBackend.rechunk_netcdf_via_xarray--rechunk_netcdfpathinputnc-pathoutputnc-time-365-lat-25-lon-25","title":"&gt;&gt;&gt; rechunk_netcdf(Path(\"input.nc\"), Path(\"output.nc\"), {'time': 365, 'lat': 25, 'lon': 25})","text":"Source code in <code>rekx/rechunk.py</code> <pre><code>def rechunk_netcdf_via_xarray(\n    input_filepath: Path,\n    output_filepath: Path,\n    time: int = None,\n    latitude: int = None,\n    longitude: int = None,\n) -&gt; None:\n    \"\"\"\n    Rechunk a NetCDF dataset and save it to a new file.\n\n    Parameters\n    ----------\n    input_filepath : Path\n        The path to the input NetCDF file.\n    output_filepath : Path\n        The path to the output NetCDF file where the rechunked dataset will be saved.\n    chunks : Dict[str, Union[int, None]]\n        A dictionary specifying the new chunk sizes for each dimension.\n        Use `None` for dimensions that should not be chunked.\n\n    Returns\n    -------\n    None\n        The function saves the rechunked dataset to `output_filepath`.\n\n    Examples\n    --------\n    # &gt;&gt;&gt; rechunk_netcdf(Path(\"input.nc\"), Path(\"output.nc\"), {'time': 365, 'lat': 25, 'lon': 25})\n    \"\"\"\n    dataset = xr.open_dataset(input_filepath)\n    chunks = {\"time\": time, \"lat\": lat, \"lon\": lon}\n    dataset_rechunked = dataset.chunk(chunks)\n    dataset_rechunked.to_netcdf(output_filepath)\n</code></pre>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk.nccopyBackend","title":"nccopyBackend","text":"<p>               Bases: <code>RechunkingBackendBase</code></p> <p>Methods:</p> Name Description <code>rechunk</code> <p>Options considered for <code>nccopy</code> :</p>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk.nccopyBackend.rechunk","title":"rechunk","text":"<pre><code>rechunk(\n    input_filepath: Path,\n    variables: List[str],\n    output_directory: Path,\n    time: Optional[int] = None,\n    latitude: Optional[int] = None,\n    longitude: Optional[int] = None,\n    fix_unlimited_dimensions: bool = False,\n    cache_size: Optional[int] = CACHE_SIZE_DEFAULT,\n    cache_elements: Optional[int] = CACHE_ELEMENTS_DEFAULT,\n    cache_preemption: Optional[\n        float\n    ] = CACHE_PREEMPTION_DEFAULT,\n    compression: str = COMPRESSION_FILTER_DEFAULT,\n    compression_level: int = COMPRESSION_LEVEL_DEFAULT,\n    shuffling: bool = SHUFFLING_DEFAULT,\n    memory: bool = RECHUNK_IN_MEMORY_DEFAULT,\n    dry_run: bool = False,\n)\n</code></pre> <p>Options considered for <code>nccopy</code> :  x  # deflate x  # shuffling x  # chunking sizes x Convert unlimited size input dimensions to fixed size output dimensions. May speed up variable-at-a-time access, but slow down record-at-a-time access. x  # read and process data in-memory, write out in the end x var1,...]   grp1,...]  x  # x  # Number of elements in cache   [x] infile [x] outfile</p> Source code in <code>rekx/rechunk.py</code> <pre><code>def rechunk(\n    self,\n    input_filepath: Path,\n    variables: List[str],\n    output_directory: Path,\n    time: Optional[int] = None,\n    latitude: Optional[int] = None,\n    longitude: Optional[int] = None,\n    fix_unlimited_dimensions: bool = False,\n    cache_size: Optional[int] = CACHE_SIZE_DEFAULT,\n    cache_elements: Optional[int] = CACHE_ELEMENTS_DEFAULT,\n    cache_preemption: Optional[float] = CACHE_PREEMPTION_DEFAULT,\n    compression: str = COMPRESSION_FILTER_DEFAULT,\n    compression_level: int = COMPRESSION_LEVEL_DEFAULT,\n    shuffling: bool = SHUFFLING_DEFAULT,\n    memory: bool = RECHUNK_IN_MEMORY_DEFAULT,\n    dry_run: bool = False,  # return command as a string ?\n):  # **kwargs):\n    \"\"\"\n    Options considered for ``nccopy`` :\n    [ ] [-k kind_name]\n    [ ] [-kind_code]\n    [x] [-d n]  # deflate\n    [x] [-s]  # shuffling\n    [x] [-c chunkspec]  # chunking sizes\n    [x] [-u] Convert unlimited size input dimensions to fixed size output dimensions. May speed up variable-at-a-time access, but slow down record-at-a-time access.\n    [x] [-w]  # read and process data in-memory, write out in the end\n    [x] [-[v|V] var1,...]\n    [ ] [-[g|G] grp1,...]\n    [ ] [-m bufsize]\n    [x] [-h chunk_cache]  #\n    [x] [-e cache_elems]  # Number of elements in cache\n    [ ] [-r]\n    [x] infile\n    [x] outfile\n    \"\"\"\n    variable_option = f\"-v {','.join(variables + [XarrayVariableSet.time])}\" if variables else \"\" # 'time' required\n    chunking_shape = (\n        f\"-c time/{time},lat/{latitude},lon/{longitude}\"\n        if all([time, latitude, longitude])\n        else \"\"\n    )\n    fixing_unlimited_dimensions = f\"-u\" if fix_unlimited_dimensions else \"\"\n    compression_options = f\"-d {compression_level}\" if compression == \"zlib\" else \"\"\n    shuffling_option = f\"-s\" if shuffling and compression_level &gt; 0 else \"\"\n    cache_size_option = f\"-h {cache_size} \" if cache_size else \"\"  # cache size in bytes\n    cache_elements_option = f\"-e {cache_elements}\" if cache_elements else \"\"\n    memory_option = f\"-w\" if memory else \"\"\n\n    # Collect all non-empty options into a list\n    options = [\n        variable_option,\n        chunking_shape,\n        fixing_unlimited_dimensions,\n        compression_options,\n        shuffling_option,\n        cache_size_option,\n        cache_elements_option,\n        memory_option,\n        input_filepath,\n    ]\n    # Build the command by joining non-empty options\n    command = \"nccopy \" + \" \".join(filter(bool, options))\n\n    # Build the output file path\n    output_filename = f\"{Path(input_filepath).stem}\"\n    output_filename += f\"_{time}\"\n    output_filename += f\"_{latitude}\"\n    output_filename += f\"_{longitude}\"\n    output_filename += f\"_{compression}\"\n    output_filename += f\"_{compression_level}\"\n    if shuffling and compression_level &gt; 0:\n        output_filename += f\"_shuffled\"\n    output_filename += f\"{Path(input_filepath).suffix}\"\n    output_directory.mkdir(parents=True, exist_ok=True)\n    output_filepath = output_directory / output_filename\n    command += f\"{output_filepath}\"\n\n    if dry_run:\n        return command\n\n    else:\n        args = shlex.split(command)\n        subprocess.run(args)\n</code></pre>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk.generate_rechunk_commands","title":"generate_rechunk_commands","text":"<pre><code>generate_rechunk_commands(\n    input_filepath: Path,\n    output: Path | None,\n    time: int | None,\n    latitude: int | None,\n    longitude: int | None,\n    fix_unlimited_dimensions: bool = FIX_UNLIMITED_DIMENSIONS_DEFAULT,\n    spatial_symmetry: bool = SPATIAL_SYMMETRY_DEFAULT,\n    variable_set: XarrayVariableSet = all,\n    cache_size: int = CACHE_SIZE_DEFAULT,\n    cache_elements: int = CACHE_ELEMENTS_DEFAULT,\n    cache_preemption: float = CACHE_PREEMPTION_DEFAULT,\n    compression: str = COMPRESSION_FILTER_DEFAULT,\n    compression_level: int = COMPRESSION_LEVEL_DEFAULT,\n    shuffling: bool = SHUFFLING_DEFAULT,\n    memory: bool = RECHUNK_IN_MEMORY_DEFAULT,\n    dask_scheduler: str = None,\n    commands_file: Path = \"rechunk_commands.txt\",\n    dry_run: bool = False,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n)\n</code></pre> <p>Generate variations of rechunking commands based on <code>nccopy</code>.</p> Source code in <code>rekx/rechunk.py</code> <pre><code>def generate_rechunk_commands(\n    input_filepath: Annotated[Path, typer.Argument(help=\"Input NetCDF file.\")],\n    output: Annotated[\n        Path | None, typer.Argument(help=\"Path to the output NetCDF file.\")\n    ],\n    time: Annotated[\n        int | None,\n        typer.Option(\n            help=\"New chunk size for the `time` dimension.\",\n            parser=parse_numerical_option,\n        ),\n    ],\n    latitude: Annotated[\n        int | None,\n        typer.Option(\n            help=\"New chunk size for the `lat` dimension.\",\n            parser=parse_numerical_option,\n        ),\n    ],\n    longitude: Annotated[\n        int | None,\n        typer.Option(\n            help=\"New chunk size for the `lon` dimension.\",\n            parser=parse_numerical_option,\n        ),\n    ],\n    fix_unlimited_dimensions: Annotated[\n        bool,\n        typer.Option(\n            help=\"Convert unlimited size input dimensions to fixed size dimensions in output.\"\n        ),\n    ] = FIX_UNLIMITED_DIMENSIONS_DEFAULT,\n    spatial_symmetry: Annotated[\n        bool,\n        typer.Option(\n            help=\"Add command only for identical latitude and longitude chunk sizes\"\n        ),\n    ] = SPATIAL_SYMMETRY_DEFAULT,\n    variable_set: Annotated[\n        XarrayVariableSet, typer.Option(help=\"Set of Xarray variables to diagnose\")\n    ] = XarrayVariableSet.all,\n    cache_size: Annotated[\n        int,\n        typer.Option(\n            help=\"Cache size\", show_default=True, parser=parse_numerical_option\n        ),\n    ] = CACHE_SIZE_DEFAULT,\n    cache_elements: Annotated[\n        int,\n        typer.Option(help=\"Number of elements in cache\", parser=parse_numerical_option),\n    ] = CACHE_ELEMENTS_DEFAULT,\n    cache_preemption: Annotated[\n        float,\n        typer.Option(\n            help=f\"Cache preemption strategy {NOT_IMPLEMENTED_CLI}\",\n            parser=parse_float_option,\n        ),\n    ] = CACHE_PREEMPTION_DEFAULT,\n    compression: Annotated[\n        str, typer.Option(help=\"Compression filter\", parser=parse_compression_filters)\n    ] = COMPRESSION_FILTER_DEFAULT,\n    compression_level: Annotated[\n        int, typer.Option(help=\"Compression level\", parser=parse_numerical_option)\n    ] = COMPRESSION_LEVEL_DEFAULT,\n    shuffling: Annotated[bool, typer.Option(help=f\"Shuffle... \")] = SHUFFLING_DEFAULT,\n    memory: Annotated[\n        bool, typer.Option(help=\"Use the -w flag to nccopy\")\n    ] = RECHUNK_IN_MEMORY_DEFAULT,\n    # backend: Annotated[RechunkingBackend, typer.Option(help=\"Backend to use for rechunking. [code]nccopy[/code] [red]Not Implemented Yet![/red]\")] = RechunkingBackend.nccopy,\n    dask_scheduler: Annotated[\n        str, typer.Option(help=\"The port:ip of the dask scheduler\")\n    ] = None,\n    commands_file: Path = \"rechunk_commands.txt\",\n    dry_run: Annotated[bool, typer_option_dry_run] = False,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n):\n    \"\"\"\n    Generate variations of rechunking commands based on `nccopy`.\n    \"\"\"\n    # Shuffling makes sense only along with compression\n    if any([level &gt; 0 for level in compression_level]) and shuffling:\n        shuffling = [shuffling, False]\n    else:\n        shuffling = [False]\n    with xr.open_dataset(input_filepath, engine=\"netcdf4\") as dataset:\n        selected_variables = select_xarray_variable_set_from_dataset(\n            XarrayVariableSet, variable_set, dataset\n        )\n        import itertools\n\n        commands = []\n        for (\n            chunking_time,\n            chunking_latitude,\n            chunking_longitude,\n            caching_size,\n            caching_elements,\n            caching_preemption,\n            compressing_filter,\n            compressing_level,\n            shuffling,\n        ) in itertools.product(\n            time,\n            latitude,\n            longitude,\n            cache_size,\n            cache_elements,\n            cache_preemption,\n            compression,\n            compression_level,\n            shuffling,\n        ):\n            backend = RechunkingBackend.nccopy.get_backend()  # hard-coded!\n            # Review Me ----------------------------------------------------\n            if spatial_symmetry and chunking_latitude != chunking_longitude:\n                continue\n            else:\n                command = backend.rechunk(\n                    input_filepath=input_filepath,\n                    variables=list(selected_variables),\n                    output_directory=output,\n                    time=chunking_time,\n                    latitude=chunking_latitude,\n                    longitude=chunking_longitude,\n                    fix_unlimited_dimensions=fix_unlimited_dimensions,\n                    cache_size=caching_size,\n                    cache_elements=caching_elements,\n                    cache_preemption=caching_preemption,\n                    compression=compressing_filter,\n                    compression_level=compressing_level,\n                    shuffling=shuffling,\n                    memory=memory,\n                    dry_run=True,  # just return the command!\n                )\n                if not command in commands:\n                    commands.append(command)\n\n    commands_file = Path(\n        commands_file.stem + \"_for_\" + Path(input_filepath).stem + commands_file.suffix\n    )\n    if verbose:\n        print(\n            f\"[bold]Writing generated commands into[/bold] [code]{commands_file}[/code]\"\n        )\n        for command in commands:\n            print(f\" [green]&gt;[/green] [code dim]{command}[/code dim]\")\n\n    if not dry_run:\n        with open(commands_file, \"w\") as f:\n            for command in commands:\n                f.write(command + \"\\n\")\n</code></pre>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk.generate_rechunk_commands_for_multiple_netcdf","title":"generate_rechunk_commands_for_multiple_netcdf","text":"<pre><code>generate_rechunk_commands_for_multiple_netcdf(\n    source_path: Path,\n    time: int,\n    latitude: int,\n    longitude: int,\n    fix_unlimited_dimensions: bool = FIX_UNLIMITED_DIMENSIONS_DEFAULT,\n    pattern: str = \"*.nc\",\n    output_directory: Path = Path(\".\"),\n    spatial_symmetry: bool = SPATIAL_SYMMETRY_DEFAULT,\n    variable_set: XarrayVariableSet = all,\n    cache_size: int = CACHE_SIZE_DEFAULT,\n    cache_elements: int = CACHE_ELEMENTS_DEFAULT,\n    cache_preemption: float = CACHE_PREEMPTION_DEFAULT,\n    compression: str = COMPRESSION_FILTER_DEFAULT,\n    compression_level: int = COMPRESSION_LEVEL_DEFAULT,\n    shuffling: bool = SHUFFLING_DEFAULT,\n    memory: bool = RECHUNK_IN_MEMORY_DEFAULT,\n    dask_scheduler: str = None,\n    commands_file: Path = \"rechunk_commands.txt\",\n    workers: int = 4,\n    dry_run: bool = False,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n)\n</code></pre> <p>Generate variations of rechunking commands based on <code>nccopy</code>.</p> Source code in <code>rekx/rechunk.py</code> <pre><code>def generate_rechunk_commands_for_multiple_netcdf(\n    source_path: Annotated[Path, typer_argument_source_path_with_pattern],\n    time: Annotated[\n        int,\n        typer.Option(\n            help=\"New chunk size for the `time` dimension.\",\n            parser=parse_numerical_option,\n        ),\n    ],\n    latitude: Annotated[\n        int,\n        typer.Option(\n            help=\"New chunk size for the `lat` dimension.\",\n            parser=parse_numerical_option,\n        ),\n    ],\n    longitude: Annotated[\n        int,\n        typer.Option(\n            help=\"New chunk size for the `lon` dimension.\",\n            parser=parse_numerical_option,\n        ),\n    ],\n    fix_unlimited_dimensions: Annotated[\n        bool,\n        typer.Option(\n            help=\"Convert unlimited size input dimensions to fixed size dimensions in output.\"\n        ),\n    ] = FIX_UNLIMITED_DIMENSIONS_DEFAULT,\n    pattern: Annotated[str, typer_option_filename_pattern] = \"*.nc\",\n    output_directory: Annotated[Path, typer_option_output_directory] = Path('.'),\n    spatial_symmetry: Annotated[\n        bool,\n        typer.Option(\n            help=\"Add command only for identical latitude and longitude chunk sizes\"\n        ),\n    ] = SPATIAL_SYMMETRY_DEFAULT,\n    variable_set: Annotated[\n        XarrayVariableSet, typer.Option(help=\"Set of Xarray variables to diagnose\")\n    ] = XarrayVariableSet.all,\n    cache_size: Annotated[\n        int,\n        typer.Option(\n            help=\"Cache size\", show_default=True, parser=parse_numerical_option\n        ),\n    ] = CACHE_SIZE_DEFAULT,\n    cache_elements: Annotated[\n        int,\n        typer.Option(help=\"Number of elements in cache\", parser=parse_numerical_option),\n    ] = CACHE_ELEMENTS_DEFAULT,\n    cache_preemption: Annotated[\n        float,\n        typer.Option(\n            help=f\"Cache preemption strategy {NOT_IMPLEMENTED_CLI}\",\n            parser=parse_float_option,\n        ),\n    ] = CACHE_PREEMPTION_DEFAULT,\n    compression: Annotated[\n        str, typer.Option(help=\"Compression filter\", parser=parse_compression_filters)\n    ] = COMPRESSION_FILTER_DEFAULT,\n    compression_level: Annotated[\n        int, typer.Option(help=\"Compression level\", parser=parse_numerical_option)\n    ] = COMPRESSION_LEVEL_DEFAULT,\n    shuffling: Annotated[\n        bool,\n        typer.Option(\n            help=f\"Shuffle... [reverse bold orange] Testing [/reverse bold orange]\"\n        ),\n    ] = SHUFFLING_DEFAULT,\n    memory: bool = RECHUNK_IN_MEMORY_DEFAULT,\n    # backend: Annotated[RechunkingBackend, typer.Option(help=\"Backend to use for rechunking. [code]nccopy[/code] [red]Not Implemented Yet![/red]\")] = RechunkingBackend.nccopy,\n    dask_scheduler: Annotated[\n        str, typer.Option(help=\"The port:ip of the dask scheduler\")\n    ] = None,\n    commands_file: Path = \"rechunk_commands.txt\",\n    workers: Annotated[int, typer.Option(help=\"Number of worker processes.\")] = 4,\n    dry_run: Annotated[bool, typer_option_dry_run] = False,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n):\n    \"\"\"\n    Generate variations of rechunking commands based on `nccopy`.\n    \"\"\"\n    input_file_paths = []\n    if source_path.is_file():\n        input_file_paths.append(source_path)\n        print(f\"[green]Identified the file in question![/green]\")\n\n    elif source_path.is_dir():\n        input_file_paths = list(str(path) for path in source_path.glob(pattern))\n\n    else:\n        print(f'Something is wrong with the [code]source_path[/code] input.')\n        return\n\n    if not list(input_file_paths):\n        print(\n            f\"No files found in [code]{source_path}[/code] matching the pattern [code]{pattern}[/code]!\"\n        )\n        return\n\n    if dry_run:\n        print(f\"[bold]Dry running operations that would be performed[/bold]:\")\n        print(f\"&gt; Reading files in [code]{source_path}[/code] matching the pattern [code]{pattern}[/code]\")\n        print(f\"&gt; Number of files matched : {len(list(input_file_paths))}\")\n        print(f\"&gt; Writing rechunking commands in [code]{commands_file}[/code]\")\n        return  # Exit for a dry run\n\n    if input_file_paths and not output_directory.exists():\n        output_directory.mkdir(parents=True, exist_ok=True)\n        if verbose &gt; 0:\n            print(f\"[yellow]Convenience action[/yellow] : creating the requested output directory [code]{output_directory}[/code].\")\n    with multiprocessing.Pool(processes=workers) as pool:\n        partial_generate_rechunk_commands = partial(\n                generate_rechunk_commands,\n                output=output_directory,\n                time=time,\n                latitude=latitude,\n                longitude=longitude,\n                fix_unlimited_dimensions=fix_unlimited_dimensions,\n                spatial_symmetry=spatial_symmetry,\n                variable_set=variable_set,\n                cache_size=cache_size,\n                cache_elements=cache_elements,\n                cache_preemption=cache_preemption,\n                compression=compression,\n                compression_level=compression_level,\n                shuffling=shuffling,\n                memory=memory,\n                dask_scheduler=dask_scheduler,\n                commands_file=commands_file,\n                dry_run=dry_run,\n                verbose=verbose,\n        )\n        pool.map(partial_generate_rechunk_commands, input_file_paths)\n    if verbose:\n        print(f\"[bold green]Done![/bold green]\")\n</code></pre>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk.modify_chunk_size","title":"modify_chunk_size","text":"<pre><code>modify_chunk_size(netcdf_file, variable, chunk_size)\n</code></pre> <p>Modify the chunk size of a variable in a NetCDF file.</p> <p>Parameters: - nc_file: path to the NetCDF file - variable_name: name of the variable to modify - new_chunk_size: tuple specifying the new chunk size, e.g., (2600, 2600)</p> Source code in <code>rekx/rechunk.py</code> <pre><code>def modify_chunk_size(\n    netcdf_file,\n    variable,\n    chunk_size,\n):\n    \"\"\"\n    Modify the chunk size of a variable in a NetCDF file.\n\n    Parameters:\n    - nc_file: path to the NetCDF file\n    - variable_name: name of the variable to modify\n    - new_chunk_size: tuple specifying the new chunk size, e.g., (2600, 2600)\n    \"\"\"\n    with nc.Dataset(netcdf_file, \"r+\") as dataset:\n        variable = dataset.variables[variable]\n\n        if variable.chunking() != [None]:\n            variable.set_auto_chunking(chunk_size)\n            print(\n                f\"Modified chunk size for variable '{variable}' in file '{netcdf_file}' to {chunk_size}.\"\n            )\n\n        else:\n            print(\n                f\"Variable '{variable}' in file '{netcdf_file}' is not chunked. Skipping.\"\n            )\n</code></pre>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/rechunk/#rekx.rechunk.rechunk","title":"rechunk","text":"<pre><code>rechunk(\n    input_filepath: Path,\n    output_directory: Optional[Path],\n    time: int,\n    latitude: int,\n    longitude: int,\n    fix_unlimited_dimensions: bool = FIX_UNLIMITED_DIMENSIONS_DEFAULT,\n    variable_set: XarrayVariableSet = all,\n    cache_size: Optional[int] = CACHE_SIZE_DEFAULT,\n    cache_elements: Optional[int] = CACHE_ELEMENTS_DEFAULT,\n    cache_preemption: Optional[\n        float\n    ] = CACHE_PREEMPTION_DEFAULT,\n    compression: str = COMPRESSION_FILTER_DEFAULT,\n    compression_level: int = COMPRESSION_LEVEL_DEFAULT,\n    shuffling: str = SHUFFLING_DEFAULT,\n    memory: bool = RECHUNK_IN_MEMORY_DEFAULT,\n    dry_run: bool = DRY_RUN_DEFAULT,\n    backend: RechunkingBackend = nccopy,\n    dask_scheduler: str = None,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n)\n</code></pre> <p>Rechunk a NetCDF4 dataset with options to fine tune the output</p> Source code in <code>rekx/rechunk.py</code> <pre><code>def rechunk(\n    input_filepath: Annotated[Path, typer.Argument(help=\"Input NetCDF file.\")],\n    output_directory: Annotated[\n        Optional[Path], typer.Argument(help=\"Path to the output NetCDF file.\")\n    ],\n    time: Annotated[int, typer.Option(help=\"New chunk size for the `time` dimension.\")],\n    latitude: Annotated[\n        int, typer.Option(help=\"New chunk size for the `lat` dimension.\")\n    ],\n    longitude: Annotated[\n        int, typer.Option(help=\"New chunk size for the `lon` dimension.\")\n    ],\n    fix_unlimited_dimensions: Annotated[\n        bool, typer.Option(help=\"Convert unlimited size input dimensions to fixed size dimensions in output.\")\n    ] = FIX_UNLIMITED_DIMENSIONS_DEFAULT,\n    variable_set: Annotated[\n        XarrayVariableSet, typer.Option(help=\"Set of Xarray variables to diagnose\")\n    ] = XarrayVariableSet.all,\n    cache_size: Optional[int] = CACHE_SIZE_DEFAULT,\n    cache_elements: Optional[int] = CACHE_ELEMENTS_DEFAULT,\n    cache_preemption: Optional[float] = CACHE_PREEMPTION_DEFAULT,\n    compression: str = COMPRESSION_FILTER_DEFAULT,\n    compression_level: int = COMPRESSION_LEVEL_DEFAULT,\n    shuffling: str = SHUFFLING_DEFAULT,\n    memory: bool = RECHUNK_IN_MEMORY_DEFAULT,\n    dry_run: Annotated[bool, typer_option_dry_run] = DRY_RUN_DEFAULT,\n    backend: Annotated[\n        RechunkingBackend,\n        typer.Option(\n            help=\"Backend to use for rechunking. [code]nccopy[/code] [red]Not Implemented Yet![/red]\"\n        ),\n    ] = RechunkingBackend.nccopy,\n    dask_scheduler: Annotated[\n        str, typer.Option(help=\"The port:ip of the dask scheduler\")\n    ] = None,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n):\n    \"\"\"\n    Rechunk a NetCDF4 dataset with options to fine tune the output\n    \"\"\"\n    if verbose:\n        import time as timer\n\n        rechunking_timer_start = timer.time()\n\n    # if dask_scheduler:\n    #     from dask.distributed import Client\n    #     client = Client(dask_scheduler)\n    #     typer.echo(f\"Using Dask scheduler at {dask_scheduler}\")\n\n    with xr.open_dataset(input_filepath, engine=\"netcdf4\") as dataset:\n        # with Dataset(input, 'r') as dataset:\n        selected_variables = select_xarray_variable_set_from_dataset(\n            XarrayVariableSet, variable_set, dataset\n        )\n        rechunk_parameters = {\n            \"input\": input,\n            \"variables\": selected_variables,\n            \"output_directory\": output_directory,\n            \"time\": time,\n            \"latitude\": latitude,\n            \"longitude\": longitude,\n            \"fix_unlimited_dimensions\": fix_unlimited_dimensions,\n            \"cache_size\": cache_size,\n            \"cache_elements\": cache_elements,\n            \"cache_preemption\": cache_preemption,\n            \"shuffling\": shuffling,\n            \"compression\": compression,\n            \"compression_level\": compression_level,\n            \"memory\": memory,\n        }\n        backend = backend.get_backend()\n        command = backend.rechunk(**rechunk_parameters, dry_run=dry_run)\n        if dry_run:\n            print(\n                f\"[bold]Dry run[/bold] the [bold]following command that would be executed[/bold]:\"\n            )\n            print(f\"    {command}\")\n            # print(f\"    {rechunk_parameters}\")\n            return  # Exit for a dry run\n\n        else:\n            command_arguments = shlex.split(command)\n            try:\n                subprocess.run(command_arguments, check=True)\n                print(f\"Command {command} executed successfully.\")\n            except subprocess.CalledProcessError as e:\n                print(f\"An error occurred while executing the command: {e}\")\n\n        if verbose:\n            rechunking_timer_end = timer.time()\n            elapsed_time = rechunking_timer_end - rechunking_timer_start\n            logger.debug(f\"Rechunking via {backend} took {elapsed_time:.2f} seconds\")\n            print(f\"Rechunking took {elapsed_time:.2f} seconds.\")\n</code></pre>","tags":["rekx","Reference","CLI","Tools","Rechunk"]},{"location":"cli/select/","title":"Select","text":"","tags":["rekx","CLI","Reference","Tools","select","statistics","CSV"]},{"location":"cli/select/#rekx.select","title":"select","text":"<p>Functions:</p> Name Description <code>select_fast</code> <p>Bare timing to read data over a location and optionally write</p> <code>select_time_series</code> <p>Select data using a Kerchunk reference file</p> <code>select_time_series_from_json</code> <p>Select data using a Kerchunk reference file</p>","tags":["rekx","CLI","Reference","Tools","select","statistics","CSV"]},{"location":"cli/select/#rekx.select.select_fast","title":"select_fast","text":"<pre><code>select_fast(\n    time_series: Path,\n    variable: str,\n    longitude: float,\n    latitude: float,\n    time_series_2: Path = None,\n    tolerance: Optional[float] = 0.1,\n    csv: Path = None,\n    tocsv: Path = None,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n)\n</code></pre> <p>Bare timing to read data over a location and optionally write comma-separated values.</p> <p>Parameters:</p> Name Type Description Default <code>time_series</code> <code>Path</code> <p>Path to Xarray-supported input file</p> required <code>variable</code> <code>str</code> <p>Name of the variable to query</p> required <code>longitude</code> <code>float</code> <p>The longitude of the location to read data</p> required <code>latitude</code> <code>float</code> <p>The latitude of the location to read data</p> required <code>time_series</code> <code>Path</code> <p>Path to second Xarray-supported input file</p> required <code>tolerance</code> <code>Optional[float]</code> <p>Maximum distance between original and new labels for inexact matches. Read Xarray manual on nearest-neighbor-lookups</p> <code>0.1</code> <code>csv</code> <code>Path</code> <p>CSV output filename</p> <code>None</code> <code>to_csv</code> <p>CSV output filename (fast implementation from xarray-extras)</p> required <p>Returns:</p> Name Type Description <code>data_retrieval_time</code> <code>float</code> <p>An estimation of the time it took to retrieve data over the requested location if no verbosity is asked.</p> Notes <p><code>mask_and_scale</code> is always set to <code>False</code> to avoid errors related with decoding timestamps.</p> Source code in <code>rekx/select.py</code> <pre><code>def select_fast(\n    time_series: Annotated[Path, typer_argument_time_series],\n    variable: Annotated[str, typer.Argument(help=\"Variable to select data from\")],\n    longitude: Annotated[float, typer_argument_longitude_in_degrees],\n    latitude: Annotated[float, typer_argument_latitude_in_degrees],\n    time_series_2: Annotated[Path, typer_option_time_series] = None,\n    tolerance: Annotated[\n        Optional[float], typer_option_tolerance\n    ] = 0.1,  # Customize default if needed\n    # in_memory: Annotated[bool, typer_option_in_memory] = False,\n    csv: Annotated[Path, typer_option_csv] = None,\n    tocsv: Annotated[Path, typer_option_csv] = None,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n):\n    \"\"\"Bare timing to read data over a location and optionally write\n    comma-separated values.\n\n    Parameters\n    ----------\n    time_series:\n        Path to Xarray-supported input file\n    variable: str\n        Name of the variable to query\n    longitude: float\n        The longitude of the location to read data\n    latitude: float\n        The latitude of the location to read data\n    time_series:\n        Path to second Xarray-supported input file\n    tolerance: float\n        Maximum distance between original and new labels for inexact matches.\n        Read Xarray manual on nearest-neighbor-lookups\n    csv:\n        CSV output filename\n    to_csv:\n        CSV output filename (fast implementation from xarray-extras)\n\n    Returns\n    -------\n    data_retrieval_time : float\n        An estimation of the time it took to retrieve data over the requested\n        location if no verbosity is asked.\n    Notes\n    -----\n    ``mask_and_scale`` is always set to ``False`` to avoid errors related with\n    decoding timestamps.\n\n    \"\"\"\n    try:\n        data_retrieval_start_time = timer.perf_counter()  # time()\n        series = xr.open_dataset(time_series, mask_and_scale=False)[variable].sel(\n            lon=longitude, lat=latitude, method=\"nearest\"\n        )\n        if time_series_2:\n            series_2 = xr.open_dataset(time_series_2, mask_and_scale=False)[\n                variable\n            ].sel(lon=longitude, lat=latitude, method=\"nearest\")\n        if csv:\n            series.to_pandas().to_csv(csv)\n            if time_series_2:\n                series_2.to_pandas().to_csv(csv.name + \"2\")\n        elif tocsv:\n            to_csv(\n                x=series,\n                path=str(tocsv),\n            )\n            if time_series_2:\n                to_csv(x=series_2, path=str(tocsv) + \"2\")\n\n        data_retrieval_time = f\"{timer.perf_counter() - data_retrieval_start_time:.3f}\"\n        if not verbose:\n            return data_retrieval_time\n        else:\n            print(\n                f\"[bold green]It worked[/bold green] and took : {data_retrieval_time}\"\n            )\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n</code></pre>","tags":["rekx","CLI","Reference","Tools","select","statistics","CSV"]},{"location":"cli/select/#rekx.select.select_time_series","title":"select_time_series","text":"<pre><code>select_time_series(\n    time_series: Path,\n    variable: str,\n    longitude: float,\n    latitude: float,\n    list_variables: bool = False,\n    timestamps: Optional[Any] = None,\n    start_time: Optional[datetime] = None,\n    end_time: Optional[datetime] = None,\n    time: Optional[int] = None,\n    lat: Optional[int] = None,\n    lon: Optional[int] = None,\n    mask_and_scale: bool = False,\n    neighbor_lookup: MethodForInexactMatches = nearest,\n    tolerance: Optional[float] = 0.1,\n    in_memory: bool = False,\n    statistics: bool = False,\n    output_filename: Path | None = None,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n) -&gt; None\n</code></pre> <p>Select data using a Kerchunk reference file</p> <p>Parameters:</p> Name Type Description Default <code>time_series</code> <code>Path</code> <p>Path to Xarray-supported input file</p> required <code>variable</code> <code>str</code> <p>Name of the variable to query</p> required <code>longitude</code> <code>float</code> <p>The longitude of the location to read data</p> required <code>latitude</code> <code>float</code> <p>The latitude of the location to read data</p> required <code>list_variables</code> <code>bool</code> <p>Optional flag to list data variables and exit without doing anything  else.</p> <code>False</code> <code>timestamps</code> <code>Optional[Any]</code> <p>A string of properly formatted timestamps to be parsed and use for temporal selection.</p> <code>None</code> <code>start_time</code> <code>Optional[datetime]</code> <p>A start time to generate a temporal selection period</p> <code>None</code> <code>end_time</code> <code>Optional[datetime]</code> <p>An end time for the generation of a temporal selection period</p> <code>None</code> <code>time</code> <code>Optional[int]</code> <p>New chunk size for the 'time' dimension</p> <code>None</code> <code>lat</code> <code>Optional[int]</code> <p>New chunk size for the 'lat' dimension</p> <code>None</code> <code>lon</code> <code>Optional[int]</code> <p>New chunk size for the 'lon' dimension</p> <code>None</code> <code>mask_and_scale</code> <code>bool</code> <p>Flag to apply masking and scaling based on the input metadata</p> <code>False</code> <code>neighbor_lookup</code> <code>MethodForInexactMatches</code> <p>Method to use for inexact matches.</p> <code>nearest</code> <code>tolerance</code> <code>Optional[float]</code> <p>Maximum distance between original and new labels for inexact matches. Read Xarray manual on nearest-neighbor-lookups</p> <code>0.1</code> <code>statistics</code> <code>bool</code> <p>Optional flag to calculate and display summary statistics</p> <code>False</code> <code>verbose</code> <code>int</code> <p>Verbosity level</p> <code>VERBOSE_LEVEL_DEFAULT</code> Source code in <code>rekx/select.py</code> <pre><code>def select_time_series(\n    time_series: Path,\n    variable: Annotated[str, typer.Argument(..., help=\"Variable name to select from\")],\n    longitude: Annotated[float, typer_argument_longitude_in_degrees],\n    latitude: Annotated[float, typer_argument_latitude_in_degrees],\n    list_variables: Annotated[bool, typer_option_list_variables] = False,\n    timestamps: Annotated[Optional[Any], typer_argument_timestamps] = None,\n    start_time: Annotated[Optional[datetime], typer_option_start_time] = None,\n    end_time: Annotated[Optional[datetime], typer_option_end_time] = None,\n    time: Annotated[\n        Optional[int], typer.Option(help=\"New chunk size for the 'time' dimension\")\n    ] = None,\n    lat: Annotated[\n        Optional[int], typer.Option(help=\"New chunk size for the 'lat' dimension\")\n    ] = None,\n    lon: Annotated[\n        Optional[int], typer.Option(help=\"New chunk size for the 'lon' dimension\")\n    ] = None,\n    # convert_longitude_360: Annotated[bool, typer_option_convert_longitude_360] = False,\n    mask_and_scale: Annotated[bool, typer_option_mask_and_scale] = False,\n    neighbor_lookup: Annotated[\n        MethodForInexactMatches, typer_option_neighbor_lookup\n    ] = MethodForInexactMatches.nearest,\n    tolerance: Annotated[\n        Optional[float], typer_option_tolerance\n    ] = 0.1,  # Customize default if needed\n    in_memory: Annotated[bool, typer_option_in_memory] = False,\n    statistics: Annotated[bool, typer_option_statistics] = False,\n    output_filename: Annotated[\n        Path|None, typer_option_output_filename\n    ] = None,\n    # output_filename: Annotated[Path, typer_option_output_filename] = 'series_in',  #Path(),\n    # variable_name_as_suffix: Annotated[bool, typer_option_variable_name_as_suffix] = True,\n    # rounding_places: Annotated[Optional[int], typer_option_rounding_places] = ROUNDING_PLACES_DEFAULT,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n) -&gt; None:\n    \"\"\"\n    Select data using a Kerchunk reference file\n\n    Parameters\n    ----------\n    time_series:\n        Path to Xarray-supported input file\n    variable: str\n        Name of the variable to query\n    longitude: float\n        The longitude of the location to read data\n    latitude: float\n        The latitude of the location to read data\n    list_variables: bool\n         Optional flag to list data variables and exit without doing anything\n         else.\n    timestamps: str\n        A string of properly formatted timestamps to be parsed and use for\n        temporal selection.\n    start_time: str\n        A start time to generate a temporal selection period\n    end_time: str\n        An end time for the generation of a temporal selection period\n    time: int\n        New chunk size for the 'time' dimension\n    lat: int\n        New chunk size for the 'lat' dimension\n    lon: int\n        New chunk size for the 'lon' dimension\n    mask_and_scale: bool\n        Flag to apply masking and scaling based on the input metadata\n    neighbor_lookup: str\n        Method to use for inexact matches.\n    tolerance: float\n        Maximum distance between original and new labels for inexact matches.\n        Read Xarray manual on nearest-neighbor-lookups\n    statistics: bool\n        Optional flag to calculate and display summary statistics\n    verbose: int\n        Verbosity level\n\n    Returns\n    -------\n\n    \"\"\"\n    # if convert_longitude_360:\n    #     longitude = longitude % 360\n    # warn_for_negative_longitude(longitude)\n\n    logger.debug(f\"Command context : {typer.Context}\")\n\n    data_retrieval_start_time = timer.time()\n    logger.debug(f\"Starting data retrieval... {data_retrieval_start_time}\")\n\n    timer_start = timer.time()\n    dataset = xr.open_dataset(\n        time_series,\n        mask_and_scale=mask_and_scale,\n    )  # is a dataset\n    timer_end = timer.time()\n    logger.debug(\n        f\"Dataset opening via Xarray took {timer_end - timer_start:.2f} seconds\"\n    )\n\n    available_variables = list(dataset.data_vars)  # Is there a faster way ?\n    if list_variables:\n        logger.info(\n            f\"The dataset contains the following variables : `{available_variables}`.\"\n        )\n        print(\n            f\"The dataset contains the following variables : `{available_variables}`.\"\n        )\n        return\n\n    if not variable in available_variables:\n        logger.debug(\n            f\"The requested variable `{variable}` does not exist! Plese select one among the available variables : {available_variables}.\"\n        )\n        print(\n            f\"The requested variable `{variable}` does not exist! Plese select one among the available variables : {available_variables}.\"\n        )\n        raise typer.Exit(code=0)\n    else:\n        timer_start = timer.time()\n        time_series = dataset[variable]\n        timer_end = timer.time()\n        logger.debug(\n            f\"Data array variable selection took {timer_end - timer_start:.2f} seconds\"\n        )\n\n        timer_start = timer.time()\n        chunks = {\"time\": time, \"lat\": lat, \"lon\": lon}\n        time_series.chunk(chunks=chunks)\n        timer_end = timer.time()\n        logger.debug(\n            f\"Data array rechunking took {timer_end - timer_start:.2f} seconds\"\n        )\n\n    timer_start = timer.time()\n    indexers = set_location_indexers(\n        data_array=time_series,\n        longitude=longitude,\n        latitude=latitude,\n        verbose=verbose,\n    )\n    timer_end = timer.time()\n    logger.debug(f\"Data array indexers : {indexers}\")\n    logger.debug(\n        f\"Data array indexers setting took {timer_end - timer_start:.2f} seconds\"\n    )\n\n    try:\n        timer_start = timer.time()\n        location_time_series = time_series.sel(\n            **indexers,\n            method=neighbor_lookup,\n            tolerance=tolerance,\n        )\n        timer_end = timer.time()\n        indentation = \" \" * 4 * 9\n        indented_location_time_series = \"\\n\".join(\n            indentation + line for line in str(location_time_series).split(\"\\n\")\n        )\n        logger.debug(\n            f\"Location time series selection :\\n{indented_location_time_series}\"\n        )\n        logger.debug(f\"Location selection took {timer_end - timer_start:.2f} seconds\")\n\n        if in_memory:\n            timer_start = timer.time()\n            location_time_series.load()  # load into memory for faster ... ?\n            timer_end = timer.time()\n            logger.debug(\n                f\"Location selection loading in memory took {timer_end - timer_start:.2f} seconds\"\n            )\n\n    except Exception as exception:\n        logger.error(f\"{ERROR_IN_SELECTING_DATA} : {exception}\")\n        print(f\"{ERROR_IN_SELECTING_DATA} : {exception}\")\n        raise SystemExit(33)\n    # ------------------------------------------------------------------------\n\n    if start_time or end_time:\n        timestamps = None  # we don't need a timestamp anymore!\n\n        if start_time and not end_time:  # set `end_time` to end of series\n            end_time = location_time_series.time.values[-1]\n\n        elif end_time and not start_time:  # set `start_time` to beginning of series\n            start_time = location_time_series.time.values[0]\n\n        else:  # Convert `start_time` &amp; `end_time` to the correct string format\n            start_time = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n            end_time = end_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        timer_start = timer.time()\n        location_time_series = location_time_series.sel(\n            time=slice(start_time, end_time)\n        )\n        timer_end = timer.time()\n        logger.debug(\n            f\"Time slicing with `start_time` and `end_time` took {timer_end - timer_start:.2f} seconds\"\n        )\n\n    if timestamps is not None and not start_time and not end_time:\n        if len(timestamps) == 1:\n            start_time = end_time = timestamps[0]\n\n        try:\n            timer_start = timer.time()\n            location_time_series = location_time_series.sel(\n                time=timestamps, method=neighbor_lookup\n            )\n            timer_end = timer.time()\n            logger.debug(\n                f\"Time selection with `timestamps` took {timer_end - timer_start:.2f} seconds\"\n            )\n\n        except KeyError:\n            print(f\"No data found for one or more of the given {timestamps}.\")\n\n    if location_time_series.size == 1:\n        timer_start = timer.time()\n        single_value = float(location_time_series.values)\n        warning = (\n            f\"{exclamation_mark} The selected timestamp \"\n            + f\"{location_time_series.time.values}\"\n            + f\" matches the single value \"\n            + f\"{single_value}\"\n        )\n        timer_end = timer.time()\n        logger.debug(\n            f\"Single value conversion to float took {timer_end - timer_start:.2f} seconds\"\n        )\n        logger.warning(warning)\n        if verbose &gt; 0:\n            print(warning)\n\n    data_retrieval_end_time = timer.time()\n    logger.debug(\n        f\"Data retrieval took {data_retrieval_end_time - data_retrieval_start_time:.2f} seconds\"\n    )\n\n    if not verbose:\n        print(location_time_series.values)\n    else:\n        print(location_time_series)\n\n    if statistics:  # after echoing series which might be Long!\n        print_series_statistics(\n            data_array=location_time_series,\n            title=\"Selected series\",\n        )\n    output_handlers = {\n        \".nc\": lambda location_time_series, path: write_to_netcdf(\n            location_time_series=location_time_series,\n            path=path,\n            longitude=longitude,\n            latitude=latitude\n        ),\n        \".csv\": lambda location_time_series, path: to_csv(\n            x=location_time_series, path=path\n        ),\n    }\n    if output_filename:\n        extension = output_filename.suffix.lower()\n        if extension in output_handlers:\n            output_handlers[extension](location_time_series, output_filename)\n        else:\n            raise ValueError(f\"Unsupported file extension: {extension}\")\n</code></pre>","tags":["rekx","CLI","Reference","Tools","select","statistics","CSV"]},{"location":"cli/select/#rekx.select.select_time_series_from_json","title":"select_time_series_from_json","text":"<pre><code>select_time_series_from_json(\n    reference_file: Path,\n    variable: str,\n    longitude: float,\n    latitude: float,\n    list_variables: bool = False,\n    timestamps: Optional[Any] = None,\n    start_time: Optional[datetime] = None,\n    end_time: Optional[datetime] = None,\n    time: Optional[int] = None,\n    lat: Optional[int] = None,\n    lon: Optional[int] = None,\n    mask_and_scale: bool = False,\n    neighbor_lookup: MethodForInexactMatches = None,\n    tolerance: Optional[float] = 0.1,\n    in_memory: bool = False,\n    statistics: bool = False,\n    csv: Path = None,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n) -&gt; None\n</code></pre> <p>Select data using a Kerchunk reference file</p> <p>Parameters:</p> Name Type Description Default <code>reference_file</code> <code>Path</code> <p>Path to an input JSON Kerchunk reference file</p> required <code>variable</code> <code>str</code> <p>Name of the variable to query</p> required <code>longitude</code> <code>float</code> <p>The longitude of the location to read data</p> required <code>latitude</code> <code>float</code> <p>The latitude of the location to read data</p> required <code>list_variables</code> <code>bool</code> <p>Optional flag to list data variables and exit without doing anything  else.</p> <code>False</code> <code>timestamps</code> <code>Optional[Any]</code> <p>A string of properly formatted timestamps to be parsed and use for temporal selection.</p> <code>None</code> <code>start_time</code> <code>Optional[datetime]</code> <p>A start time to generate a temporal selection period</p> <code>None</code> <code>end_time</code> <code>Optional[datetime]</code> <p>An end time for the generation of a temporal selection period</p> <code>None</code> <code>time</code> <code>Optional[int]</code> <p>New chunk size for the 'time' dimension</p> <code>None</code> <code>lat</code> <code>Optional[int]</code> <p>New chunk size for the 'lat' dimension</p> <code>None</code> <code>lon</code> <code>Optional[int]</code> <p>New chunk size for the 'lon' dimension</p> <code>None</code> <code>mask_and_scale</code> <code>bool</code> <p>Flag to apply masking and scaling based on the input metadata</p> <code>False</code> <code>neighbor_lookup</code> <code>MethodForInexactMatches</code> <p>Method to use for inexact matches.</p> <code>None</code> <code>tolerance</code> <code>Optional[float]</code> <p>Maximum distance between original and new labels for inexact matches. Read Xarray manual on nearest-neighbor-lookups</p> <code>0.1</code> <code>in_memory</code> <code>bool</code> <p>?</p> <code>False</code> <code>statistics</code> <code>bool</code> <p>Optional flag to calculate and display summary statistics</p> <code>False</code> <code>csv</code> <code>Path</code> <p>CSV output filename</p> <code>None</code> <code>verbose</code> <code>int</code> <p>Verbosity level</p> <code>VERBOSE_LEVEL_DEFAULT</code> Source code in <code>rekx/select.py</code> <pre><code>def select_time_series_from_json(\n    reference_file: Annotated[\n        Path, typer.Argument(..., help=\"Path to the kerchunk reference file\")\n    ],\n    variable: Annotated[str, typer.Argument(..., help=\"Variable name to select from\")],\n    longitude: Annotated[float, typer_argument_longitude_in_degrees],\n    latitude: Annotated[float, typer_argument_latitude_in_degrees],\n    list_variables: Annotated[bool, typer_option_list_variables] = False,\n    timestamps: Annotated[Optional[Any], typer_argument_timestamps] = None,\n    start_time: Annotated[Optional[datetime], typer_option_start_time] = None,\n    end_time: Annotated[Optional[datetime], typer_option_end_time] = None,\n    time: Annotated[\n        Optional[int], typer.Option(help=\"New chunk size for the 'time' dimension\")\n    ] = None,\n    lat: Annotated[\n        Optional[int], typer.Option(help=\"New chunk size for the 'lat' dimension\")\n    ] = None,\n    lon: Annotated[\n        Optional[int], typer.Option(help=\"New chunk size for the 'lon' dimension\")\n    ] = None,\n    # convert_longitude_360: Annotated[bool, typer_option_convert_longitude_360] = False,\n    mask_and_scale: Annotated[bool, typer_option_mask_and_scale] = False,\n    neighbor_lookup: Annotated[\n        MethodForInexactMatches, typer_option_neighbor_lookup\n    ] = None,\n    tolerance: Annotated[\n        Optional[float], typer_option_tolerance\n    ] = 0.1,  # Customize default if needed\n    in_memory: Annotated[bool, typer_option_in_memory] = False,\n    statistics: Annotated[bool, typer_option_statistics] = False,\n    csv: Annotated[Path, typer_option_csv] = None,\n    # output_filename: Annotated[Path, typer_option_output_filename] = 'series_in',  #Path(),\n    # variable_name_as_suffix: Annotated[bool, typer_option_variable_name_as_suffix] = True,\n    # rounding_places: Annotated[Optional[int], typer_option_rounding_places] = ROUNDING_PLACES_DEFAULT,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n) -&gt; None:\n    \"\"\"\n    Select data using a Kerchunk reference file\n\n    Parameters\n    ----------\n    reference_file:\n        Path to an input JSON Kerchunk reference file\n    variable: str\n        Name of the variable to query\n    longitude: float\n        The longitude of the location to read data\n    latitude: float\n        The latitude of the location to read data\n    list_variables: bool\n         Optional flag to list data variables and exit without doing anything\n         else.\n    timestamps: str\n        A string of properly formatted timestamps to be parsed and use for\n        temporal selection.\n    start_time: str\n        A start time to generate a temporal selection period\n    end_time: str\n        An end time for the generation of a temporal selection period\n    time: int\n        New chunk size for the 'time' dimension\n    lat: int\n        New chunk size for the 'lat' dimension\n    lon: int\n        New chunk size for the 'lon' dimension\n    mask_and_scale: bool\n        Flag to apply masking and scaling based on the input metadata\n    neighbor_lookup: str\n        Method to use for inexact matches.\n    tolerance: float\n        Maximum distance between original and new labels for inexact matches.\n        Read Xarray manual on nearest-neighbor-lookups\n    in_memory: bool\n        ?\n    statistics: bool\n        Optional flag to calculate and display summary statistics\n    csv:\n        CSV output filename\n    verbose: int\n        Verbosity level\n    \"\"\"\n    # if convert_longitude_360:\n    #     longitude = longitude % 360\n    # warn_for_negative_longitude(longitude)\n\n    # logger.debug(f'Command context : {print(typer.Context)}')\n\n    data_retrieval_start_time = timer.time()\n    logger.debug(f\"Starting data retrieval... {data_retrieval_start_time}\")\n\n    timer_start = timer.time()\n    mapper = fsspec.get_mapper(\n        \"reference://\",\n        fo=str(reference_file),\n        remote_protocol=\"file\",\n        remote_options={\"skip_instance_cache\": True},\n    )\n    timer_end = timer.time()\n    logger.debug(f\"Mapper creation took {timer_end - timer_start:.2f} seconds\")\n    timer_start = timer.time()\n    dataset = xr.open_dataset(\n        mapper,\n        engine=\"zarr\",\n        backend_kwargs={\"consolidated\": False},\n        chunks=None,\n        mask_and_scale=mask_and_scale,\n    )  # is a dataset\n    timer_end = timer.time()\n    logger.debug(\n        f\"Dataset opening via Xarray took {timer_end - timer_start:.2f} seconds\"\n    )\n\n    available_variables = list(dataset.data_vars)  # Is there a faster way ?\n    if list_variables:\n        print(\n            f\"The dataset contains the following variables : `{available_variables}`.\"\n        )\n        return\n\n    if not variable in available_variables:\n        logger.error(\n            f\"The requested variable `{variable}` does not exist! Plese select one among the available variables : {available_variables}.\"\n        )\n        print(\n            f\"The requested variable `{variable}` does not exist! Plese select one among the available variables : {available_variables}.\"\n        )\n        raise typer.Exit(code=0)\n    else:\n        # variable\n        timer_start = timer.time()\n        time_series = dataset[variable]\n        timer_end = timer.time()\n        logger.debug(\n            f\"Data array variable selection took {timer_end - timer_start:.2f} seconds\"\n        )\n\n        # chunking\n        timer_start = timer.time()\n        chunks = {\"time\": time, \"lat\": lat, \"lon\": lon}\n        time_series.chunk(chunks=chunks)\n        timer_end = timer.time()\n        logger.debug(\n            f\"Data array rechunking took {timer_end - timer_start:.2f} seconds\"\n        )\n\n        # ReviewMe --------------------------------------------------------- ?\n        # in-memory\n        if in_memory:\n            timer_start = timer.time()\n            location_time_series.load()  # load into memory for faster ... ?\n            timer_end = timer.time()\n            logger.debug(\n                f\"Location selection loading in memory took {timer_end - timer_start:.2f} seconds\"\n            )\n        # --------------------------------------------------------------------\n\n    timer_start = timer.time()\n    indexers = set_location_indexers(\n        data_array=time_series,\n        longitude=longitude,\n        latitude=latitude,\n        verbose=verbose,\n    )\n    timer_end = timer.time()\n    logger.debug(\n        f\"Data array indexers setting took {timer_end - timer_start:.2f} seconds\"\n    )\n\n    try:\n        timer_start = timer.time()\n        location_time_series = time_series.sel(\n            **indexers,\n            method=neighbor_lookup,\n            tolerance=tolerance,\n        )\n        timer_end = timer.time()\n        logger.debug(f\"Location selection took {timer_end - timer_start:.2f} seconds\")\n\n        # in-memory\n        if in_memory:\n            timer_start = timer.time()\n            location_time_series.load()  # load into memory for faster ... ?\n            timer_end = timer.time()\n            logger.debug(\n                f\"Location selection loading in memory took {timer_end - timer_start:.2f} seconds\"\n            )\n\n    except Exception as exception:\n        logger.error(f\"{ERROR_IN_SELECTING_DATA} : {exception}\")\n        print(f\"{ERROR_IN_SELECTING_DATA} : {exception}\")\n        raise SystemExit(33)\n    # ------------------------------------------------------------------------\n\n    if start_time or end_time:\n        timestamps = None  # we don't need a timestamp anymore!\n\n        if start_time and not end_time:  # set `end_time` to end of series\n            end_time = location_time_series.time.values[-1]\n\n        elif end_time and not start_time:  # set `start_time` to beginning of series\n            start_time = location_time_series.time.values[0]\n\n        else:  # Convert `start_time` &amp; `end_time` to the correct string format\n            start_time = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n            end_time = end_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        timer_start = timer.time()\n        location_time_series = location_time_series.sel(\n            time=slice(start_time, end_time)\n        )\n        timer_end = timer.time()\n        logger.debug(\n            f\"Time slicing with `start_time` and `end_time` took {timer_end - timer_start:.2f} seconds\"\n        )\n\n    if timestamps is not None and not start_time and not end_time:\n        if len(timestamps) == 1:\n            start_time = end_time = timestamps[0]\n\n        try:\n            timer_start = timer.time()\n            location_time_series = location_time_series.sel(\n                time=timestamps, method=neighbor_lookup\n            )\n            timer_end = timer.time()\n            logger.debug(\n                f\"Time selection with `timestamps` took {timer_end - timer_start:.2f} seconds\"\n            )\n\n        except KeyError:\n            logger.error(f\"No data found for one or more of the given {timestamps}.\")\n            print(f\"No data found for one or more of the given {timestamps}.\")\n\n    if location_time_series.size == 1:\n        timer_start = timer.time()\n        single_value = float(location_time_series.values)\n        warning = (\n            f\"{exclamation_mark} The selected timestamp \"\n            + f\"{location_time_series.time.values}\"\n            + f\" matches the single value \"\n            + f\"{single_value}\"\n        )\n        timer_end = timer.time()\n        logger.debug(\n            f\"Single value conversion to float took {timer_end - timer_start:.2f} seconds\"\n        )\n        logger.warning(warning)\n        if verbose &gt; 0:\n            print(warning)\n\n    data_retrieval_end_time = timer.time()\n    logger.debug(\n        f\"Data retrieval took {data_retrieval_end_time - data_retrieval_start_time:.2f} seconds\"\n    )\n\n    if not verbose:\n        print(location_time_series.values)\n    else:\n        print(location_time_series)\n\n    # special case!\n    if location_time_series is not None and timestamps is None:\n        timestamps = location_time_series.time.to_numpy()\n\n    if statistics:  # after echoing series which might be Long!\n        print_series_statistics(\n            data_array=location_time_series,\n            timestamps=timestamps,\n            title=\"Selected series\",\n        )\n    if csv:\n        to_csv(\n            x=location_time_series,\n            path=csv,\n        )\n</code></pre>","tags":["rekx","CLI","Reference","Tools","select","statistics","CSV"]},{"location":"cli/select/#rekx.statistics","title":"statistics","text":"<p>Functions:</p> Name Description <code>print_series_statistics</code>","tags":["rekx","CLI","Reference","Tools","select","statistics","CSV"]},{"location":"cli/select/#rekx.statistics.print_series_statistics","title":"print_series_statistics","text":"<pre><code>print_series_statistics(\n    data_array,\n    timestamps,\n    title=\"Time series\",\n    rounding_places: int = None,\n)\n</code></pre> Source code in <code>rekx/statistics.py</code> <pre><code>def print_series_statistics(\n    data_array,\n    timestamps,\n    title=\"Time series\",\n    rounding_places: int = None,\n):\n    \"\"\" \"\"\"\n    statistics = calculate_series_statistics(data_array, timestamps)\n    from rich import box\n    from rich.table import Table\n\n    table = Table(\n        title=title,\n        caption=\"Caption text\",\n        show_header=True,\n        header_style=\"bold magenta\",\n        row_styles=[\"none\", \"dim\"],\n        box=box.SIMPLE_HEAD,\n        highlight=True,\n    )\n    table.add_column(\"Statistic\", justify=\"right\", style=\"magenta\", no_wrap=True)\n    table.add_column(\"Value\", style=\"cyan\")\n\n    # Basic metadata\n    basic_metadata = [\"Start\", \"End\", \"Count\"]\n    for key in basic_metadata:\n        if key in statistics:\n            table.add_row(key, str(statistics[key]))\n\n    # Separate!\n    table.add_row(\"\", \"\")\n\n    # Index of items\n    index_metadata = [\n        \"Time of Min\",\n        \"Index of Min\",\n        \"Time of Max\",\n        \"Index of Max\",\n    ]\n\n    # Add statistics\n    for key, value in statistics.items():\n        if key not in basic_metadata and key not in index_metadata:\n            # table.add_row(key, str(round_float_values(value, rounding_places)))\n            table.add_row(key, str(value))\n\n    # Separate!\n    table.add_row(\"\", \"\")\n\n    # Index of\n    for key, value in statistics.items():\n        if key in index_metadata:\n            # table.add_row(key, str(round_float_values(value, rounding_places)))\n            table.add_row(key, str(value))\n\n    from rich.console import Console\n\n    console = Console()\n    console.print(table)\n</code></pre>","tags":["rekx","CLI","Reference","Tools","select","statistics","CSV"]},{"location":"cli/select/#rekx.csv","title":"csv","text":"<p>Multi-threaded CSV writer, much faster than :meth:<code>pandas.DataFrame.to_csv</code>, with full support for <code>dask &lt;http://dask.org/&gt;</code>_ and <code>dask distributed &lt;http://distributed.dask.org/&gt;</code>_.</p> <p>Functions:</p> Name Description <code>to_csv</code> <p>Print DataArray to CSV.</p>","tags":["rekx","CLI","Reference","Tools","select","statistics","CSV"]},{"location":"cli/select/#rekx.csv.to_csv","title":"to_csv","text":"<pre><code>to_csv(\n    x: DataArray,\n    path: str | Path,\n    *,\n    nogil: bool = True,\n    **kwargs\n)\n</code></pre> <p>Print DataArray to CSV.</p> <p>When x has numpy backend, this function is functionally equivalent to (but much) faster than)::</p> <pre><code>x.to_pandas().to_csv(path_or_buf, **kwargs)\n</code></pre> <p>When x has dask backend, this function returns a dask delayed object which will write to the disk only when its .compute() method is invoked.</p> <p>Formatting and optional compression are parallelised across all available CPUs, using one dask task per chunk on the first dimension. Chunks on other dimensions will be merged ahead of computation.</p> <p>:param x:     :class:<code>~xarray.DataArray</code> with one or two dimensions :param str path:     Output file path :param bool nogil:     If True, use accelerated C implementation. Several kwargs won't be     processed correctly (see limitations below). If False, use pandas     to_csv method (slow, and does not release the GIL).     nogil=True exclusively supports float and integer values dtypes (but     the coords can be anything). In case of incompatible dtype, nogil     is automatically switched to False. :param kwargs:     Passed verbatim to :meth:<code>pandas.DataFrame.to_csv</code> or     :meth:<code>pandas.Series.to_csv</code></p> <p>Limitations</p> <ul> <li>Fancy URIs are not (yet) supported.</li> <li>compression='zip' is not supported. All other compression methods (gzip,   bz2, xz) are supported.</li> <li>When running with nogil=True, the following parameters are ignored:   columns, quoting, quotechar, doublequote, escapechar, chunksize, decimal</li> </ul> <p>Distributed computing</p> <p>This function supports <code>dask distributed</code>_, with the caveat that all workers must write to the same shared mountpoint and that the shared filesystem must strictly guarantee close-open coherency, meaning that one must be able to call write() and then close() on a file descriptor from one host and then immediately afterwards open() from another host and see the output from the first host. Note that, for performance reasons, most network filesystems do not enable this feature by default.</p> <p>Alternatively, one may write to local mountpoints and then manually collect and concatenate the partial outputs.</p> Source code in <code>rekx/csv.py</code> <pre><code>def to_csv(\n    x: xarray.DataArray,\n    path: str | Path,\n    *,\n    nogil: bool = True,\n    **kwargs,\n):\n    \"\"\"Print DataArray to CSV.\n\n    When x has numpy backend, this function is functionally equivalent to (but\n    much) faster than)::\n\n        x.to_pandas().to_csv(path_or_buf, **kwargs)\n\n    When x has dask backend, this function returns a dask delayed object which\n    will write to the disk only when its .compute() method is invoked.\n\n    Formatting and optional compression are parallelised across all available\n    CPUs, using one dask task per chunk on the first dimension. Chunks on other\n    dimensions will be merged ahead of computation.\n\n    :param x:\n        :class:`~xarray.DataArray` with one or two dimensions\n    :param str path:\n        Output file path\n    :param bool nogil:\n        If True, use accelerated C implementation. Several kwargs won't be\n        processed correctly (see limitations below). If False, use pandas\n        to_csv method (slow, and does not release the GIL).\n        nogil=True exclusively supports float and integer values dtypes (but\n        the coords can be anything). In case of incompatible dtype, nogil\n        is automatically switched to False.\n    :param kwargs:\n        Passed verbatim to :meth:`pandas.DataFrame.to_csv` or\n        :meth:`pandas.Series.to_csv`\n\n    **Limitations**\n\n    - Fancy URIs are not (yet) supported.\n    - compression='zip' is not supported. All other compression methods (gzip,\n      bz2, xz) are supported.\n    - When running with nogil=True, the following parameters are ignored:\n      columns, quoting, quotechar, doublequote, escapechar, chunksize, decimal\n\n    **Distributed computing**\n\n    This function supports `dask distributed`_, with the caveat that all workers\n    must write to the same shared mountpoint and that the shared filesystem\n    must strictly guarantee **close-open coherency**, meaning that one must be\n    able to call write() and then close() on a file descriptor from one host\n    and then immediately afterwards open() from another host and see the output\n    from the first host. Note that, for performance reasons, most network\n    filesystems do not enable this feature by default.\n\n    Alternatively, one may write to local mountpoints and then manually collect\n    and concatenate the partial outputs.\n    \"\"\"\n    if not isinstance(x, xarray.DataArray):\n        raise ValueError(\"first argument must be a DataArray\")\n\n    # Health checks\n    if not isinstance(path, Path):\n        try:\n            path = Path(path)\n        except:\n            raise ValueError(\"path_or_buf must be a file path\")\n\n    if x.ndim not in (1, 2):\n        raise ValueError(\n            \"cannot convert arrays with %d dimensions into \" \"pandas objects\" % x.ndim\n        )\n\n    if nogil and x.dtype.kind not in \"if\":\n        nogil = False\n\n    # Extract row and columns indices\n    indices = [x.get_index(dim) for dim in x.dims]\n    if x.ndim == 2:\n        index, columns = indices\n    else:\n        index = indices[0]\n        columns = None\n\n    compression = kwargs.pop(\"compression\", \"infer\")\n    compress = _compress_func(path, compression)\n    mode = kwargs.pop(\"mode\", \"w\")\n    if mode not in \"wa\":\n        raise ValueError('mode: expected w or a; got \"%s\"' % mode)\n\n    # Fast exit for numpy backend\n    if not x.chunks:\n        bdata = kernels.to_csv(x.values, index, columns, True, nogil, kwargs)\n        if compress:\n            bdata = compress(bdata)\n        with open(path, mode + \"b\") as fh:\n            fh.write(bdata)\n        return None\n\n    # Merge chunks on all dimensions beyond the first\n    x = x.chunk((x.chunks[0],) + tuple((s,) for s in x.shape[1:]))\n\n    # Manually define the dask graph\n    tok = tokenize(x.data, index, columns, compression, path, kwargs)\n    name1 = \"to_csv_encode-\" + tok\n    name2 = \"to_csv_compress-\" + tok\n    name3 = \"to_csv_write-\" + tok\n    name4 = \"to_csv-\" + tok\n\n    dsk: dict[str | tuple, tuple] = {}\n\n    assert x.chunks\n    assert x.chunks[0]\n    offset = 0\n    for i, size in enumerate(x.chunks[0]):\n        # Slice index\n        index_i = index[offset : offset + size]\n        offset += size\n\n        x_i = (x.data.name, i) + (0,) * (x.ndim - 1)\n\n        # Step 1: convert to CSV and encode to binary blob\n        if i == 0:\n            # First chunk: print header\n            dsk[name1, i] = (kernels.to_csv, x_i, index_i, columns, True, nogil, kwargs)\n        else:\n            kwargs_i = kwargs.copy()\n            kwargs_i[\"header\"] = False\n            dsk[name1, i] = (kernels.to_csv, x_i, index_i, None, False, nogil, kwargs_i)\n\n        # Step 2 (optional): compress\n        if compress:\n            prevname = name2\n            dsk[name2, i] = compress, (name1, i)\n        else:\n            prevname = name1\n\n        # Step 3: write to file\n        if i == 0:\n            # First chunk: overwrite file if it already exists\n            dsk[name3, i] = kernels.to_file, path, mode + \"b\", (prevname, i)\n        else:\n            # Next chunks: wait for previous chunk to complete and append\n            dsk[name3, i] = (kernels.to_file, path, \"ab\", (prevname, i), (name3, i - 1))\n\n    # Rename final key\n    dsk[name4] = dsk.pop((name3, i))\n\n    hlg = HighLevelGraph.from_collections(name4, dsk, (x,))\n    return Delayed(name4, hlg)\n</code></pre>","tags":["rekx","CLI","Reference","Tools","select","statistics","CSV"]},{"location":"cli/select/#rekx.csv.write_metadata_dictionary_to_csv","title":"write_metadata_dictionary_to_csv","text":"<pre><code>write_metadata_dictionary_to_csv(\n    dictionary: dict, output_filename: Path\n) -&gt; None\n</code></pre> <p>Write a metadata dictionary to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>A dictionary containing the metadata.</p> required <code>output_filename</code> <code>Path</code> <p>Path to the output CSV file.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>rekx/csv.py</code> <pre><code>def write_metadata_dictionary_to_csv(\n    dictionary: dict,\n    output_filename: Path,\n) -&gt; None:\n    \"\"\"\n    Write a metadata dictionary to a CSV file.\n\n    Parameters\n    ----------\n    dictionary:\n        A dictionary containing the metadata.\n    output_filename: Path\n        Path to the output CSV file.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    if not dictionary:\n        raise ValueError(\"The given dictionary is empty!\")\n\n    headers = [\n        \"File Name\",\n        \"File Size\",\n        \"Variable\",\n        \"Shape\",\n        \"Type\",\n        \"Compression\",\n        \"Read time\",\n    ]\n\n    with open(output_filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n        writer = csv.writer(file)\n        writer.writerow(headers)\n\n        file_name = dictionary.get(\"File name\", \"\")\n        file_size = dictionary.get(\"File size\", \"\")\n\n        for variable, metadata in dictionary.get(\"Variables\", {}).items():\n            if \"Compression\" in metadata:\n                from .print import format_compression\n\n                compression_details = format_compression(metadata[\"Compression\"])\n            row = [\n                file_name,\n                file_size,\n                variable,\n                metadata.get(\"Shape\", \"\"),\n                metadata.get(\"Type\", \"\"),\n                metadata.get(\"Scale\", \"\"),\n                metadata.get(\"Offset\", \"\"),\n                compression_details[\"Filters\"] if compression_details else None,\n                compression_details[\"Level\"] if compression_details else None,\n                metadata.get(\"Shuffling\", \"\"),\n                metadata.get(\"Read time\", \"\"),\n            ]\n            writer.writerow(row)\n    print(f\"Output written to [code]{output_filename}[/code]\")\n</code></pre>","tags":["rekx","CLI","Reference","Tools","select","statistics","CSV"]},{"location":"cli/select/#rekx.csv.write_nested_dictionary_to_csv","title":"write_nested_dictionary_to_csv","text":"<pre><code>write_nested_dictionary_to_csv(\n    nested_dictionary: dict, output_filename: Path\n) -&gt; None\n</code></pre> Source code in <code>rekx/csv.py</code> <pre><code>def write_nested_dictionary_to_csv(\n    nested_dictionary: dict,\n    output_filename: Path,\n) -&gt; None:\n    \"\"\" \"\"\"\n    if not nested_dictionary:\n        raise ValueError(\"The given dictionary is empty!\")\n\n    with open(output_filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n        writer = csv.writer(file)\n        writer.writerow(\n            [\n                \"File\",\n                \"Size\",\n                \"Variable\",\n                \"Shape\",\n                \"Chunks\",\n                \"Cache\",\n                \"Elements\",\n                \"Preemption\",\n                \"Type\",\n                \"Scale\",\n                \"Offset\",\n                \"Compression\",\n                \"Level\",\n                \"Shuffling\",\n                # \"Repetitions\",\n                \"Read time\",\n            ]\n        )\n\n        for file_name, file_data in nested_dictionary.items():\n            for variable, metadata in file_data.get(\"Variables\", {}).items():\n                row = [\n                    file_data.get(\"File name\", \"\"),\n                    file_data.get(\"File size\", \"\"),\n                    variable,\n                    metadata.get(\"Shape\", \"\"),\n                    metadata.get(\"Chunks\", \"\"),\n                    metadata.get(\"Cache\", \"\"),\n                    metadata.get(\"Elements\", \"\"),\n                    metadata.get(\"Preemption\", \"\"),\n                    metadata.get(\"Type\", \"\"),\n                    metadata.get(\"Scale\", \"\"),\n                    metadata.get(\"Offset\", \"\"),\n                    metadata.get(\"Compression\", \"\"),\n                    metadata.get(\"Level\", \"\"),\n                    metadata.get(\"Shuffling\", \"\"),\n                    metadata.get(\"Read time\", \"\"),\n                ]\n                writer.writerow(row)\n    print(f\"Output written to [code]{output_filename}[/code]\")\n</code></pre>","tags":["rekx","CLI","Reference","Tools","select","statistics","CSV"]},{"location":"cli/shapes/","title":"Shapes","text":"","tags":["rekx","Reference","CLI","Tools","shapes"]},{"location":"cli/shapes/#rekx.shapes","title":"shapes","text":"<p>Functions:</p> Name Description <code>diagnose_chunking_shapes</code> <p>Diagnose the chunking shapes of multiple Xarray-supported files.</p>","tags":["rekx","Reference","CLI","Tools","shapes"]},{"location":"cli/shapes/#rekx.shapes.diagnose_chunking_shapes","title":"diagnose_chunking_shapes","text":"<pre><code>diagnose_chunking_shapes(\n    source_directory: Path,\n    pattern: str = \"*.nc\",\n    variable_set: XarrayVariableSet = all,\n    validate_consistency: bool = False,\n    common_shapes: bool = False,\n    csv: Path = None,\n    verbose: int = VERBOSE_LEVEL_DEFAULT,\n)\n</code></pre> <p>Diagnose the chunking shapes of multiple Xarray-supported files.</p> <p>Scan the <code>source_directory</code> for Xarray-supported files that match the given <code>pattern</code> and diagnose the chunking shapes for each variable or determine the maximum common chunking shape across the input data.</p> <p>Parameters:</p> Name Type Description Default <code>source_directory</code> <code>Path</code> <p>The source directory to scan for files matching the <code>pattern</code></p> required <code>pattern</code> <code>str</code> <p>The filename pattern to match files</p> <code>'*.nc'</code> <code>variable_set</code> <code>XarrayVariableSet</code> <p>Name of the set of variables to query. See also docstring of XarrayVariableSet</p> <code>all</code> <code>verbose</code> <code>int</code> <p>Verbosity level</p> <code>VERBOSE_LEVEL_DEFAULT</code> <p>Returns:</p> Type Description <code># common_chunking_shapes: dict</code> <code>#     A dictionary with the common maximum chunking shapes for each variable</code> <code>#     identified in the input data.</code> Source code in <code>rekx/shapes.py</code> <pre><code>def diagnose_chunking_shapes(\n    source_directory: Annotated[Path, typer_argument_source_directory],\n    pattern: Annotated[str, typer_option_filename_pattern] = \"*.nc\",\n    variable_set: Annotated[\n        XarrayVariableSet, typer.Option(help=\"Set of Xarray variables to diagnose\")\n    ] = XarrayVariableSet.all,\n    validate_consistency: Annotated[bool, typer.Option(help=\"\")] = False,\n    common_shapes: Annotated[\n        bool, typer.Option(help=\"Report common maximum chunking shape\")\n    ] = False,\n    csv: Annotated[Path, typer_option_csv] = None,\n    verbose: Annotated[int, typer_option_verbose] = VERBOSE_LEVEL_DEFAULT,\n):\n    \"\"\"Diagnose the chunking shapes of multiple Xarray-supported files.\n\n    Scan the `source_directory` for Xarray-supported files that match\n    the given `pattern` and diagnose the chunking shapes for each variable\n    or determine the maximum common chunking shape across the input data.\n\n    Parameters\n    ----------\n    source_directory: Path\n        The source directory to scan for files matching the `pattern`\n    pattern: str\n        The filename pattern to match files\n    variable_set: XarrayVariableSet\n        Name of the set of variables to query. See also docstring of\n        XarrayVariableSet\n    verbose: int\n        Verbosity level\n\n    Returns\n    -------\n    # common_chunking_shapes: dict\n    #     A dictionary with the common maximum chunking shapes for each variable\n    #     identified in the input data.\n\n    \"\"\"\n    source_directory = Path(source_directory)\n    if not source_directory.exists() or not any(source_directory.iterdir()):\n        print(\n            f\"[red]The directory [code]{source_directory}[/code] does not exist or is empty[/red].\"\n        )\n        return\n    file_paths = list(source_directory.glob(pattern))\n    if not file_paths:\n        print(\n            f\"No files matching the pattern [code]{pattern}[/code] found in [code]{source_directory}[/code]!\"\n        )\n        return\n\n    mode = DisplayMode(verbose)\n    with display_context[mode]:\n        try:\n            chunking_shapes = detect_chunking_shapes_parallel(\n                file_paths=file_paths,\n                variable_set=variable_set,\n            )\n        except TypeError as e:\n            raise ValueError(\"Error occurred:\", e)\n\n    if validate_consistency:\n        inconsistent_variables = {}\n        for variable, shapes in chunking_shapes.items():\n            if len(shapes) &gt; 1:\n                inconsistent_variables[variable] = {\n                    shape: list(files) for shape, files in shapes.items()\n                }\n\n        if inconsistent_variables:\n            validation_message = f\"{x_mark} [bold red]Variables are not consistently shaped across all files![/bold red]\"\n        else:\n            validation_message = f\"{check_mark} [green]Variables are consistently shaped across all files![/green]\"\n        if not verbose:\n            print(validation_message)\n            return\n        else:\n            print(validation_message)\n            print_chunking_shapes_consistency_validation_long_table(\n                inconsistent_variables\n            )\n            return\n\n    if common_shapes:\n        common_chunking_shapes = {}\n        for variable, shapes in chunking_shapes.items():\n            import numpy as np\n\n            max_shape = np.array(next(iter(shapes)), dtype=int)\n            for shape in shapes:\n                current_shape = np.array(shape, dtype=int)\n                max_shape = np.maximum(max_shape, current_shape)\n            common_chunking_shapes[variable] = tuple(max_shape)\n\n        print_common_chunk_layouts(common_chunking_shapes)\n        # return common_chunking_shapes\n\n    print_chunk_shapes_table(chunking_shapes)  # , highlight_variables)  : Idea\n    if csv:\n        write_nested_dictionary_to_csv(\n            # nested_dictionary=chunking_shapes,\n            nested_dictionary=chunking_shapes\n            if not common_shapes\n            else common_chunking_shapes,\n            output_filename=csv,\n        )\n</code></pre>","tags":["rekx","Reference","CLI","Tools","shapes"]},{"location":"cli/suggest/","title":"Suggest","text":"","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/suggest/#rekx.suggest","title":"suggest","text":"<p>Functions:</p> Name Description <code>adjust_first_dimension</code> <p>Adjust the size of the first dimension of the chunk shape</p> <code>binlist</code> <p>Return list of bits that represent a non-negative integer.</p> <code>calculate_ideal_number_of_chunks</code> <p>Calculate the ideal number of chunks based on the variable shape and chunk size</p> <code>determine_chunking_shape</code> <p>Determine optimal chunk shape for a 3D array.</p> <code>determine_chunking_shape_alternative</code> <p>Determine optimal chunk shape for a variable with additional constraints.</p> <code>determine_chunking_shape_alternative_symmetrical</code> <p>Determine optimal symmetrical chunk shape for a variable with additional constraints.</p> <code>find_nearest_divisor</code> <p>Find the nearest acceptable divisor for a number.</p> <code>is_power_of_two</code> <p>Check if a number is a power of two.</p> <code>perturb_shape</code> <p>Return shape perturbed by adding 1 to elements corresponding to 1 bits in on_bits</p> <code>suggest_chunking_shape</code> <code>suggest_chunking_shape_alternative</code> <code>suggest_chunking_shape_alternative_symmetrical</code>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/suggest/#rekx.suggest.adjust_first_dimension","title":"adjust_first_dimension","text":"<pre><code>adjust_first_dimension(\n    variable_shape: List[int],\n    number_of_chunks_per_axis: float,\n) -&gt; float\n</code></pre> <p>Adjust the size of the first dimension of the chunk shape</p> Source code in <code>rekx/suggest.py</code> <pre><code>def adjust_first_dimension(\n    variable_shape: List[int], number_of_chunks_per_axis: float\n) -&gt; float:\n    \"\"\"Adjust the size of the first dimension of the chunk shape\"\"\"\n    if variable_shape[0] / (number_of_chunks_per_axis**2) &lt; 1:\n        return 1.0, number_of_chunks_per_axis / math.sqrt(\n            variable_shape[0] / (number_of_chunks_per_axis**2)\n        )\n    return (\n        variable_shape[0] // (number_of_chunks_per_axis**2),\n        number_of_chunks_per_axis,\n    )\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/suggest/#rekx.suggest.binlist","title":"binlist","text":"<pre><code>binlist(n, width=0)\n</code></pre> <p>Return list of bits that represent a non-negative integer. n      -- non-negative integer width  -- number of bits in returned zero-filled list (default 0)</p> Source code in <code>rekx/suggest.py</code> <pre><code>def binlist(n, width=0):\n    \"\"\"Return list of bits that represent a non-negative integer.\n    n      -- non-negative integer\n    width  -- number of bits in returned zero-filled list (default 0)\n    \"\"\"\n    return [int(bit) for bit in bin(n)[2:].zfill(width)]\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/suggest/#rekx.suggest.calculate_ideal_number_of_chunks","title":"calculate_ideal_number_of_chunks","text":"<pre><code>calculate_ideal_number_of_chunks(\n    variable_shape: List[int],\n    float_size: int,\n    chunk_size: int,\n) -&gt; float\n</code></pre> <p>Calculate the ideal number of chunks based on the variable shape and chunk size</p> Source code in <code>rekx/suggest.py</code> <pre><code>def calculate_ideal_number_of_chunks(\n    variable_shape: List[int], float_size: int, chunk_size: int\n) -&gt; float:\n    \"\"\"Calculate the ideal number of chunks based on the variable shape and chunk size\"\"\"\n    # ideal_number_of_values = chunk_size / float_size if chunk_size &gt; float_size else 1\n    ideal_number_of_values = max(chunk_size // float_size, 1)\n    ideal_number_of_chunks = np.prod(variable_shape) / ideal_number_of_values\n    return ideal_number_of_chunks\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/suggest/#rekx.suggest.determine_chunking_shape","title":"determine_chunking_shape","text":"<pre><code>determine_chunking_shape(\n    variable_shape: VariableShapeModel,\n    float_size: int = 4,\n    chunk_size: int = 4096,\n) -&gt; List[int]\n</code></pre> <p>Determine optimal chunk shape for a 3D array.</p> <p>Based on Python code and algorithm developed by Russ Rew, posted at \"Chunking Data: Choosing Shapes\", https://www.unidata.ucar.edu/blog_content/data/2013/chunk_shape_3D.py accessed on 31 October 2023</p> <p>Parameters:</p> Name Type Description Default <code>variable_shape</code> <code>VariableShapeModel</code> <p>The shape of the 3D array</p> required <code>float_size</code> <code>int</code> <p>Size of a float value in bytes</p> <code>4</code> <code>chunk_size</code> <code>int</code> <p>Maximum allowable chunk size in bytes which cannot be greater</p> <code>4096</code> <code>than</code> required <code>dimensions</code> <p>Number of dimensions (should be 3 for a 3D array)</p> required <p>Returns:</p> Type Description <code>Optimal chunk shape as a list of integers</code> Source code in <code>rekx/suggest.py</code> <pre><code>def determine_chunking_shape(\n    variable_shape: VariableShapeModel,\n    float_size: int = 4,\n    chunk_size: int = 4096,\n    # dimensions: int = 3,\n) -&gt; List[int]:\n    \"\"\"Determine optimal chunk shape for a 3D array.\n\n    Based on Python code and algorithm developed by Russ Rew, posted at\n    \"Chunking Data: Choosing Shapes\",\n    https://www.unidata.ucar.edu/blog_content/data/2013/chunk_shape_3D.py\n    accessed on 31 October 2023\n\n    Parameters\n    ----------\n    variable_shape:\n        The shape of the 3D array\n\n    float_size:\n        Size of a float value in bytes\n    chunk_size:\n        Maximum allowable chunk size in bytes which cannot be greater\n    than the size of the physical block\n    dimensions:\n        Number of dimensions (should be 3 for a 3D array)\n\n    Returns\n    -------\n    Optimal chunk shape as a list of integers\n    \"\"\"\n    # if verbose:\n    #     print(f\"Variable Shape: {variable_shape.variable_shape}\")\n\n    # Calculate ideal number of chunks\n    ideal_number_of_chunks = calculate_ideal_number_of_chunks(\n        variable_shape, float_size, chunk_size\n    )\n    number_of_chunks_per_axis = ideal_number_of_chunks**0.25\n\n    # Initialize the first candidate chunking shape\n    first_dimension, number_of_chunks_per_axis = adjust_first_dimension(\n        variable_shape, number_of_chunks_per_axis\n    )\n    first_candidate_chunking_shape = [first_dimension]\n\n    # Factor to increase other dimensions to at least 1 if required\n    sizing_factor = 1.0\n    for dimension_size in variable_shape[1:]:\n        if dimension_size / number_of_chunks_per_axis &lt; 1:\n            sizing_factor *= number_of_chunks_per_axis / dimension_size\n\n    # Adjust other dimensions\n    for dimension_size in variable_shape[1:]:\n        chunking_shape = (\n            1.0\n            if dimension_size / number_of_chunks_per_axis &lt; 1\n            else (sizing_factor * dimension_size) // number_of_chunks_per_axis\n        )\n        first_candidate_chunking_shape.append(chunking_shape)\n\n    # Fine-tuning to find the best chunk shape\n    best_chunk_size = 0\n    best_chunking_shape = first_candidate_chunking_shape\n    for index in range(8):  # Total number of dimensions is 3, so 2^3 = 8\n        # a candidate chunk shape during the fine-tuning process\n        candidate_chunking_shape = perturb_shape(first_candidate_chunking_shape, index)\n        number_of_values_in_chunk = np.prod(candidate_chunking_shape)\n        this_chunk_size = float_size * number_of_values_in_chunk\n        if best_chunk_size &lt; this_chunk_size &lt;= chunk_size:\n            best_chunk_size = this_chunk_size  # Update best chunk size\n            best_chunking_shape = list(candidate_chunking_shape)  # Update best shape\n\n    return list(map(int, best_chunking_shape))\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/suggest/#rekx.suggest.determine_chunking_shape_alternative","title":"determine_chunking_shape_alternative","text":"<pre><code>determine_chunking_shape_alternative(\n    variable_shape,\n    float_size=4,\n    chunk_size=4096,\n    max_chunks: int = None,\n    min_chunk_size: int = None,\n    force_power_of_two: bool = False,\n    spatial_divisors: List[int] = None,\n)\n</code></pre> <p>Determine optimal chunk shape for a variable with additional constraints.</p> <p>Parameters:</p> Name Type Description Default <code>variable_shape</code> <code>list of int</code> <p>The shape of the variable (time, latitude, longitude).</p> required <code>float_size</code> <code>int</code> <p>Size of a float value in bytes.</p> <code>4</code> <code>chunk_size</code> <code>int</code> <p>Maximum allowable chunk size in bytes.</p> <code>4096</code> <code>max_chunks</code> <code>int</code> <p>Maximum number of chunks allowed.</p> <code>None</code> <code>min_chunk_size</code> <code>int</code> <p>Minimum allowable chunk size in bytes.</p> <code>None</code> <code>force_power_of_two</code> <code>bool</code> <p>Whether to force chunk dimensions to be powers of two.</p> <code>False</code> <code>spatial_divisors</code> <code>list of int</code> <p>Acceptable divisors for the spatial dimensions.</p> <code>None</code> <p>Returns:</p> Type Description <code>list of int</code> <p>Optimal chunk shape.</p> <p>Examples:</p> <p>variable_shape_example = [100, 400, 400]  # Example 3D variable shape force_power_of_two_example = True  # Enforce that chunk dimensions are powers of two spatial_divisors_example = [2**i for i in range(1, 9)]  # Acceptable spatial divisors determine_chunking_shape(     variable_shape_example,     force_power_of_two=force_power_of_two_example,     spatial_divisors=spatial_divisors_example )</p> Source code in <code>rekx/suggest.py</code> <pre><code>def determine_chunking_shape_alternative(\n    variable_shape,\n    float_size=4,\n    chunk_size=4096,\n    max_chunks: int = None,\n    min_chunk_size: int = None,\n    force_power_of_two: bool = False,\n    spatial_divisors: List[int] = None,\n):\n    \"\"\"\n    Determine optimal chunk shape for a variable with additional constraints.\n\n    Parameters\n    ----------\n    variable_shape : list of int\n        The shape of the variable (time, latitude, longitude).\n    float_size : int\n        Size of a float value in bytes.\n    chunk_size : int\n        Maximum allowable chunk size in bytes.\n    max_chunks : int, optional\n        Maximum number of chunks allowed.\n    min_chunk_size : int, optional\n        Minimum allowable chunk size in bytes.\n    force_power_of_two : bool, optional\n        Whether to force chunk dimensions to be powers of two.\n    spatial_divisors : list of int, optional\n        Acceptable divisors for the spatial dimensions.\n\n    Returns\n    -------\n    list of int\n        Optimal chunk shape.\n\n    Examples\n    --------\n    variable_shape_example = [100, 400, 400]  # Example 3D variable shape\n    force_power_of_two_example = True  # Enforce that chunk dimensions are powers of two\n    spatial_divisors_example = [2**i for i in range(1, 9)]  # Acceptable spatial divisors\n    determine_chunking_shape(\n        variable_shape_example,\n        force_power_of_two=force_power_of_two_example,\n        spatial_divisors=spatial_divisors_example\n    )\n    \"\"\"\n    # ideal number of chunks\n    variable_size = np.prod(variable_shape) * float_size\n    ideal_chunk_volume = min(\n        chunk_size, variable_size // max_chunks if max_chunks else variable_size\n    )\n    if min_chunk_size:\n        ideal_chunk_volume = max(ideal_chunk_volume, min_chunk_size)\n\n    # initial chunk dimensions without considering power of two or specific divisors\n    chunk_dimensions = [\n        int(\n            np.round(\n                (ideal_chunk_volume / (float_size * np.prod(variable_shape[: i + 1])))\n                ** (1 / (len(variable_shape) - i))\n            )\n        )\n        for i in range(len(variable_shape))\n    ]\n    chunk_dimensions = [\n        max(1, min(dim, variable_shape[i])) for i, dim in enumerate(chunk_dimensions)\n    ]\n\n    # Adjust dimensions to meet power of two constraint\n    if force_power_of_two:\n        chunk_dimensions = [2 ** int(np.log2(dim)) for dim in chunk_dimensions]\n\n    # Adjust spatial dimensions to meet specific divisors constraint\n    if spatial_divisors:\n        for i in range(1, len(variable_shape)):\n            chunk_dimensions[i] = find_nearest_divisor(\n                variable_shape[i], spatial_divisors\n            )\n\n    # Ensure that the chunk size is within the limits\n    chunk_volume = np.prod(chunk_dimensions) * float_size\n    while chunk_volume &gt; chunk_size and any(dim &gt; 1 for dim in chunk_dimensions):\n        for i in range(len(chunk_dimensions)):\n            if chunk_dimensions[i] &gt; 1:\n                chunk_dimensions[i] //= 2\n                break\n        chunk_volume = np.prod(chunk_dimensions) * float_size\n\n    # Ensure chunk dimensions do not exceed the variable dimensions\n    chunk_dimensions = [\n        min(dim, variable_shape[i]) for i, dim in enumerate(chunk_dimensions)\n    ]\n\n    # Ensure the total number of chunks does not exceed max_chunks if set\n    if max_chunks:\n        while (\n            np.prod(np.ceil(np.array(variable_shape) / np.array(chunk_dimensions)))\n            &gt; max_chunks\n        ):\n            for i in range(len(chunk_dimensions)):\n                if chunk_dimensions[i] &lt; variable_shape[i]:\n                    chunk_dimensions[i] *= 2\n                break\n\n    return chunk_dimensions\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/suggest/#rekx.suggest.determine_chunking_shape_alternative_symmetrical","title":"determine_chunking_shape_alternative_symmetrical","text":"<pre><code>determine_chunking_shape_alternative_symmetrical(\n    variable_shape,\n    float_size=4,\n    chunk_size=4096,\n    max_chunks: int = None,\n    min_chunk_size: int = None,\n    force_power_of_two: bool = False,\n    spatial_divisors: List[int] = None,\n)\n</code></pre> <p>Determine optimal symmetrical chunk shape for a variable with additional constraints.</p> <p>Parameters:</p> Name Type Description Default <code>variable_shape</code> <code>list of int</code> <p>The shape of the variable (time, latitude, longitude).</p> required <code>float_size</code> <code>int</code> <p>Size of a float value in bytes.</p> <code>4</code> <code>chunk_size</code> <code>int</code> <p>Maximum allowable chunk size in bytes.</p> <code>4096</code> <code>max_chunks</code> <code>int</code> <p>Maximum number of chunks allowed.</p> <code>None</code> <code>min_chunk_size</code> <code>int</code> <p>Minimum allowable chunk size in bytes.</p> <code>None</code> <code>force_power_of_two</code> <code>bool</code> <p>Whether to force chunk dimensions to be powers of two.</p> <code>False</code> <code>spatial_divisor</code> <code>int</code> <p>Acceptable divisor for the spatial dimensions.</p> required <p>Returns:</p> Type Description <code>list of int</code> <p>Optimal symmetrical chunk shape.</p> <p>Examples:</p> <p>variable_shape_example = [100, 2600, 2600] force_power_of_two_example = True spatial_divisor_example = 32 determine_chunking_shape_symmetrical(     variable_shape_example,     force_power_of_two=force_power_of_two_example,     spatial_divisor=spatial_divisor_example )</p> Source code in <code>rekx/suggest.py</code> <pre><code>def determine_chunking_shape_alternative_symmetrical(\n    variable_shape,\n    float_size=4,\n    chunk_size=4096,\n    max_chunks: int = None,\n    min_chunk_size: int = None,\n    force_power_of_two: bool = False,\n    spatial_divisors: List[int] = None,\n):\n    \"\"\"\n    Determine optimal symmetrical chunk shape for a variable with additional constraints.\n\n    Parameters\n    ----------\n    variable_shape : list of int\n        The shape of the variable (time, latitude, longitude).\n    float_size : int\n        Size of a float value in bytes.\n    chunk_size : int\n        Maximum allowable chunk size in bytes.\n    max_chunks : int, optional\n        Maximum number of chunks allowed.\n    min_chunk_size : int, optional\n        Minimum allowable chunk size in bytes.\n    force_power_of_two : bool, optional\n        Whether to force chunk dimensions to be powers of two.\n    spatial_divisor : int, optional\n        Acceptable divisor for the spatial dimensions.\n\n    Returns\n    -------\n    list of int\n        Optimal symmetrical chunk shape.\n\n    Examples\n    --------\n    variable_shape_example = [100, 2600, 2600]\n    force_power_of_two_example = True\n    spatial_divisor_example = 32\n    determine_chunking_shape_symmetrical(\n        variable_shape_example,\n        force_power_of_two=force_power_of_two_example,\n        spatial_divisor=spatial_divisor_example\n    )\n    \"\"\"\n\n    def find_largest_divisor_in_list(n, divisors):\n        \"\"\"\n        Find the largest divisor of 'n' within the provided list of 'divisors'.\n\n        Parameters:\n        n (int): The number for which to find the divisor.\n        divisors (list of int): A list of potential divisors.\n\n        Returns:\n        int: The largest divisor in the list that divides 'n', or 1 if none are found.\n        \"\"\"\n        largest_divisor = 1\n        for divisor in sorted(divisors, reverse=True):\n            if n % divisor == 0:\n                largest_divisor = divisor\n                break\n        return largest_divisor\n\n    # Determine the largest spatial dimension that is less than or equal to the spatial_divisor\n    # and is a divisor of the spatial dimensions of the variable\n    if spatial_divisors:\n        max_spatial_divisor = find_largest_divisor_in_list(\n            min(variable_shape[1:]), spatial_divisors\n        )\n    else:\n        max_spatial_divisor = min(variable_shape[1:])\n\n    # If enforcing power of two, find the nearest power of two that is less than or equal to the max_spatial_divisor\n    if force_power_of_two:\n        max_spatial_divisor = 2 ** int(np.floor(np.log2(max_spatial_divisor)))\n\n    # Calculate the maximum chunk volume given the constraints\n    max_chunk_volume = chunk_size / float_size\n    if min_chunk_size:\n        max_chunk_volume = max(max_chunk_volume, min_chunk_size / float_size)\n\n    # Initialize the spatial dimensions as large as possible given the constraints\n    spatial_dimension = max_spatial_divisor\n    while spatial_dimension &gt; 1 and (spatial_dimension**2 &gt; max_chunk_volume):\n        spatial_dimension //= 2\n\n    # Ensure the spatial dimensions do not exceed the variable dimensions\n    spatial_dimension = min(spatial_dimension, variable_shape[1], variable_shape[2])\n\n    # Calculate the time dimension given the maximum chunk volume and spatial dimensions\n    time_dimension = int(np.floor(max_chunk_volume / (spatial_dimension**2)))\n    time_dimension = max(1, min(time_dimension, variable_shape[0]))\n\n    # Create the chunk shape\n    chunk_shape = [time_dimension, spatial_dimension, spatial_dimension]\n\n    # Ensure the chunk size does not exceed the chunk_size limit\n    while np.prod(chunk_shape) * float_size &gt; chunk_size:\n        # Reduce the spatial dimensions if possible\n        if spatial_dimension &gt; 1:\n            spatial_dimension //= 2\n            chunk_shape = [time_dimension, spatial_dimension, spatial_dimension]\n        # Otherwise, reduce the time dimension\n        elif time_dimension &gt; 1:\n            time_dimension //= 2\n            chunk_shape[0] = time_dimension\n\n    # Adjust if the number of chunks exceeds max_chunks\n    if (\n        max_chunks\n        and np.prod(np.ceil(np.array(variable_shape) / np.array(chunk_shape)))\n        &gt; max_chunks\n    ):\n        # Increase the chunk size by increasing the time dimension\n        while (\n            time_dimension &lt; variable_shape[0]\n            and np.prod(np.ceil(np.array(variable_shape) / np.array(chunk_shape)))\n            &gt; max_chunks\n        ):\n            time_dimension *= 2\n            chunk_shape[0] = time_dimension\n\n    return chunk_shape\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/suggest/#rekx.suggest.find_nearest_divisor","title":"find_nearest_divisor","text":"<pre><code>find_nearest_divisor(n, divisors)\n</code></pre> <p>Find the nearest acceptable divisor for a number.</p> Source code in <code>rekx/suggest.py</code> <pre><code>def find_nearest_divisor(n, divisors):\n    \"\"\"Find the nearest acceptable divisor for a number.\"\"\"\n    nearest_divisor = min(\n        divisors, key=lambda x: abs(x - n) if n % x == 0 else float(\"inf\")\n    )\n    return nearest_divisor if n % nearest_divisor == 0 else n\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/suggest/#rekx.suggest.is_power_of_two","title":"is_power_of_two","text":"<pre><code>is_power_of_two(n)\n</code></pre> <p>Check if a number is a power of two.</p> Source code in <code>rekx/suggest.py</code> <pre><code>def is_power_of_two(n):\n    \"\"\"Check if a number is a power of two.\"\"\"\n    return (n != 0) and (n &amp; (n - 1) == 0)\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/suggest/#rekx.suggest.perturb_shape","title":"perturb_shape","text":"<pre><code>perturb_shape(shape, on_bits)\n</code></pre> <p>Return shape perturbed by adding 1 to elements corresponding to 1 bits in on_bits shape  -- list of variable dimension sizes on_bits -- non-negative integer less than 2**len(shape)</p> Source code in <code>rekx/suggest.py</code> <pre><code>def perturb_shape(shape, on_bits):\n    \"\"\"Return shape perturbed by adding 1 to elements corresponding to 1 bits in on_bits\n    shape  -- list of variable dimension sizes\n    on_bits -- non-negative integer less than 2**len(shape)\n    \"\"\"\n    return [dim + bit for dim, bit in zip(shape, binlist(on_bits, len(shape)))]\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/suggest/#rekx.suggest.suggest_chunking_shape","title":"suggest_chunking_shape","text":"<pre><code>suggest_chunking_shape(\n    variable_shape: VariableShapeModel,\n    float_size: int = 4,\n    chunk_size: int = 4096,\n) -&gt; None\n</code></pre> Source code in <code>rekx/suggest.py</code> <pre><code>def suggest_chunking_shape(\n    variable_shape: Annotated[VariableShapeModel, typer_argument_variable_shape],\n    float_size: int = 4,\n    chunk_size: int = 4096,\n) -&gt; None:\n    \"\"\" \"\"\"\n    good_chunking_shape = determine_chunking_shape(\n        variable_shape=variable_shape,\n        float_size=float_size,\n        chunk_size=chunk_size,\n    )\n    print(good_chunking_shape)\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/suggest/#rekx.suggest.suggest_chunking_shape_alternative","title":"suggest_chunking_shape_alternative","text":"<pre><code>suggest_chunking_shape_alternative(\n    variable_shape: VariableShapeModel,\n    float_size: int = 4,\n    chunk_size: int = 4096,\n    max_chunks: int = None,\n    min_chunk_size: int = None,\n    force_power_of_two: bool = False,\n    spatial_divisors: List[int] = [\n        2**i for i in range(6, 12)\n    ],\n) -&gt; None\n</code></pre> Source code in <code>rekx/suggest.py</code> <pre><code>def suggest_chunking_shape_alternative(\n    variable_shape: Annotated[VariableShapeModel, typer_argument_variable_shape],\n    float_size: int = 4,\n    chunk_size: int = 4096,\n    max_chunks: int = None,\n    min_chunk_size: int = None,\n    force_power_of_two: bool = False,\n    spatial_divisors: List[int] = [2**i for i in range(6, 12)],\n) -&gt; None:\n    \"\"\" \"\"\"\n    good_chunking_shape = determine_chunking_shape_alternative(\n        variable_shape=variable_shape,\n        float_size=float_size,\n        chunk_size=chunk_size,\n        max_chunks=max_chunks,\n        min_chunk_size=min_chunk_size,\n        force_power_of_two=force_power_of_two,\n        spatial_divisors=spatial_divisors,\n    )\n    print(good_chunking_shape)\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"cli/suggest/#rekx.suggest.suggest_chunking_shape_alternative_symmetrical","title":"suggest_chunking_shape_alternative_symmetrical","text":"<pre><code>suggest_chunking_shape_alternative_symmetrical(\n    variable_shape: VariableShapeModel,\n    float_size: int = 4,\n    chunk_size: int = 4096,\n    max_chunks: int = None,\n    min_chunk_size: int = None,\n    force_power_of_two: bool = False,\n    spatial_divisors: List[int] = [\n        2**i for i in range(6, 12)\n    ],\n) -&gt; None\n</code></pre> Source code in <code>rekx/suggest.py</code> <pre><code>def suggest_chunking_shape_alternative_symmetrical(\n    variable_shape: Annotated[VariableShapeModel, typer_argument_variable_shape],\n    float_size: int = 4,\n    chunk_size: int = 4096,\n    max_chunks: int = None,\n    min_chunk_size: int = None,\n    force_power_of_two: bool = False,\n    spatial_divisors: List[int] = [2**i for i in range(6, 12)],\n) -&gt; None:\n    \"\"\" \"\"\"\n    good_chunking_shape = determine_chunking_shape_alternative_symmetrical(\n        variable_shape=variable_shape,\n        float_size=float_size,\n        chunk_size=chunk_size,\n        max_chunks=max_chunks,\n        min_chunk_size=min_chunk_size,\n        force_power_of_two=force_power_of_two,\n        spatial_divisors=spatial_divisors,\n    )\n    print(good_chunking_shape)\n</code></pre>","tags":["rekx","CLI","Reference","Tools"]},{"location":"how_to/","title":"Index","text":"<p>Learn how to ...</p> <p> Diagnose structural characteristics of data packed in NetCDF/HDF5 files</p> <ul> <li> <p> <code>inspect</code></p> </li> <li> <p> <code>shapes</code></p> </li> <li> <p> <code>validate</code> uniform chunking across multiple files</p> </li> </ul> <p>Experimental</p> <p> <code>suggest</code> good chunking shapes</p> <p> <code>rechunk</code> NetCDF datasets</p> <p>Create <code>parquet</code>  and JSON Kerchunk <code>reference</code> sets</p> <p> <code>combine</code> Kerchunk reference sets</p> <p> <code>select</code> data from Kerchunk reference sets</p> <p> Get an idea about <code>read-performance</code> of data from NetCDF/HDF files and Kerchunk reference sets</p> <p>Data</p> <p>Many examples in this documentation use NetCDF files from the Surface Solar Radiation Climate Data Record (SARAH-3)<sup>1</sup>.</p> <p>Literage programming!</p> <p>The How-To examples are real commands executed at the time of building this very documentation.  This ensures that the command output, is indeed what you'd really get by installing <code>rekx</code>!  This literate programming experience is achieved with the use of the wonderful plugin markdown-exec.</p> <ol> <li> <p>Uwe Pfeifroth, Steffen Kothe, Jaqueline Dr\u00fccke, J\u00f6rg Trentmann, Marc Schr\u00f6der, Nathalie Selbach, and Rainer Hollmann. Surface radiation data set - heliosat (sarah) - edition 3. 2023. URL: https://wui.cmsaf.eu/safira/action/viewDoiDetails?acronym=SARAH_V003, doi:10.5676/EUM_SAF_CM/SARAH/V003.\u00a0\u21a9</p> </li> </ol>","tags":["How-To","CLI","Data","SARAH-3"]},{"location":"how_to/chunking_parameterisation/","title":"Chunk appropriately ?","text":"<p>Fix Me</p> <p>Explain better the idea, the commands and the example !</p>","tags":["How-To","rekx","CLI","NetCDF","Chunking","Caching","nccopy","Parallel processing","GNU Parallel"]},{"location":"how_to/chunking_parameterisation/#chunking-parameterisation","title":"Chunking parameterisation","text":"","tags":["How-To","rekx","CLI","NetCDF","Chunking","Caching","nccopy","Parallel processing","GNU Parallel"]},{"location":"how_to/chunking_parameterisation/#combining-parameters","title":"Combining parameters","text":"<p>Identifying the appropriate configuration for data stored in NetCDF files, involves systematic experimentation with various structural parameters. The <code>rechunk-generator</code> command varies structural data parameters and options for NetCDF files that influence the file size and data access speed and generates a series of <code>nccopy</code> commands for rechunking NetCDF files. The list of commands can be fed to  GNU Parallel which will take care to run them in parallel. Subsequently, <code>rekx inspect</code> can report on the new data structures and the average time it takes to retrieve data over a geographic location.</p> <p>Given the initial NetCDF file <code>SISin202001010000004231000101MA.nc</code>, we can use <code>rekx rechunk-generator</code>  to generate a series of experimental <code>nccopy</code> commands  and save them in a file prefixed with <code>rechunk_commands_for_</code> followed after the name of the input NetCDF file :</p> <pre><code>\u276f rekx rechunk-generator SISin202001010000004231000101MA.nc rechunking_test --time 48 --latitude 64,128,256 --longitude 64,128,256 --compression-level 0,3,6,9 -v --shuffling --memory\n</code></pre> <p>Example</p> <p>In this example, we ask for possible chunk sizes for :</p> <ul> <li> <p><code>time</code> to be \\(48\\), that is only one size</p> </li> <li> <p><code>latitude</code> and <code>longitude</code> sizes \\(64\\), \\(128\\) and \\(256\\)</p> </li> <li> <p><code>compression-level</code>s \\(0\\), \\(3\\), \\(64\\), and \\(9\\)</p> </li> </ul> <p>In addition, we ask for <code>--shuffling</code>.   Since shuffling wouldn't make sense for uncompressed data,   <code>rekx</code> takes care to only add it along with compression levels greater than 0.</p> <ul> <li><code>caching</code> parameters will be set to the default values since we did not   specify them</li> </ul> <p>There are more options and we can list them with the typical --help option.</p> <p>The above command will generate</p> <pre><code>Writing generated commands into rechunk_commands_for_SISin202001010000004231000101MA.txt\nnccopy -c time/48,lat/64,lon/64 -d 0  -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_64_64_zlib_0.nc\nnccopy -c time/48,lat/64,lon/64 -d 3 -s -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_64_64_zlib_3_shuffled.nc\nnccopy -c time/48,lat/64,lon/64 -d 3  -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_64_64_zlib_3.nc\nnccopy -c time/48,lat/64,lon/64 -d 6 -s -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_64_64_zlib_6_shuffled.nc\nnccopy -c time/48,lat/64,lon/64 -d 6  -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_64_64_zlib_6.nc\nnccopy -c time/48,lat/64,lon/64 -d 9 -s -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_64_64_zlib_9_shuffled.nc\nnccopy -c time/48,lat/64,lon/64 -d 9  -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_64_64_zlib_9.nc\nnccopy -c time/48,lat/128,lon/128 -d 0  -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_128_128_zlib_0.nc\nnccopy -c time/48,lat/128,lon/128 -d 3 -s -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_128_128_zlib_3_shuffled.nc\nnccopy -c time/48,lat/128,lon/128 -d 3  -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_128_128_zlib_3.nc\nnccopy -c time/48,lat/128,lon/128 -d 6 -s -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_128_128_zlib_6_shuffled.nc\nnccopy -c time/48,lat/128,lon/128 -d 6  -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_128_128_zlib_6.nc\nnccopy -c time/48,lat/128,lon/128 -d 9 -s -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_128_128_zlib_9_shuffled.nc\nnccopy -c time/48,lat/128,lon/128 -d 9  -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_128_128_zlib_9.nc\nnccopy -c time/48,lat/256,lon/256 -d 0  -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_256_256_zlib_0.nc\nnccopy -c time/48,lat/256,lon/256 -d 3 -s -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_256_256_zlib_3_shuffled.nc\nnccopy -c time/48,lat/256,lon/256 -d 3  -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_256_256_zlib_3.nc\nnccopy -c time/48,lat/256,lon/256 -d 6 -s -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_256_256_zlib_6_shuffled.nc\nnccopy -c time/48,lat/256,lon/256 -d 6  -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_256_256_zlib_6.nc\nnccopy -c time/48,lat/256,lon/256 -d 9 -s -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_256_256_zlib_9_shuffled.nc\nnccopy -c time/48,lat/256,lon/256 -d 9  -h 16777216 -e 4133 -w SISin202001010000004231000101MA.nc rechunking_test/SISin202001010000004231000101MA_48_256_256_zlib_9.nc\n</code></pre>","tags":["How-To","rekx","CLI","NetCDF","Chunking","Caching","nccopy","Parallel processing","GNU Parallel"]},{"location":"how_to/chunking_parameterisation/#process-in-parallel","title":"Process in parallel","text":"<p>We let the mighty GNU Parallel execute these commands in parallel</p> <pre><code>parallel &lt; rechunk_commands_for_SISin202001010000004231000101MA.txt\n</code></pre> <p>While the output comprises as many new NetCDF files as the <code>nccopy</code> commands, for the sake of showcasing <code>rekx</code>, let us only inspect new NetCDF files whose filename contains the string <code>256</code> :</p> <pre><code>\u276f rekx inspect rechunking_test --repetitions 3 --humanize --long-table --variable-set data --pattern \"*256*\"\n</code></pre> <p>The above command will return</p> <pre><code>  Name                   Size        Dimensions            Variable   Shape              Chunks           Cache      Elements   Preemption   Type    Scale   Offset   Compression     Level   Shuffling   Read Time\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  SISin20200101000000\u2026   726.1 MiB   2 x 48 x 2600 x       SIS        48 x 2600 x 2600   48 x 256 x 256   16777216   4133       0.75         int16   -       -                        0       False       0.024\n                                     2600\n  SISin20200101000000\u2026   137.6 MiB   2 x 48 x 2600 x       SIS        48 x 2600 x 2600   48 x 256 x 256   16777216   4133       0.75         int16   -       -        zlib            6       False       0.025\n                                     2600\n  SISin20200101000000\u2026   137.4 MiB   2 x 48 x 2600 x       SIS        48 x 2600 x 2600   48 x 256 x 256   16777216   4133       0.75         int16   -       -        zlib            9       False       0.030\n                                     2600\n  SISin20200101000000\u2026   123.7 MiB   2 x 48 x 2600 x       SIS        48 x 2600 x 2600   48 x 256 x 256   16777216   4133       0.75         int16   -       -        zlib, shuffle   3       True        0.032\n                                     2600\n  SISin20200101000000\u2026   140.1 MiB   2 x 48 x 2600 x       SIS        48 x 2600 x 2600   48 x 256 x 256   16777216   4133       0.75         int16   -       -        zlib            3       False       0.032\n                                     2600\n  SISin20200101000000\u2026   120.9 MiB   2 x 48 x 2600 x       SIS        48 x 2600 x 2600   48 x 256 x 256   16777216   4133       0.75         int16   -       -        zlib, shuffle   6       True        0.034\n                                     2600\n  SISin20200101000000\u2026   119.8 MiB   2 x 48 x 2600 x       SIS        48 x 2600 x 2600   48 x 256 x 256   16777216   4133       0.75         int16   -       -        zlib, shuffle   9       True        0.034\n                                     2600\n\n                                               ^ Dimensions: lat x time x lon x bnds * Cache: Size in bytes, Number of elements, Preemption strategy ranging in [0, 1]\n</code></pre> <p>Info</p> <p>Note, the reported reading times are averages of repeated reads of the data in the memory to ensure we are really retrieving data values! Look for the <code>repetitions</code> parameter in <code>rekx inspect --help</code>.</p> <p>The output reports on dataset structure, chunking, compression levels, and the average time to read data over a geographic location. Analysing such results, can guide us in choosing an effective chunking shape and compression strategy in order to optimize our data structure.</p> <p>To get a machine readable output of such an analysis, <code>rekx</code> can write this out in a CSV file via the <code>--csv</code> option.</p> <p>Complete Me</p> <p>Complete documentation of this example!</p>","tags":["How-To","rekx","CLI","NetCDF","Chunking","Caching","nccopy","Parallel processing","GNU Parallel"]},{"location":"how_to/chunking_parameterisation/#see-also","title":"See also","text":"<ul> <li>A Utility to Help Benchmark Results: bm_file</li> </ul>","tags":["How-To","rekx","CLI","NetCDF","Chunking","Caching","nccopy","Parallel processing","GNU Parallel"]},{"location":"how_to/help/","title":"Help ?","text":"<p>For each and every command, there is a <code>--help</code> option. Please consult it to grasp the details for a command, its arguments and optional parameters, default values and settings that can further shape the output.</p> <p>For example,</p> <pre><code>rekx --help\n</code></pre> <pre><code>                                                                                \n Usage: rekx [OPTIONS] COMMAND [ARGS]...                                        \n                                                                                \n \ud83d\ude7e  \ud83e\udd96 Rekx command line interface prototype                                    \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --version                     Show the version and exit.                     \u2502\n\u2502 --log                         Enable logging.                                \u2502\n\u2502 --install-completion          Install completion for the current shell.      \u2502\n\u2502 --show-completion             Show completion for the current shell, to copy \u2502\n\u2502                               it or customize the installation.              \u2502\n\u2502 --help                        Show this message and exit.                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Diagnose chunking layout \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 inspect                   Inspect Xarray-supported data                      \u2502\n\u2502 shapes                    Diagnose chunking shapes in multiple               \u2502\n\u2502                           Xarray-supported data                              \u2502\n\u2502 validate-json             Validate chunk size consistency along multiple     \u2502\n\u2502                           Kerchunk reference files How to get available      \u2502\n\u2502                           variables?                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Suggest chunking layout \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 suggest                   Suggest a good chunking shape, ex.                 \u2502\n\u2502                           '8784,2600,2600' Needs a review!                   \u2502\n\u2502 suggest-alternative       Suggest a good chunking shape Merge to suggest     \u2502\n\u2502 suggest-symmetrical       Suggest a good chunking shape Merge to suggest     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Rechunk data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 modify-chunks             Modify in-place the chunk size metadata in NetCDF  \u2502\n\u2502                           files Yet not implemented!                         \u2502\n\u2502 rechunk                   Rechunk NetCDF file                                \u2502\n\u2502 rechunk-generator         Generate variations of rechunking commands for     \u2502\n\u2502                           multiple files                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Create references \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 reference                 Create Kerchunk JSON reference files               \u2502\n\u2502 reference-parquet         Create Parquet references to an HDF5/NetCDF file   \u2502\n\u2502                           Merge to reference                                 \u2502\n\u2502 reference-multi-parquet   Create Parquet references to multiple HDF5/NetCDF  \u2502\n\u2502                           files Merge to reference-parquet                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Combine references \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 combine                   Combine Kerchunk reference sets (JSONs to JSON)    \u2502\n\u2502 combine-to-parquet        Combine Kerchunk reference sets into a single      \u2502\n\u2502                           Parquet store (JSONs to Parquet)                   \u2502\n\u2502 combine-parquet-stores    Combine multiple Parquet stores (Parquets to       \u2502\n\u2502                           Parquet)                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Select from time series \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 clip                      %&lt; Select time series over a location              \u2502\n\u2502 select                    \ueaf1  Select time series over a location              \u2502\n\u2502 select-fast               \ueaf1  Bare read time series from Xarray-supported     \u2502\n\u2502                           data and optionally write to CSV  \u23f2 Performance    \u2502\n\u2502                           Test                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Select from Kerchunk references \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 select-json               \ueaf1  Select time series over a location from a JSON  \u2502\n\u2502                           Kerchunk reference set                             \u2502\n\u2502 select-parquet            \ueaf1 Select data from a Parquet references store      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500  \u23f2 Read performance  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 read-performance          \ueaf1  Measure read &amp; load in-memory operations of a   \u2502\n\u2502                           point time series from Xarray-supported data       \u2502\n\u2502 read-performance-area     \ueaf1  Measure read &amp; load in-memory operations of an  \u2502\n\u2502                           area time series from Xarray-supported data        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The help for the command <code>shapes</code></p> <pre><code>rekx shapes --help\n</code></pre> <pre><code>                                                                                \n Usage: rekx shapes [OPTIONS] SOURCE_DIRECTORY                                  \n                                                                                \n Diagnose chunking shapes in multiple Xarray-supported data                     \n\n\u256d\u2500 Time series \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    source_directory      PATH  Source directory path [default: None]       \u2502\n\u2502                                  [required]                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --pattern                                TEXT              Filename pattern  \u2502\n\u2502                                                            to match          \u2502\n\u2502                                                            [default: *.nc]   \u2502\n\u2502 --variable-set                           [all|coordinates  Set of Xarray     \u2502\n\u2502                                          |coordinates-wit  variables to      \u2502\n\u2502                                          hout-data|data|m  diagnose          \u2502\n\u2502                                          etadata|time]     [default: all]    \u2502\n\u2502 --validate-consi\u2026    --no-validate-c\u2026                      [default:         \u2502\n\u2502                                                            no-validate-cons\u2026 \u2502\n\u2502 --common-shapes      --no-common-sha\u2026                      Report common     \u2502\n\u2502                                                            maximum chunking  \u2502\n\u2502                                                            shape             \u2502\n\u2502                                                            [default:         \u2502\n\u2502                                                            no-common-shapes] \u2502\n\u2502 --help                                                     Show this message \u2502\n\u2502                                                            and exit.         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Input / Output \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --csv              PATH     CSV output filename [default: None]              \u2502\n\u2502 --verbose  -v      INTEGER  Show details while executing commands            \u2502\n\u2502                             [default: 0]                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>","tags":["rekx","CLI","How-To","Help","Logging","Debugging"]},{"location":"how_to/help/#verbosity","title":"Verbosity","text":"<p>Most of the commands feature an extra <code>--verbose</code> or shortly <code>-v</code> flag. It'll make <code>rekx</code> to be more communicative about what he did.</p> <p>For example check the difference of executing the same command without <code>-v</code></p> <pre><code>rekx shapes . --variable-set data --validate-consistency\n</code></pre> <pre><code>\ud83d\uddf4 Variables are not consistently shaped across all files!\n</code></pre> <p>and with <code>-v</code></p> <pre><code>rekx shapes . --variable-set data --validate-consistency -v\n</code></pre> <pre><code>\ud83d\uddf4 Variables are not consistently shaped across all files!\n                                      SIS                                      \n\n  Variable   Shape             Files                                           \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  SIS        1 x 1 x 2600      SISin202001010000004231000101MA.nc              \n  SIS        1 x 2600 x 2600   SISin200001010000004231000101MA_1_2600_2600.nc  \n</code></pre>","tags":["rekx","CLI","How-To","Help","Logging","Debugging"]},{"location":"how_to/help/#logging","title":"Logging","text":"<p><code>rekx</code> is growing and learning as we all do, by trial &amp; error :-). To get some background information on how <code>rekx</code> is crunching data, we can instruct the <code>--log</code> option right before any subcommand :</p> <pre><code>rekx --log inspect data/single_file/SISin202001010000004231000101MA.nc --variable-set data\n</code></pre> <pre><code>                       SISin202001010000004231000101MA.nc                       \n\n  V\u2026   Shape        Chunks    C\u2026   El\u2026   Pre\u2026        \u2026   Comp\u2026      Sh\u2026   Rea\u2026  \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  S\u2026   48 x 2600\u2026   1 x 1 \u2026   1\u2026   10\u2026   0.75        -   zlib       Fa\u2026   0.0\u2026  \n\nFile size: 181550165 bytes, Dimensions: time: 48, lon: 2600, bnds: 2, lat: 2600 \n    * Cache: Size in bytes, Number of elements, Preemption ranging in [0, 1]    \n</code></pre> <p>A <code>.log</code> is created containing timestamped details on the execution of important commands and their output.</p> <p>Example :</p> <pre><code>cat *.log\n</code></pre> <pre><code>2025-01-24 05:36:59.756 | INFO     | rekx.cli:main:75 - Logging initialized\n</code></pre>","tags":["rekx","CLI","How-To","Help","Logging","Debugging"]},{"location":"how_to/inspect/","title":"Inspect data","text":"<p><code>rekx</code> can diagnose the structure of data stored in Xarray-supported file formats.</p>","tags":["How-To","rekx","CLI","Diagnose","inspect"]},{"location":"how_to/inspect/#a-single-file","title":"A single file","text":"<p>Inspect a single NetCDF file</p> <pre><code>rekx inspect data/single_file/SISin202001010000004231000101MA.nc\n</code></pre> <pre><code>                                                              SISin202001010000004231000101MA.nc                                                               \n\n  Variable        Shape              Chunks         Cache      Elements   Preemption   Type      Scale   Offset   Compression   Level   Shuffling   Read Time  \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  lon_bnds        2600 x 2           2600 x 2       16777216   1000       0.75         float32   -       -        zlib          4       False       -          \n  SIS             48 x 2600 x 2600   1 x 1 x 2600   16777216   1000       0.75         int16     -       -        zlib          4       False       0.008      \n  lon             2600               2600           16777216   1000       0.75         float32   -       -        zlib          4       False       -          \n  lat             2600               2600           16777216   1000       0.75         float32   -       -        zlib          4       False       -          \n  time            48                 512            16777216   1000       0.75         float64   -       -        zlib          4       False       -          \n  lat_bnds        2600 x 2           2600 x 2       16777216   1000       0.75         float32   -       -        zlib          4       False       -          \n  record_status   48                 48             16777216   1000       0.75         int8      -       -        zlib          4       False       -          \n\n                                        File size: 181550165 bytes, Dimensions: time: 48, lon: 2600, bnds: 2, lat: 2600                                        \n                                           * Cache: Size in bytes, Number of elements, Preemption ranging in [0, 1]                                            \n</code></pre> <p>Perhaps restrict inspection on data variables only</p> <pre><code>rekx inspect data/single_file/SISin202001010000004231000101MA.nc --variable-set data\n</code></pre> <pre><code>                                                           SISin202001010000004231000101MA.nc                                                           \n\n  Variable   Shape              Chunks         Cache      Elements   Preemption   Type    Scale   Offset   Compression   Level   Shuffling   Read Time  \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  SIS        48 x 2600 x 2600   1 x 1 x 2600   16777216   1000       0.75         int16   -       -        zlib          4       False       0.008      \n\n                                    File size: 181550165 bytes, Dimensions: time: 48, lon: 2600, bnds: 2, lat: 2600                                     \n                                        * Cache: Size in bytes, Number of elements, Preemption ranging in [0, 1]                                        \n</code></pre> <p>Hint</p> <p><code>rekx</code> can scan selectively for the following <code>--variable-set</code>s : <code>[all|coordinates|coordinates-without-data|data|metadata|time]</code>. List them via <code>rekx inspect --help</code>.</p> <p>or even show humanised size figures</p> <pre><code>rekx inspect data/single_file/SISin202001010000004231000101MA.nc --variable-set data --humanize\n</code></pre> <pre><code>                                                           SISin202001010000004231000101MA.nc                                                           \n\n  Variable   Shape              Chunks         Cache      Elements   Preemption   Type    Scale   Offset   Compression   Level   Shuffling   Read Time  \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  SIS        48 x 2600 x 2600   1 x 1 x 2600   16777216   1000       0.75         int16   -       -        zlib          4       False       0.008      \n\n                                    File size: 173.1 MiB bytes, Dimensions: time: 48, lon: 2600, bnds: 2, lat: 2600                                     \n                                        * Cache: Size in bytes, Number of elements, Preemption ranging in [0, 1]                                        \n</code></pre>","tags":["How-To","rekx","CLI","Diagnose","inspect"]},{"location":"how_to/inspect/#a-directory-with-multiple-files","title":"A directory with multiple files","text":"<p>Let's consider a directory with 2 NetCDF files</p> <pre><code>ls -1 data/multiple_files_unique_shape/\n</code></pre> <pre><code>SISin202001010000004231000101MA.nc\nSISin202001010000004231000101MA_structure.csv\nSISin202001020000004231000101MA.nc\n</code></pre> <p>and inspect them all, in this case scanning only for data variables in the current directory</p> <p><pre><code>cd data/multiple_files_unique_shape/\nrekx inspect . --variable-set data\n</code></pre> <pre><code>  Name                                 Size        Dimensions             Variable   Shape              Chunks         Cache      Elements   Preemption   Type    Scale   Offset   Compression   Level   Shuffling   Read Time  \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  SISin202001010000004231000101MA.nc   181550165   2 x 48 x 2600 x 2600   SIS        48 x 2600 x 2600   1 x 1 x 2600   16777216   1000       0.75         int16   -       -        zlib          4       False       0.009      \n  SISin202001020000004231000101MA.nc   182167423   2 x 48 x 2600 x 2600   SIS        48 x 2600 x 2600   1 x 1 x 2600   16777216   1000       0.75         int16   -       -        zlib          4       False       0.009      \n\n                                  Dimensions: time x lat x bnds x lon | Cache size in bytes | Number of elements | Preemption strategy ranging in [0, 1] | Average time of 10 reads in seconds                                  \n</code></pre></p> <p>Info</p> <p>The <code>.</code> means in Linux the current working directory</p> <p>By default, multiple files are reported on a long table. For whatever the reason might be, we night not want this. We can instead ask for independent tables per input file :</p> <pre><code>rekx inspect data/multiple_files_unique_shape/ --variable-set data --no-long-table\n</code></pre> <pre><code>                                                           SISin202001010000004231000101MA.nc                                                           \n\n  Variable   Shape              Chunks         Cache      Elements   Preemption   Type    Scale   Offset   Compression   Level   Shuffling   Read Time  \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  SIS        48 x 2600 x 2600   1 x 1 x 2600   16777216   1000       0.75         int16   -       -        zlib          4       False       0.008      \n\n                                    File size: 181550165 bytes, Dimensions: time: 48, lon: 2600, bnds: 2, lat: 2600                                     \n                                        * Cache: Size in bytes, Number of elements, Preemption ranging in [0, 1]                                        \n                                                           SISin202001020000004231000101MA.nc                                                           \n\n  Variable   Shape              Chunks         Cache      Elements   Preemption   Type    Scale   Offset   Compression   Level   Shuffling   Read Time  \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  SIS        48 x 2600 x 2600   1 x 1 x 2600   16777216   1000       0.75         int16   -       -        zlib          4       False       0.008      \n\n                                    File size: 182167423 bytes, Dimensions: time: 48, lon: 2600, bnds: 2, lat: 2600                                     \n                                        * Cache: Size in bytes, Number of elements, Preemption ranging in [0, 1]                                        \n</code></pre>","tags":["How-To","rekx","CLI","Diagnose","inspect"]},{"location":"how_to/inspect/#csv-output","title":"CSV Output","text":"<p>We all need machine readable output. Here's how to get one for <code>rekx</code>' <code>inspect</code> command</p> <pre><code>rekx inspect SISin202001010000004231000101MA.nc --csv SISin202001010000004231000101MA_structure.csv\n</code></pre> <pre><code>Output written to SISin202001010000004231000101MA_structure.csv\n</code></pre> <p>Let's verify it worked well</p> <pre><code>file SISin202001010000004231000101MA_structure.csv\n</code></pre> <pre><code>SISin202001010000004231000101MA_structure.csv: ASCII text, with CRLF line terminators\n</code></pre> <p>Note</p> <p>Here's how it render's in this documentation page using mkdocs-table-reader-plugin</p> File Name File Size Variable Shape Type Compression Read time float32 - - zlib 4 False - float32 - - zlib 4 False - int8 - - zlib 4 False - float32 - - zlib 4 False - float32 - - zlib 4 False - int16 - - zlib 4 False 0.008 float64 - - zlib 4 False -","tags":["How-To","rekx","CLI","Diagnose","inspect"]},{"location":"how_to/kerchunk_to_json/","title":"Kerchunking to JSON","text":"","tags":["How-To","rekx","CLI","Kerchunk","JSON","SARAH3","SIS"]},{"location":"how_to/kerchunk_to_json/#example-data","title":"Example data","text":"<p>Let us work with the following example files from the SARAH3 climate data records </p> <pre><code>cd data/multiple_files_unique_shape/\nls -1\n</code></pre> <pre><code>SISin202001010000004231000101MA.nc\nSISin202001010000004231000101MA_structure.csv\nSISin202001020000004231000101MA.nc\n</code></pre>","tags":["How-To","rekx","CLI","Kerchunk","JSON","SARAH3","SIS"]},{"location":"how_to/kerchunk_to_json/#check-for-consistency","title":"Check for consistency","text":"<p>In order to create a Kerchunk reference, all datasets need to be identically shaped in terms of chunk sizes! Thus, let us confirm this is the case with our sample data :</p> <pre><code>rekx shapes . --validate-consistency\n</code></pre> <pre><code>\u2713 Variables are consistently shaped across all files!\n</code></pre> <p>Or we can add <code>-v</code> to report the shapes for each variable :</p> <pre><code>rekx shapes . --validate-consistency -v\n</code></pre> <pre><code>\u2713 Variables are consistently shaped across all files!\n</code></pre>","tags":["How-To","rekx","CLI","Kerchunk","JSON","SARAH3","SIS"]},{"location":"how_to/kerchunk_to_json/#reference-input-files","title":"Reference input files","text":"<p>We can proceed to create a JSON Kerchunk reference for each of the input NetCDF files.  However, before producing any new file, let's <code>--dry-run</code> the command in question to see what will happen :</p> <pre><code>rekx reference . sarah3_sis_kerchunk_references_json -v --dry-run\n</code></pre> <pre><code>Dry run of operations that would be performed:\n&gt; Reading files in . matching the pattern *.nc\n&gt; Number of files matched: 2\n&gt; Creating single reference files to sarah3_sis_kerchunk_references_json\n</code></pre> <p><code>--dry-run</code> is quite useful -- we need some indication things are right before engaging with real massive processing!</p> <p>This looks okay, so let's give it a real go </p> <pre><code>rekx reference . sarah3_sis_kerchunk_references_json -v\n</code></pre> <pre><code>\n</code></pre> <p>Note that Kerchunking processes run in parallel!</p> <p>The output of the above command is </p> <pre><code>tree sarah3_sis_kerchunk_references_json/\n</code></pre> <pre><code>sarah3_sis_kerchunk_references_json/\n\u251c\u2500\u2500 SISin202001010000004231000101MA.json\n\u251c\u2500\u2500 SISin202001010000004231000101MA.json.hash\n\u251c\u2500\u2500 SISin202001020000004231000101MA.json\n\u2514\u2500\u2500 SISin202001020000004231000101MA.json.hash\n\n1 directory, 4 files\n</code></pre>","tags":["How-To","rekx","CLI","Kerchunk","JSON","SARAH3","SIS"]},{"location":"how_to/kerchunk_to_json/#aggregate-references","title":"Aggregate references","text":"<p>Next, we want to cobine the single references into one file. Let's dry-run the <code>combine</code> command :</p> <pre><code>rekx combine sarah3_sis_kerchunk_references_json sarah3_sis_kerchunk_reference_json --dry-run\n</code></pre> <pre><code>Dry run of operations that would be performed:\n&gt; Reading files in sarah3_sis_kerchunk_references_json matching the pattern \n*.json\n&gt; Number of files matched: 2\n&gt; Writing combined reference file to sarah3_sis_kerchunk_reference_json\n</code></pre> <p>Warning</p> <p>In the above example not the subtle name difference : <code>sarah3_sis_kerchunk_references_json</code> != <code>sarah3_sis_kerchunk_reference_json</code></p> <p>This also looks fine. So let's create the single reference file</p> <pre><code>rekx combine sarah3_sis_kerchunk_references_json sarah3_sis_kerchunk_reference_json -v\n</code></pre> <pre><code>\n</code></pre> <p>The file <code>sarah3_sis_kerchunk_reference_json</code> has been created an seems to be a valid one</p> <pre><code>file sarah3_sis_kerchunk_reference_json\n</code></pre> <pre><code>sarah3_sis_kerchunk_reference_json: ASCII text, with very long lines (65536), with no line terminators\n</code></pre>","tags":["How-To","rekx","CLI","Kerchunk","JSON","SARAH3","SIS"]},{"location":"how_to/kerchunk_to_json/#test-it","title":"Test it!","text":"<p>Let's try to retrieve data over a geographic location though</p> <pre><code>rekx select-json sarah3_sis_kerchunk_reference_json SIS 8 45 --neighbor-lookup nearest -v\n</code></pre> <pre><code>\u2713 Coordinates : 8.0, 45.0.\n&lt;xarray.DataArray 'SIS' (time: 96)&gt; Size: 192B\n[96 values with dtype=int16]\nCoordinates:\n    lat      float32 4B 45.03\n    lon      float32 4B 8.025\n  * time     (time) datetime64[ns] 768B 2020-01-01 ... 2020-01-02T23:30:00\nAttributes:\n    cell_methods:   time: point\n    long_name:      Surface Downwelling Shortwave Radiation\n    missing_value:  -999\n    standard_name:  surface_downwelling_shortwave_flux_in_air\n    units:          W m-2\n    _FillValue:     -999\n</code></pre> <p>The final report of the data series over the location lon, lat <code>(8, 45)</code> verifies that Kerchunking worked as expected.</p>","tags":["How-To","rekx","CLI","Kerchunk","JSON","SARAH3","SIS"]},{"location":"how_to/kerchunk_to_parquet/","title":"Kerchunking to Parquet","text":"<p>Proof-of-Concept with an issue pending software updates</p> <p>The example works with not-yet-released versions of :</p> <ul> <li>filesystem_spec post commit 2e3f022</li> <li>Kerchunk post [commit b9659c3]</li> </ul> https://github.com/fsspec/kerchunk.git@b9659c32449539ef6addcb7a12520715cecf3253","tags":["To Do","How-To","rekx","CLI","Kerchunk","Parquet","SARAH3","SIS","read-performance"]},{"location":"how_to/kerchunk_to_parquet/#example-data","title":"Example data","text":"<p>This goes on with the same example data as in Kerchunking to JSON.</p> <pre><code>ls -1\nSISin202001010000004231000101MA.nc\nSISin202001020000004231000101MA.nc\nSISin202001030000004231000101MA.nc\nSISin202001040000004231000101MA.nc\n</code></pre>","tags":["To Do","How-To","rekx","CLI","Kerchunk","Parquet","SARAH3","SIS","read-performance"]},{"location":"how_to/kerchunk_to_parquet/#reference-to-parquet-store","title":"Reference to Parquet store","text":"<p>We create Parquet stores using the Kerchunk engine </p> <pre><code>rekx reference-multi-parquet . -v\nCreating the following Parquet stores in . :\n  SISin202001020000004231000101MA.parquet\n  SISin202001030000004231000101MA.parquet\n  SISin202001040000004231000101MA.parquet\n  SISin202001010000004231000101MA.parquet\nDone!\n</code></pre> <p>Let's check for the new files :</p> <pre><code>ls -1\nSISin202001010000004231000101MA.nc\nSISin202001010000004231000101MA.parquet\nSISin202001020000004231000101MA.nc\nSISin202001020000004231000101MA.parquet\nSISin202001030000004231000101MA.nc\nSISin202001030000004231000101MA.parquet\nSISin202001040000004231000101MA.nc\nSISin202001040000004231000101MA.parquet\n</code></pre> <p>There is one <code>.parquet</code> store for each input file.</p>","tags":["To Do","How-To","rekx","CLI","Kerchunk","Parquet","SARAH3","SIS","read-performance"]},{"location":"how_to/kerchunk_to_parquet/#combine-references","title":"Combine references","text":"<p>We then combine the multiple Parquet stores into a single one</p> <pre><code>rekx combine-parquet-stores . -v\nCombined reference name : combined_kerchunk.parquet\n</code></pre> <p>We verify the new file is there :</p> <pre><code>ls -1tr\nSISin202001030000004231000101MA.nc\nSISin202001010000004231000101MA.nc\nSISin202001020000004231000101MA.nc\nSISin202001040000004231000101MA.nc\nSISin202001010000004231000101MA.parquet\nSISin202001030000004231000101MA.parquet\nSISin202001020000004231000101MA.parquet\nSISin202001040000004231000101MA.parquet\ncombined_kerchunk.parquet\n</code></pre>","tags":["To Do","How-To","rekx","CLI","Kerchunk","Parquet","SARAH3","SIS","read-performance"]},{"location":"how_to/kerchunk_to_parquet/#verify","title":"Verify","text":"<p>Does it work ? We verify the aggregated Parquet store is readable</p> <pre><code>rekx read-performance combined_kerchunk.parquet SIS 8 45 -v\nData read in memory in : 0.132 \u26a1\u26a1\n</code></pre> <p><code>read-performance</code> won't show more than just the time it took to load the data in memory.  Let's go a step further and print out the values :</p> <pre><code>rekx select-parquet combined_kerchunk.parquet SIS 8 45\n\ud83d\uddf4 Something went wrong in selecting the data : \"not all values found in index 'lon'. Try setting the\n`method` keyword argument (example: method='nearest').\"\n</code></pre> <p>Error</p> <p>No panic! The above error is a good sign actually since there is no exact pair of coordinates at longitude, latitude : (8, 45) over which location to retrieve data.</p> <p>Let's get the closest pair of coordinates that really exists in the data by instructing the <code>--neighbor-lookup nearest</code> option :</p> <pre><code>rekx select-parquet combined_kerchunk.parquet SIS 8 45 --neighbor-lookup nearest\n0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 46.0, 114.0, 179.0, 238.0,\n290.0, 333.0, 359.0, 379.0, 377.0, 372.0, 344.0, 306.0, 262.0, 206.0, 137.0, 69.0, 0.0, 0.0, 0.0, 0.0,\n0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 46.0, 110.0, 175.0, 231.0, 291.0, 332.0, 356.0, 378.0, 376.0, 370.0,\n344.0, 308.0, 260.0, 203.0, 137.0, 69.0, 7.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.0, 61.0,\n74.0, 112.0, 142.0, 162.0, 185.0, 251.0, 251.0, 176.0, 152.0, 136.0, 111.0, 84.0, 65.0, 44.0, 3.0, 0.0,\n0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 46.0, 105.0, 173.0, 236.0, 259.0, 322.0, 371.0, 373.0, 382.0,\n358.0, 347.0, 311.0, 267.0, 205.0, 147.0, 74.0, 9.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n</code></pre> <p>or add <code>-v</code> for a Xarray-styled output</p> <pre><code>rekx select-parquet combined_kerchunk.parquet SIS 8 45 --neighbor-lookup nearest -v\n\u2713 Coordinates : 8.0, 45.0.\n&lt;xarray.DataArray 'SIS' (time: 192)&gt;\narray([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,  46., 114., 179., 238., 290., 333., 359., 379., 377.,\n       372., 344., 306., 262., 206., 137.,  69.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,  46., 110., 175., 231., 291., 332., 356., 378., 376.,\n       370., 344., 308., 260., 203., 137.,  69.,   7.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,  13.,  61.,  74., 112., 142., 162., 185., 251., 251.,\n       176., 152., 136., 111.,  84.,  65.,  44.,   3.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,  46., 105., 173., 236., 259., 322., 371., 373., 382.,\n       358., 347., 311., 267., 205., 147.,  74.,   9.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n      dtype=float32)\nCoordinates:\n    lat      float32 45.03\n    lon      float32 8.025\n  * time     (time) datetime64[ns] 2020-01-01 ... 2020-01-04T23:30:00\nAttributes:\n    cell_methods:   time: point\n    long_name:      Surface Downwelling Shortwave Radiation\n    standard_name:  surface_downwelling_shortwave_flux_in_air\n    units:          W m-2\n</code></pre> <p>Now it worked!  One more option : let's get a statistical overview instead :</p> <pre><code>rekx select-parquet combined_kerchunk.parquet SIS 8 45 --neighbor-lookup nearest --statistics\n                   Selected series\n\n           Statistic   Value\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n               Start   2020-01-01T00:00:00.000000000\n                 End   2020-01-04T23:30:00.000000000\n               Count   192\n\n                 Min   0.0\n     25th Percentile   0.0\n                Mean   72.97396\n              Median   0.0\n                Mode   0.0\n                 Max   382.0\n                 Sum   14011.0\n            Variance   14933.45\n  Standard deviation   122.2025\n\n         Time of Min   2020-01-01T00:00:00.000000000\n        Index of Min   0\n         Time of Max   2020-01-04T11:30:00.000000000\n        Index of Max   167\n\n                     Caption text\n</code></pre>","tags":["To Do","How-To","rekx","CLI","Kerchunk","Parquet","SARAH3","SIS","read-performance"]},{"location":"how_to/read_performance/","title":"Read performance","text":"<p>rekx can measure the average time it takes to read over a geographic location and load it im memory. It understands both Xarray-supported file formats as well as Kerchunk reference sets.</p> <p>Tip</p> <p>In fact, <code>rekx</code> can repeat the reading + loading operation as many times as you see fit ! Look out for the <code>repetitions</code> parameter.</p>","tags":["How-To","rekx","CLI","Performance","read-performance"]},{"location":"how_to/read_performance/#netcdf","title":"NetCDF","text":"<pre><code>\u276f rekx read-performance SISin202001040000004231000101MA.nc SIS 8 45 -v\nData read in memory in : 0.011 seconds \u26a1\u26a1\n&lt;xarray.DataArray 'SIS' (time: 48)&gt;\narray([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,  46, 105, 173, 236, 259, 322, 371, 373, 382, 358, 347,\n       311, 267, 205, 147,  74,   9,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int16)\nCoordinates:\n  * time     (time) datetime64 2020-01-04 ... 2020-01-04T23:30:00\n    lon      float32 8.025\n    lat      float32 45.03\nAttributes:\n    _FillValue:     -999\n    missing_value:  -999\n    standard_name:  surface_downwelling_shortwave_flux_in_air\n    long_name:      Surface Downwelling Shortwave Radiation\n    units:          W m-2\n    cell_methods:   time: point\n</code></pre> <p>or indeed let the read operation run 100 times</p> <pre><code>\u276f rekx read-performance SISin202001040000004231000101MA.nc SIS 8 45 -v --repetitions 100\nData read in memory in : 0.009 seconds \u26a1\u26a1\n&lt;xarray.DataArray 'SIS' (time: 48)&gt;\narray([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,  46, 105, 173, 236, 259, 322, 371, 373, 382, 358, 347,\n       311, 267, 205, 147,  74,   9,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int16)\nCoordinates:\n  * time     (time) datetime64 2020-01-04 ... 2020-01-04T23:30:00\n    lon      float32 8.025\n    lat      float32 45.03\nAttributes:\n    _FillValue:     -999\n    missing_value:  -999\n    standard_name:  surface_downwelling_shortwave_flux_in_air\n    long_name:      Surface Downwelling Shortwave Radiation\n    units:          W m-2\n    cell_methods:   time: point\n</code></pre>","tags":["How-To","rekx","CLI","Performance","read-performance"]},{"location":"how_to/read_performance/#json","title":"JSON","text":"<p>Support for JSON in-progress</p> <p><code>read-performance</code> still needs some refactoring to support JSON Kerchunk reference sets !</p>","tags":["How-To","rekx","CLI","Performance","read-performance"]},{"location":"how_to/read_performance/#parquet","title":"Parquet","text":"<p>See Kerchunking to Parquet.</p>","tags":["How-To","rekx","CLI","Performance","read-performance"]},{"location":"how_to/rechunk/","title":"<code>rekx rechunk</code>","text":"<p>Warning</p> <p><code>rekx rechunk</code> yet not implemented!</p>","tags":["To Do","How-To","rekx","CLI","Rechunk","NetCDF","nccopy"]},{"location":"how_to/rechunk/#nccopy","title":"<code>nccopy</code>","text":"<p>Reorganizing the data into chunks that include all timestamps in each chunk for a few lat and lon coordinates would greatly speed up such access. To chunk the data in the input file <code>slow.nc</code>, a netCDF file of any type, to the output file <code>fast.nc</code>, we can use <code>nccopy</code> :</p> <pre><code>nccopy -c time/1000,lat/40,lon/40 slow.nc fast.nc\n</code></pre> <p>to specify data chunks of 1000 times, 40 latitudes, and 40 longitudes.</p> <p>More : nccopy examples</p> <p>Given enough memory to contain the output file, the rechunking operation can be significantly speed up by creating the output in memory before writing it to disk on close:</p> <pre><code>nccopy -w -c time/1000,lat/40,lon/40 slow.nc fast.nc\n</code></pre>","tags":["To Do","How-To","rekx","CLI","Rechunk","NetCDF","nccopy"]},{"location":"how_to/rechunk/#rechunking-performance","title":"Rechunking Performance","text":"<p>Rechunking existing dataset can be time-consuming especially for large datasets. Tools like <code>nccopy</code> for netCDF-4 and <code>h5repack</code> for both HDF5 and netCDF-4 are available for rechunking. Usually it takes a small multiple of the time to copy the data from one file to another.</p>","tags":["To Do","How-To","rekx","CLI","Rechunk","NetCDF","nccopy"]},{"location":"how_to/rechunk/#example-of-nccopy-operations","title":"Example of <code>nccopy</code> operations","text":"<p>Timing the rechunking of SID files from the SARAH3 collection using <code>nccopy</code> on a laptop-with-ssd <sup>1</sup></p> <p>Why does it take 300+ seconds?</p> <p>Why does the duration of rechunking jump from ~15 to 300+ seconds for some chunkings shape combinations where the chunk size for <code>time</code> is 48 ?</p> Time Latitude Longitude Duration<sup>2</sup> 1 2600 2600 8.659999999999998 1 325 325 10.55 1 128 128 10.606666666666667 1 256 256 10.966666666666667 48 128 128 11.063333333333333 1 650 650 11.063333333333334 1 1300 1300 11.203333333333333 48 256 256 11.223333333333334 48 325 325 11.24 48 64 64 11.339999999999998 1 512 512 11.756666666666666 1 1024 1024 12.24 1 64 64 12.856666666666667 1 2048 2048 15.049999999999999 1 32 32 15.63 48 32 32 304.3833333333334 48 650 650 343.68333333333334 48 1300 1300 354.58 48 2600 2600 367.32 48 512 512 417.2133333333333 48 1024 1024 431.3666666666666 48 2048 2048 641.8233333333334 <ol> <li> <p>Laptop with SSD disk\u00a0\u21a9</p> </li> <li> <p>Average of 3 repetitions\u00a0\u21a9</p> </li> </ol>","tags":["To Do","How-To","rekx","CLI","Rechunk","NetCDF","nccopy"]},{"location":"how_to/select/","title":"Select","text":"<p>With <code>rekx</code> we can retrieve values from Xarray-supported data</p> <pre><code>cd data/single_file/\nrekx select SISin202001010000004231000101MA.nc SIS 8 45\n</code></pre> <pre><code>[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 114 179\n 238 290 333 359 379 377 372 344 306 262 206 137  69   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n</code></pre> <p>or print the regular Xarray-style data array overview</p> <pre><code>rekx select SISin202001010000004231000101MA.nc SIS 8 45 -v\n</code></pre> <pre><code>\u2713 Coordinates : 8.0, 45.0.\n&lt;xarray.DataArray 'SIS' (time: 48)&gt; Size: 96B\n[48 values with dtype=int16]\nCoordinates:\n  * time     (time) datetime64[ns] 384B 2020-01-01 ... 2020-01-01T23:30:00\n    lon      float32 4B 8.025\n    lat      float32 4B 45.03\nAttributes:\n    _FillValue:     -999\n    missing_value:  -999\n    standard_name:  surface_downwelling_shortwave_flux_in_air\n    long_name:      Surface Downwelling Shortwave Radiation\n    units:          W m-2\n    cell_methods:   time: point\n</code></pre>","tags":["How-To","rekx","CLI","select"]},{"location":"how_to/shapes/","title":"Chunking shape","text":"<p>The chunking shape refers to the chunk sizes of the variables typically found in a NetCDF file, or else any Xarray-supported file format. <code>rekx</code> can scan a <code>source_directory</code> for files that match a given <code>pattern</code> and report the chunking shapes across all of them.</p> <p>Given the following files in the current directory</p> <pre><code>ls -1 data/multiple_files_multiple_products/\n</code></pre> <pre><code>SISin200001010000004231000101MA_1_2600_2600.nc\nSISin202001010000004231000101MA.nc\nSRImm201301010000003231000101MA.nc\n</code></pre> <p>we scan for filenames starting with <code>SIS</code> and having the suffix <code>.nc</code> :</p> <pre><code>rekx shapes data/multiple_files_multiple_products/ --pattern \"SIS*.nc\"\n</code></pre> <pre><code>  Variable    Shapes        Files                                           C\u2026  \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  SIS         1 x 1 x 26\u2026   SISin202001010000004231000101MA.nc              1   \n  SIS         1 x 2600 x\u2026   SISin200001010000004231000101MA_1_2600_2600.\u2026   1   \n  lat_bnds    2600 x 2      SISin200001010000004231000101MA_1_2600_2600.\u2026   2   \n  time        512           SISin202001010000004231000101MA.nc              1   \n  time        1             SISin200001010000004231000101MA_1_2600_2600.\u2026   1   \n  record_s\u2026   48            SISin202001010000004231000101MA.nc              1   \n  record_s\u2026   1             SISin200001010000004231000101MA_1_2600_2600.\u2026   1   \n  lon_bnds    2600 x 2      SISin200001010000004231000101MA_1_2600_2600.\u2026   2   \n  lat         2600          SISin200001010000004231000101MA_1_2600_2600.\u2026   2   \n  lon         2600          SISin200001010000004231000101MA_1_2600_2600.\u2026   2   \n</code></pre> <p>or restrict the same scan to data variables only</p> <pre><code>rekx shapes data/multiple_files_multiple_products/ --pattern \"SIS*.nc\" --variable-set data\n</code></pre> <pre><code>  Varia\u2026   Shapes          Files                                          Cou\u2026  \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  SIS      1 x 1 x 2600    SISin202001010000004231000101MA.nc             1     \n  SIS      1 x 2600 x 2\u2026   SISin200001010000004231000101MA_1_2600_2600\u2026   1     \n</code></pre>","tags":["How-To","rekx","CLI","Diagnose","shapes"]},{"location":"how_to/shapes/#uniform-chunking-shape","title":"Uniform chunking shape?","text":"<p>We can also verify the uniqueness of one chunking shape across all input files. To exemplify, in a directory containing :</p> <pre><code>ls -1 data/multiple_files_unique_shape/\n</code></pre> <pre><code>SISin202001010000004231000101MA.nc\nSISin202001010000004231000101MA_structure.csv\nSISin202001020000004231000101MA.nc\nsarah3_sis_kerchunk_reference_json\nsarah3_sis_kerchunk_references_json\n</code></pre> <p>we navigate to the directory in question and scan for chunking shapes in the current directory</p> <pre><code>cd data/multiple_files_unique_shape/\nrekx shapes . --variable-set data\n</code></pre> <pre><code>  Variable   Shapes         Files                                   Count  \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  SIS        1 x 1 x 2600   SISin202001020000004231000101MA.nc ..   2      \n</code></pre> <p>We can verify the one and only chunking shape via <code>--validate-consistency</code></p> <pre><code>rekx shapes . --variable-set data --validate-consistency\n</code></pre> <pre><code>\u2713 Variables are consistently shaped across all files!\n</code></pre> <p>Else, let us scan another directory containing the files</p> <pre><code>ls data/multiple_files_multiple_shapes/\n</code></pre> <pre><code>SISin200001010000004231000101MA_1_2600_2600.nc\nSISin202001010000004231000101MA.nc\n</code></pre> <p>For the following shapes</p> <pre><code>cd data/multiple_files_multiple_shapes/\nrekx shapes . --variable-set data\n</code></pre> <pre><code>  Varia\u2026   Shapes          Files                                          Cou\u2026  \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  SIS      1 x 1 x 2600    SISin202001010000004231000101MA.nc             1     \n  SIS      1 x 2600 x 2\u2026   SISin200001010000004231000101MA_1_2600_2600\u2026   1     \n</code></pre> <p>we check for chunking consistency and expect a negative response since we have more than one shape :</p> <pre><code>\ud83d\uddf4 Variables are not consistently shaped across all files!\n</code></pre> <p>Interested for a long table ? Use the verbosity flag :</p> <pre><code>rekx shapes . --variable-set data --validate-consistency -v\n</code></pre> <pre><code>\ud83d\uddf4 Variables are not consistently shaped across all files!\n                                      SIS                                      \n\n  Variable   Shape             Files                                           \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  SIS        1 x 1 x 2600      SISin202001010000004231000101MA.nc              \n  SIS        1 x 2600 x 2600   SISin200001010000004231000101MA_1_2600_2600.nc  \n</code></pre>","tags":["How-To","rekx","CLI","Diagnose","shapes"]},{"location":"how_to/shapes/#maximum-common-chunking-shape","title":"Maximum common chunking shape","text":"<p>It might be useful to know the maximum common chunking shape across files of a product series, like the SIS or SID products  from the SARAH3 climate data records. </p> <p>Say for example a directory contains the following SARAH3 products :</p> <pre><code>ls data/multiple_files_multiple_products/\n</code></pre> <pre><code>SISin200001010000004231000101MA_1_2600_2600.nc\nSISin202001010000004231000101MA.nc\nSRImm201301010000003231000101MA.nc\n</code></pre> <p><code>rekx</code> will fetch the maximum common shapes like so :</p> <p>Output format subject to change</p> <p>The output format will likely be modified in one single table that features both the Shapes and Common Shape columns</p> <pre><code>rekx shapes . --variable-set data --common-shapes\n</code></pre> <pre><code>  Variable    Common Shape       \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  SRI         1 x 4 x 401 x 401  \n  kato_bnds   29 x 2             \n  SIS         1 x 2600 x 2600    \n\n\n  Variab\u2026   Shapes           Files                                          C\u2026  \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  SRI       1 x 4 x 401 x\u2026   SRImm201301010000003231000101MA.nc             1   \n  kato_b\u2026   29 x 2           SRImm201301010000003231000101MA.nc             1   \n  SIS       1 x 1 x 2600     SISin202001010000004231000101MA.nc             1   \n  SIS       1 x 2600 x 26\u2026   SISin200001010000004231000101MA_1_2600_2600\u2026   1   \n</code></pre>","tags":["How-To","rekx","CLI","Diagnose","shapes"]},{"location":"how_to/shapes/#consistency","title":"Consistency","text":"<p>Consider a case where we want only to know if our NetCDF data are uniformely chunked or not. No more or less than a yes or a no answer. <code>rekx</code> can <code>validate</code> for a uniform chunking shape across multiple NetCDF files.</p> <p>Let's list some NetCDF files which differ in terms of their chunking shapes :</p> <pre><code>ls data/multiple_files_multiple_products/*.nc\n</code></pre> <pre><code>data/multiple_files_multiple_products/SISin200001010000004231000101MA_1_2600_2600.nc\ndata/multiple_files_multiple_products/SISin202001010000004231000101MA.nc\ndata/multiple_files_multiple_products/SRImm201301010000003231000101MA.nc\n</code></pre> <p>From the file names, we expect to have at least two different chunking shapes. Let's try first with the files named after a common pattern :</p> <pre><code>cd data/multiple_files_multiple_products/\nrekx shapes . --pattern \"SIS*MA.nc\" --validate-consistency\n</code></pre> <pre><code>\u2713 Variables are consistently shaped across all files!\n</code></pre> <p>Indeed, the requested files are chunked identically. What about the other file <code>SISin200001010000004231000101MA_1_2600_2600.nc</code> ?</p> <pre><code>cd data/multiple_files_multiple_products/\nrekx shapes . --validate-consistency\n</code></pre> <pre><code>\ud83d\uddf4 Variables are not consistently shaped across all files!\n</code></pre> <p>Voil\u00e0, this is a no !</p>","tags":["How-To","rekx","CLI","Diagnose","shapes"]},{"location":"how_to/suggest/","title":"Suggest","text":"<p>Experimental</p> <p>Would this make sense ?</p>","tags":["To Do","How-To","rekx","CLI","suggest","Experimental"]},{"location":"install/install/","title":"Install","text":"<p>Experimental</p> <p>Everything is under heavy development and subject to change!</p>","tags":["rekx","Python","install","pip","direnv","virtual environment"]},{"location":"install/install/#virtual-environment","title":"Virtual environment","text":"<p>Before all, create a virtual environment!</p> Tip <p>Regardless of our favourite programming language or tool to manage environments, chances are high we'd benefit from using <code>direnv</code>. In the context of a Python package, like <code>rekx</code>, <code>direnv</code> supports all such well known tools from standard <code>venv</code>, <code>pyenv</code> and <code>pipenv</code>, to <code>anaconda</code>, <code>Poetry</code>, <code>Hatch</code>, <code>Rye</code> and <code>PDM</code>. Have a look at direnv's Wiki page for Python.</p> <p><code>rekx</code> is developed inside a virtual environment (re-)created and (re-)activated via <code>direnv</code>. The following <code>.envrc</code> does all of it :</p> .envrc<pre><code>\n</code></pre> <p>Find more about <code>layout python</code> in direnv/wiki/Python#venv-stdlib-module.</p>","tags":["rekx","Python","install","pip","direnv","virtual environment"]},{"location":"install/install/#pip-install","title":"<code>pip install</code>","text":"<p>Once inside a dedicated virtual environment, we can install <code>rekx</code> using <code>pip</code> : </p> <pre><code>pip install git+https://github.com/NikosAlexandris/rekx\n</code></pre>","tags":["rekx","Python","install","pip","direnv","virtual environment"]},{"location":"install/install/#pip-uninstall","title":"<code>pip uninstall</code>","text":"<p>Done with <code>rekx</code> ?  Uninstall via</p> <pre><code>pip uninstall rekx\n</code></pre> <p>Of course, we can remove the entire virtual environment we created for it!</p> Clean pip cache? <p>In case we need to clean it from the cache too, we can do</p> <pre><code>pip cache remove rekx\n</code></pre>","tags":["rekx","Python","install","pip","direnv","virtual environment"]},{"location":"install/install/#verify","title":"Verify","text":"<pre><code>..\n</code></pre>","tags":["rekx","Python","install","pip","direnv","virtual environment"]},{"location":"meta/","title":"Index","text":"<p>Warning</p> <p>Complete me!</p> <p><code>rekx</code> </p> <ul> <li>builds its command-line interface on top of the awesome Typer</li> <li>interfaces the Kerchunk library</li> <li>netcdf4-python</li> <li>uses Xarray and <code>zarr</code> for data handling and storage</li> </ul> <ul> <li>uses <code>numpy</code> for some numerical operations</li> <li>relies on <code>pandas</code> for datetime indexing and exporting data to CSV</li> <li>wraps over <code>fastparquet</code> and <code>kerchunk</code>, of course, for specific data format handling</li> </ul>","tags":["Metadata","Dependencies","To Do"]},{"location":"meta/#code-formatting","title":"Code Formatting","text":"<p><code>rekx</code> uses Black for code style consistency throughout the project. It is setup as a pre-commit hook and run with each git commit.</p>","tags":["Metadata","Dependencies","To Do"]},{"location":"reference/","title":"Index","text":"<p>Warning</p> <p>To sort out!</p>","tags":["Context","Background","Chunking"]},{"location":"reference/#context","title":"Context","text":"","tags":["Context","Background","Chunking"]},{"location":"reference/#first-time-questions","title":"First time questions","text":"<ul> <li>Data format?</li> <li>Chunking strategy?</li> <li>Reference file customization?</li> </ul>","tags":["Context","Background","Chunking"]},{"location":"reference/#background","title":"Background","text":"","tags":["Context","Background","Chunking"]},{"location":"reference/#challenges-with-long-term-observations","title":"Challenges with long-term observations","text":"<ul> <li> <p>Size, Volume</p> <ul> <li>example : half-hourly SARAH3 daily netCDF files are \\(~150\\) - \\(180\\) MB each</li> <li>or \\(10\\) years \\(\\approx0.5\\) TB</li> </ul> </li> <li> <p>Format</p> <ul> <li>Past: CSV, JSON, XML</li> <li>Present: HDF5, NetCDF</li> </ul> </li> <li> <p>Metadata extraction</p> </li> <li> <p>Data access</p> <ul> <li>concurrent &amp; parallel access</li> </ul> </li> </ul>","tags":["Context","Background","Chunking"]},{"location":"reference/#data-models","title":"Data models","text":"<ul> <li>Binary Buffers</li> </ul> <pre><code>full_array = read_entire_file(\"10_year_data.hdf5\")\none_day_data = full_array[0:24]\n</code></pre> <ul> <li> <p>HDF5</p> </li> <li> <p>yet not cloud optimised</p> </li> <li>H5coro : cloud-optimised read-only library</li> </ul> <p>HDF5 supports direct reading from cloud storage, whether over HTTP or by passing fsspec instances.</p>","tags":["Context","Background","Chunking"]},{"location":"reference/chunking/","title":"Chunking","text":"<p>Warning</p> <p>To sort out!</p>","tags":["Context","Chunking"]},{"location":"reference/chunking/#what-is-chunking","title":"What is Chunking ?","text":"<p>Chunking splits data for easier reading.</p> <p>In physical terms, a data variable is contiguous. However, and especially in the case of NetCDF/HDF files, the physical storage of the data on disk, is chunked in fixed and equal-sized pieces.</p> <p>Original</p> Hex Text <code>01 02 03 04 05 06 07 08 09</code> <code>1, 2, 3, 4, 5, 6, 7, 8, 9</code> <p>Chunked</p> Hex Text Size Number <code>AA 05 01</code> <code>01 02 03 04 05</code> <code>AA 05 01</code> <code>06 07 08 09 -</code> <code>[1, 2, 3, 4, 5]</code> <code>[6, 7, 8, 9, -]</code> 5 2 <code>AA 04 01</code> <code>01 02 03 04</code> <code>AA 04 01</code> <code>05 06 07 08</code> <code>AA 04 01</code> <code>09 - - -</code> <code>[1, 2, 3, 4]</code> <code>[5, 6, 7, 8]</code> <code>[9, -, -, -]</code> 4 3 <code>AA 03 01</code> <code>01 02 03</code> <code>AA 03 01</code> <code>04 05 06</code> <code>AA 03 01</code> <code>07 08 09</code> <code>[1, 2, 3]</code> <code>[4, 5, 6]</code> <code>[7, 8, 9]</code> 3 3 <code>AA 02 01</code> <code>01 02</code> <code>AA 02 01</code> <code>03 04</code> <code>AA 02 01</code> <code>05 06</code> <code>AA 02 01</code> <code>07 08</code> <code>AA 02 01</code> <code>09 -</code> <code>[1, 2]</code> <code>[3, 4]</code> <code>[5, 6]</code> <code>[7, 8]</code> <code>[9, -]</code> 2 5 <p><code>AA 0? 01</code> sequences mark the start of a chunk</p>","tags":["Context","Chunking"]},{"location":"reference/chunking/#why","title":"Why ?","text":"<p>While chunking does not affect the logical relationship of data of a variable, the read/write operations may be impacted heavily. Chunking can optimize read operations for accessing data in various ways :</p> <pre><code>- by rows\n- by columns\n- as a rectangular subgrid\n</code></pre> <p>The idea is to minimize the number of disk accesses.</p>","tags":["Context","Chunking"]},{"location":"reference/chunking/#how","title":"How ?","text":"<p>By aligning chunk sizes/borders with the most common or preferred data access patterns.</p>","tags":["Context","Chunking"]},{"location":"reference/chunking/#good-chunking-shapes","title":"Good chunking shapes","text":"<ul> <li> <p>For 2D data,      rectangular chunks help balance the disk access times for      both row-wise and column-wise access.</p> </li> <li> <p>For 3D data or higher,      the chunking strategy may need to be adjusted based      on the most common access patterns.</p> </li> </ul>","tags":["Context","Chunking"]},{"location":"reference/chunking/#optimal-layout","title":"Optimal layout?","text":"<p>An algorithm discussed in  \"Chunking Data: Choosing Shapes\", by RussRew :  chunk_shape_3D.py</p> <p>Seealso</p> <ul> <li>How to <code>suggest</code> a good chunking shape with <code>rekx</code> (Experimental)</li> <li>Source code reference for <code>suggest</code></li> </ul> <p>An algebraic formulation for optimal chunking bases on equalizing the number of chunks accessed for a 1D time series and a 2D horizontal slice in a 3D dataset.</p> <p>Note</p> <p><code>rekx</code> prefers to avoid the word optimal, as possible. There is no one-size-fits-all and hence, it may be more appropriate to speak about good, appropriate or preferred chunking.</p> <p>Let \\(D\\) be the number of values you want in a chunk, \\(N\\) be the total number of chunks used to partition the array, and \\(c\\) be a scaling factor derived from \\(D\\) and the dimensions of the array. The chunk shape would then be given by the formula:</p> \\[c = \\left( \\frac{D}{{25256 \\times 37 \\times 256 \\times 512}} \\right)^{1/4}\\] <p>The resulting chunk shape is obtained by multiplying each dimension size by \\(c\\) and truncating to an integer. The chunk shape will thus be:</p> \\[\\text{chunk shape} = \\left( \\left\\lfloor 25256 \\times c \\right\\rfloor, \\left\\lfloor 37 \\times c \\right\\rfloor, \\left\\lfloor 256 \\times c \\right\\rfloor, \\left\\lfloor 512 \\times c \\right\\rfloor \\right)\\] <p>This formula assumes that the optimal chunk shape will distribute the chunks equally along each dimension, and the scaling factor \\(c\\) is calculated to ensure the total number of values in a chunk is close to \\(D\\), without exceeding it.</p>","tags":["Context","Chunking"]},{"location":"reference/chunking/#size-and-number-of-chunks","title":"Size and number of chunks","text":"<p>Tip</p> <p>Important is the total number of chunks.</p> <p>Chunks affect both memory usage and processing efficiency. An important distinction for efficient data handling is between the size and the number of chunks in a dataset. The <code>chunks</code> keyword in NetCDF, Xarray and similar libraries, specifies the size of each chunk in a dimension, not the number of chunks! Thus, the smaller a chunk size, the larger the number of chunks and vice versa.</p> <p>Consequently, decreasing the size of chunks will increase the number of chunks which in turn can lead to higher memory overhead due to the larger number of chunks, rather than direct memory consumption of the data.</p> Chunking shape Size or Number of elements in chunk Number of chunks <code>{'time':1, 'y':768, 'x':922}</code> (708,096) Smaller Larger <code>{'time':168, 'y':384, 'x':288}</code> (18,579,456) Larger Smaller <p>While a larger number of smaller chunks might increase overhead, it doesn't necessarily mean a higher memory footprint. Memory usage is not exclusively determined by the size or number of chunks. Rather it depends by how many chunks are loaded into memory at once. Hence, even if we have a large number of small chunks, it won't necessarily increase the total memory used by the data, as long as we don't load all chunks into memory simultaneously.</p> <p>Memory usage depends on the number of chunks loaded into memory at once.</p> <p>The focus is on the efficiency and optimization of data processing and access, rather than memory usage implied by the number and size of the chunks.</p>","tags":["Context","Chunking"]},{"location":"reference/chunking/#compression","title":"Compression","text":"<ul> <li>Chunking splits the data in equal-sized blocks or chunks of a pre-defined size</li> <li>Only the chunks of data required are accessed</li> <li>The HDF5 file format stores compressed data in chunks </li> <li>a chunk is the atomic unit of compression as well as disk access</li> <li>thus, compressed data is forcedly chunked</li> <li> <p>rechunking compressed data involves several steps:</p> <p><code>read</code> \\(\\rightarrow\\) <code>uncompress</code> \\(\\rightarrow\\) <code>rechunk</code> \\(\\rightarrow\\) <code>recompress</code> \\(\\rightarrow\\) <code>write</code> new chunks</p> </li> <li> <p>rechunking compressed data can sometimes be faster due to savings in disk   I/O!</p> </li> </ul> <p>Chunking is required for :     - compression and other filters     - creating extendible or unlimited dimension datasets     - subsetting very large datasets to improve performance</p> <p>While chunking can improve performance for large datasets, using a chunking layout without considering the consequences of the chunk size, can lead to poor performance. Unfortunately, it is easy to end up with some random and inefficient chunking layout due to ...</p>","tags":["Context","Chunking"]},{"location":"reference/chunking/#caching","title":"Caching","text":"<p>To Do</p> <p>Discuss about caching in HDF5!</p>","tags":["Context","Chunking"]},{"location":"reference/chunking/#problems","title":"Problems","text":"<p>Issues that can cause performance problems with chunking include:</p> <ul> <li> <p>Very small chunks can create very large datasets which can degrade access performance.</p> <ul> <li>The smaller the chunk size the more chunks that HDF5 has to keep track of, and the more time it will take to search for a chunk.</li> </ul> </li> <li> <p>Very large chunks need to be read and uncompressed entirely before any access operation.</p> <ul> <li>There can be a performance penalty for reading a small subset, if the chunk size is substantially larger than the subset. Also, a dataset may be larger than expected if there are chunks that only contain a small amount of data.</li> </ul> </li> <li> <p>Smaller chunk-cache than the predefined chunk size. </p> <ul> <li>A chunk does not fit in the Chunk Cache. Every chunked dataset has a chunk cache associated with it that has a default size of 1 MB. The purpose of the chunk cache is to improve performance by keeping chunks that are accessed frequently in memory so that they do not have to be accessed from disk.</li> <li>If a chunk is too large to fit in the chunk cache, it can significantly degrade performance.</li> </ul> </li> </ul>","tags":["Context","Chunking"]},{"location":"reference/chunking/#recommendations","title":"Recommendations","text":"<p>It is a good idea to:</p> <ul> <li>Avoid very small chunk sizes</li> <li>Be aware of the 1 MB chunk cache size default</li> <li>Test the data with different chunk sizes to determine the optimal chunk size to use.</li> <li>Consider the chunk size in terms of the most common access patterns for the data.</li> </ul>","tags":["Context","Chunking"]},{"location":"reference/chunking/#resources","title":"Resources","text":"<p>See also :</p> <p>Information regarding chunking and various data structure characteristics is largely sourced from resources provided by Unidata. Recognized as an authoritative entity, Unidata is not only the developer but also the maintainer of NetCDF.</p> <ul> <li> <p>Chunking Data: Choosing Shapes</p> </li> <li> <p>Parallel I/O using netCDF, National Supercomputing Service (CSCS)</p> </li> <li> <p>Chunking and Deflating Data with NetCDF-4 - 2012 Unidata NetCDF Workshop</p> </li> <li> <p>Chunking and Deflating Data with NetCDF-4 - 2011 Unidata NetCDF Workshop<sup>1</sup></p> </li> <li> <p>Support on Chunking in HDF5 -- No longer maintained</p> </li> <li> <p>Caching in HDF5</p> </li> <li> <p>https://docs.hdfgroup.org/hdf5/develop/_l_b_com_dset.html</p> </li> <li> <p>https://docs.hdfgroup.org/hdf5/develop/_l_b_dset_layout.html</p> </li> <li> <p>https://ntrs.nasa.gov/api/citations/20180008456/downloads/20180008456.pdf</p> </li> <li> <p>https://www.hdfgroup.org/2017/05/hdf5-data-compression-demystified-2-performance-tuning/</p> </li> <li> <p>https://docs.unidata.ucar.edu/nug/current/netcdf_perf_chunking.html#default_chunking_4_1</p> </li> </ul> <ol> <li> <p>Unidata. NetCDF-4 Chunking - Workshop 2011. \\url https://www.unidata.ucar.edu/software/netcdf/workshops/2011/nc4chunking/. Accessed: 10. 01. 2024.\u00a0\u21a9</p> </li> </ol>","tags":["Context","Chunking"]},{"location":"reference/concepts/","title":"Concepts","text":"<p>Warning</p> <p>Unsorted notes</p>","tags":["Concepts","Parallel","Concurrent","Chunking","Consolidation","Asynchronous","Serverless","Frontend","Backend"]},{"location":"reference/concepts/#parallel","title":"Parallel","text":"<p>Parallel are operations running simultaneously yet independently in multiple threads, processes or machines. For CPU-intensive workloads, past the overhead of arranging such a system, a speedup equal to the number of CPU cores is possible.</p> Parallel access with Scenario Lock Resource A (locked) -&gt; Process 1 No Lock Resource A \\(\\longrightarrow\\) Process 1Resource A \\(\\longrightarrow\\) Process 2","tags":["Concepts","Parallel","Concurrent","Chunking","Consolidation","Asynchronous","Serverless","Frontend","Backend"]},{"location":"reference/concepts/#concurrent","title":"Concurrent","text":"<p>Concurrent are multiple operations managed during overlapping periods yet not necessarily executed at the exact same instant. For the particular case of cloud storage, the latency to get the first byte of a read can be comparable or dominate the total time for a request. Practically, launching many requests, and only pay the overhead cost once (they all wait together), enables a large speedup.</p> Execution Task Non-Concurrent A \\(\\rightarrow\\) Run \\(\\rightarrow\\) Complete \\(\\longrightarrow\\) B \\(\\rightarrow\\) Run \\(\\rightarrow\\) Complete Concurrent A \\(\\rightarrow\\) .. \\(\\rightarrow\\) Complete .. B \\(\\longrightarrow\\) .. \\(\\rightarrow\\) CompleteC \\(\\longrightarrow\\) .. \\(\\longrightarrow\\) Complete .. .. D \\(\\longrightarrow\\) .. \\(\\rightarrow\\) Complete .. E \\(\\rightarrow\\) .. \\(\\longrightarrow\\) Complete","tags":["Concepts","Parallel","Concurrent","Chunking","Consolidation","Asynchronous","Serverless","Frontend","Backend"]},{"location":"reference/concepts/#chunks","title":"Chunks","text":"<p>Reading a chunk?</p> Byte range Index Original <code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</code> Selection <code>[3, 4, 5, 6]</code>","tags":["Concepts","Parallel","Concurrent","Chunking","Consolidation","Asynchronous","Serverless","Frontend","Backend"]},{"location":"reference/concepts/#compression","title":"Compression","text":"<p>Warning</p> <p>To Do !</p>","tags":["Concepts","Parallel","Concurrent","Chunking","Consolidation","Asynchronous","Serverless","Frontend","Backend"]},{"location":"reference/concepts/#descriptive-metadata","title":"Descriptive metadata","text":"Compression Size % Decompressed <code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</code> 0 Compressed <code>[0, 1, 2, 3, 4, 5, 6]</code> 30","tags":["Concepts","Parallel","Concurrent","Chunking","Consolidation","Asynchronous","Serverless","Frontend","Backend"]},{"location":"reference/concepts/#consolidation","title":"Consolidation","text":"<p>A single indexible aggregate dataset</p> Consolidation Data Parts Scattered <code>[-]</code> <code>[-]</code> <code>[-]</code> 3 Consolidated <code>[-----------]</code> 1 Aggregation Simple Simple Virtual File A B V Points to \\(\\rightarrow\\) A B A, B <p>Metadata consolidation in a Zarr context, is the combination of all separate metadata files associated with the different arrays and groups within a Zarr hierarchy into a single metadata file. It is a performance optimization technique that reduces the number of read operations required to access metadata. It can be particularly beneficial when working with remote or distributed storage systems.</p>","tags":["Concepts","Parallel","Concurrent","Chunking","Consolidation","Asynchronous","Serverless","Frontend","Backend"]},{"location":"reference/concepts/#asynchronous","title":"Asynchronous","text":"<p>Asynchronous is a mechanism performing tasks without waiting for other tasks to complete.</p> Operation Execution Sequential A \\(\\longrightarrow\\) Complete \\(\\rightarrow\\) B \\(\\longrightarrow\\) Complete \\(\\rightarrow\\) C \\(\\longrightarrow\\) Complete Asynchronous A \\(\\longrightarrow\\) .. \\(\\longrightarrow\\) CompleteB \\(\\longrightarrow\\) .. .. \\(\\longrightarrow\\) CompleteC \\(\\longrightarrow\\) .. \\(\\longrightarrow\\) Complete","tags":["Concepts","Parallel","Concurrent","Chunking","Consolidation","Asynchronous","Serverless","Frontend","Backend"]},{"location":"reference/concepts/#serverless","title":"Serverless","text":"<p>Serverless is a deployment of code in a cloud service which in turn handles server maintenance, scaling, updates and more. </p> Deployment Management Traditional Manual server, maintenance, scaling, updates, .. Serverless Automated cloud service, deployment, scaling, ..","tags":["Concepts","Parallel","Concurrent","Chunking","Consolidation","Asynchronous","Serverless","Frontend","Backend"]},{"location":"reference/concepts/#front-and-back-end","title":"Front- and Back-end","text":"Component Role Backend Data storage, algorithms, API, processing, serving Frontend User interface &amp; experience using a browsers or application","tags":["Concepts","Parallel","Concurrent","Chunking","Consolidation","Asynchronous","Serverless","Frontend","Backend"]},{"location":"reference/dask/","title":"Chunking &amp; Dask","text":"<ul> <li>https://docs.dask.org/en/latest/array-best-practices.html#select-a-good-chunk-size</li> <li>https://docs.dask.org/en/latest/array-best-practices.html#orient-your-chunks</li> </ul>","tags":["To Do","Dask","Chunking","Best practices"]},{"location":"reference/kerchunking/","title":"Kerchunk","text":"<p>Warning</p> <p>Unsorted notes</p> <p>kerchunk supports cloud-friendly access of data with specific reference to netCDF4/HDF5 files.<sup>1</sup></p> <p>How? Kerchunk</p> <ul> <li>extracts metadata in a single scan</li> <li>arranges multiple chunks from multiple files</li> <li>with dask and zarr,   reads chunks in parallel and/or   concurrently within a single indexible aggregate dataset</li> </ul>","tags":["To Do","Kerchunk"]},{"location":"reference/kerchunking/#advantages","title":"+ advantages","text":"<ul> <li>supports parallel and concurrent reads  </li> <li>memory efficiency</li> <li>parallel processing</li> <li>data locality</li> </ul>","tags":["To Do","Kerchunk"]},{"location":"reference/kerchunking/#-drawbacks","title":"- drawbacks","text":"<ul> <li>?</li> </ul>","tags":["To Do","Kerchunk"]},{"location":"reference/kerchunking/#how-does-it-work","title":"How does it work?","text":"<ul> <li>Combines <code>fsspec</code>, <code>dask</code>, and <code>zarr</code></li> </ul> <pre><code>one_day_data = read_chunk(\"10_year_data_chunked.hdf5\", chunk_index=0)\n</code></pre> <ul> <li>Reference file :</li> </ul> <pre><code>{\n  \"version\": 1,\n  \"shapes\": {\"var1\": [365, 24]},\n  \"refs\": {\n    \"var1/0\": [\"file_1.nc\", \"0:24\"],\n    \"var1/1\": [\"file_2.nc\", \"0:24\"],\n    // ...\n  }\n}\n</code></pre> <ol> <li> <p>Development supported by NASA fundung https://doi.org/10.6084/m9.figshare.22266433.v1 \u21a9</p> </li> <li> <p>see Parallel \u21a9</p> </li> <li> <p>see Concurrency \u21a9</p> </li> </ol>","tags":["To Do","Kerchunk"]},{"location":"reference/notes/","title":"Unsorted notes","text":"<p>Warning</p> <p>To sort out!</p>","tags":["To Do","Notes","Unsorted"]},{"location":"reference/notes/#ssd-vs-spinning-disk","title":"SSD vs Spinning Disk","text":"<ul> <li>Solid State Drives (SSD) can significantly speed up the rechunking process      and data access times as compared to traditional spinning disks.</li> </ul>","tags":["To Do","Notes","Unsorted"]},{"location":"reference/xarray/","title":"Chunking &amp; Xarray","text":"<p>Warning</p> <p>To sort out!</p> <p>See useful hints at :</p> <ul> <li>Chunking and performance</li> <li>Optimisation tips</li> <li>Dask array best practices.</li> </ul>","tags":["To Do","Xarray","Chunking","Performance","Optimisation","Best practices"]},{"location":"references/references/","title":"References","text":"<ol> <li> <p>European Commission, Directorate-General for Digital Services, and P Schmitz. European Union Public Licence (EUPL) \u2013 Guidelines July 2021. Publications Office, 2021. doi:doi/10.2799/77160.\u00a0\u21a9</p> </li> <li> <p>Martin Durant, Max Jones, Ryan Abernathey, David Hoese, and James Bednar. Pangeo-ML Augmentation - Enabling Cloud-native access to archival data with Kerchunk. 3 2023. URL: https://figshare.com/articles/preprint/Pangeo-ML_Augmentation_-_Enabling_Cloud-native_access_to_archival_data_with_Kerchunk/22266433, doi:10.6084/m9.figshare.22266433.v1.\u00a0\u21a9</p> </li> <li> <p>Unidata. NetCDF-4 Chunking - Workshop 2012. \\url https://www.unidata.ucar.edu/software/netcdf/workshops/most-recent/nc4chunking/index.html. Accessed: 10. 01. 2024.\u00a0\u21a9</p> </li> <li> <p>Unidata. NetCDF Factsheet. \\url https://www.unidata.ucar.edu/publications/factsheets/current/factsheet_netcdf.pdf. Accessed: 10. 01. 2024.\u00a0\u21a9</p> </li> <li> <p>Unidata. NetCDF-4 Chunking - Workshop 2011. \\url https://www.unidata.ucar.edu/software/netcdf/workshops/2011/nc4chunking/. Accessed: 10. 01. 2024.\u00a0\u21a9</p> </li> <li> <p>Uwe Pfeifroth, Steffen Kothe, Jaqueline Dr\u00fccke, J\u00f6rg Trentmann, Marc Schr\u00f6der, Nathalie Selbach, and Rainer Hollmann. Surface radiation data set - heliosat (sarah) - edition 3. 2023. URL: https://wui.cmsaf.eu/safira/action/viewDoiDetails?acronym=SARAH_V003, doi:10.5676/EUM_SAF_CM/SARAH/V003.\u00a0\u21a9</p> </li> </ol>"},{"location":"references/see_also/","title":"Relevant projects","text":"<p>Projects to learn from and get inspired</p> <ul> <li>https://github.com/coecms/nccompress</li> <li>https://github.com/pmav99/inspectds</li> </ul>","tags":["See-Also","Relevant-Projects","Other Tools"]},{"location":"references/see_also/#well-known-tools","title":"Well known tools","text":"<p>NetCDF (Network Common Data Form) and HDF (Hierarchical Data Format) are widely used formats for storing and handling large scientific data sets. There are several tools available for working with these formats, each offering different functionalities. from basic file manipulation to sophisticated data analysis and visualization.</p> <p>A list of some well-known tools :</p> Tool Description netCDF4 A Python interface to the netCDF C library for reading and writing netCDF files. HDFView A visual tool for browsing and editing HDF4 and HDF5 files. CO (NetCDF Operators) A suite of command-line tools designed for manipulating and analyzing NetCDF files. CDO (Climate Data Operators) A collection of command-line operators to manipulate and analyze climate and NWP model data stored in NetCDF or HDF formats. Panoply A cross-platform application that plots geo-gridded and other arrays from netCDF, HDF, and other datasets. ncview A visual browser for NetCDF format files. PyHDF A Python package for accessing HDF4 files. h5py A Python interface to the HDF5 binary data format. HDF-EOS A specialized version of HDF designed to support Earth Observation System data. ncdump/ncgen Utilities that come with the NetCDF library for displaying data and converting it to and from text format. Xarray A Python library for labeled, multi-dimensional arrays and datasets, commonly used with NetCDF data. NetCDF-Java A Java library for reading and writing NetCDF and other file formats. GDAL (Geospatial Data Abstraction Library) Supports reading and writing of HDF and netCDF data formats, commonly used for geospatial data processing. FERRET An interactive computer visualization and analysis environment designed to meet the needs of oceanographers and meteorologists analyzing large and complex gridded data sets. GrADS (Grid Analysis and Display System) A tool for easy access, manipulation, and visualization of earth science data stored in binary, GRIB, NetCDF, or HDF formats. UV-CDAT (Ultrascale Visualization Climate Data Analysis Tools) A powerful tool for climate data analysis and visualization that supports NetCDF and HDF among other formats. Thematic Real-time Environmental Distributed Data Services (THREDDS) A web server that provides metadata and data access for scientific datasets, using OPeNDAP, Web Coverage Service (WCS), and other remote data access protocols. MATLAB and Octave Both provide support for reading and writing NetCDF and HDF files. NCL (NCAR Command Language) A scripting language designed specifically for scientific data analysis and visualization, supporting HDF and NetCDF formats.","tags":["See-Also","Relevant-Projects","Other Tools"]},{"location":"tutorials/massive_json_kerchunking/","title":"Massive JSON Kerchunking","text":"<p>Experimental</p> <p>Following is an incomplete experiment, eventually worth revisiting.</p>","tags":["Tutorial","Kerchunk","Kerchunking","JSON","Experimental","Revisit-Me"]},{"location":"tutorials/massive_json_kerchunking/#overview","title":"Overview","text":"<ul> <li>Creating SARAH3 daily netCDF reference files can take \\(4+\\) hours</li> <li>optimizing chunking can reduce this</li> </ul>","tags":["Tutorial","Kerchunk","Kerchunking","JSON","Experimental","Revisit-Me"]},{"location":"tutorials/massive_json_kerchunking/#input","title":"Input","text":"<ul> <li> <p>Daily NetCDF files from 1999 to 2021   (actually missing a year, so normally it'd be up to 2022) that make up about about 1.2T</p> <ul> <li>Each daily NetCDF file contains 48 half-hourly maps of 2600 x 2600 pixels</li> <li>Noted here : mixed chunking shapes between years (e.g. <code>time, lat, lon</code> : <code>1 x 2600 x 2600</code>, <code>1 x 1300 x 1300</code>, and maybe more)</li> <li>rechunked to <code>1 x 32 x 32</code>, thus now 1.25T</li> </ul> </li> <li> <p>A first set of JSON reference files (one reference file per rechunked input NetCDF file) is about ~377G.</p> </li> <li> <p>A second step of (24 should be in total) yearly JSON reference files (based on the first reference set) is ~300G</p> </li> <li> <p>Finally, the goal is to create a single reference file to cover the complete time series</p> </li> </ul>","tags":["Tutorial","Kerchunk","Kerchunking","JSON","Experimental","Revisit-Me"]},{"location":"tutorials/massive_json_kerchunking/#hardware","title":"Hardware","text":"<pre><code>\u276f free -hm\n              total        used        free      shared  buff/cache   available\nMem:          503Gi       4.7Gi       495Gi       2.8Gi       3.1Gi       494Gi\nSwap:            0B          0B          0B\n</code></pre>","tags":["Tutorial","Kerchunk","Kerchunking","JSON","Experimental","Revisit-Me"]},{"location":"tutorials/massive_json_kerchunking/#trials","title":"Trials","text":"<pre><code>13G Nov  2 10:07 sarah3_sid_reference_1999.json\n13G Nov  2 09:58 sarah3_sid_reference_2000.json\n13G Nov  2 11:00 sarah3_sid_reference_2001.json\n13G Nov  2 11:08 sarah3_sid_reference_2002.json\n13G Nov  2 12:04 sarah3_sid_reference_2003.json\n13G Nov  2 12:12 sarah3_sid_reference_2004.json\n13G Nov  2 13:07 sarah3_sid_reference_2005.json\n13G Nov  2 14:29 sarah3_sid_reference_2006.json\n13G Nov  2 15:27 sarah3_sid_reference_2007.json\n13G Nov  2 16:45 sarah3_sid_reference_2008.json\n13G Nov  2 17:43 sarah3_sid_reference_2009.json\n13G Nov  2 19:02 sarah3_sid_reference_2010.json\n13G Nov  2 19:58 sarah3_sid_reference_2011.json\n13G Nov  2 21:25 sarah3_sid_reference_2012.json\n13G Nov  2 22:13 sarah3_sid_reference_2013.json\n13G Nov  2 23:43 sarah3_sid_reference_2014.json\n13G Nov  3 00:36 sarah3_sid_reference_2015.json\n13G Nov  3 02:03 sarah3_sid_reference_2016.json\n13G Nov  3 02:58 sarah3_sid_reference_2017.json\n13G Nov  3 04:24 sarah3_sid_reference_2018.json\n13G Nov  3 05:21 sarah3_sid_reference_2019.json\n13G Nov  3 06:48 sarah3_sid_reference_2020.json\n13G Nov  3 07:41 sarah3_sid_reference_2021.json\n</code></pre> <p>Trying to combine the above to a single reference set, fails with the following error message :</p> <pre><code>JSONDecodeError: Could not reserve memory block\n</code></pre>","tags":["Tutorial","Kerchunk","Kerchunking","JSON","Experimental","Revisit-Me"]},{"location":"tutorials/massive_json_kerchunking/#take-away-message","title":"Take away message","text":"<p>The limiting factor for the size of the reference sets is not the total number of bytes but the total number of references. Hence, the chunking scheme is perhaps more important here.</p>","tags":["Tutorial","Kerchunk","Kerchunking","JSON","Experimental","Revisit-Me"]},{"location":"tutorials/tutorials/","title":"Tutorials","text":"","tags":["Tutorials"]},{"location":"includes/tags/","title":"Tags","text":"<p>Tags in this documentation site : </p>"},{"location":"includes/tags/#asynchronous","title":"Asynchronous","text":"<ul> <li>Concepts</li> </ul>"},{"location":"includes/tags/#backend","title":"Backend","text":"<ul> <li>Concepts</li> </ul>"},{"location":"includes/tags/#background","title":"Background","text":"<ul> <li>Index</li> </ul>"},{"location":"includes/tags/#best-practices","title":"Best practices","text":"<ul> <li>Chunking &amp; Dask</li> <li>Xarray</li> </ul>"},{"location":"includes/tags/#cli","title":"CLI","text":"<ul> <li>To Do</li> <li>Index</li> <li>Index</li> <li>Inspect</li> <li>Command Line Interface</li> <li>Kerchunking</li> <li>Logging &amp; Verbosity</li> <li>Rechunk</li> <li>Select</li> <li>Shapes</li> <li>Suggest</li> <li>Index</li> <li>Chunk appropriately ?</li> <li>Help</li> <li>Inspect</li> <li>Kerchunk to JSON</li> <li>Kerchunk to Parquet</li> <li>Performance</li> <li>Rechunk</li> <li>Select</li> <li>Shapes</li> <li>Suggest</li> </ul>"},{"location":"includes/tags/#csv","title":"CSV","text":"<ul> <li>Select</li> </ul>"},{"location":"includes/tags/#caching","title":"Caching","text":"<ul> <li>Chunk appropriately ?</li> </ul>"},{"location":"includes/tags/#chunking","title":"Chunking","text":"<ul> <li>Chunk appropriately ?</li> <li>Index</li> <li>Chunking</li> <li>Concepts</li> <li>Chunking &amp; Dask</li> <li>Xarray</li> </ul>"},{"location":"includes/tags/#concepts","title":"Concepts","text":"<ul> <li>Concepts</li> </ul>"},{"location":"includes/tags/#concurrent","title":"Concurrent","text":"<ul> <li>Concepts</li> </ul>"},{"location":"includes/tags/#consolidation","title":"Consolidation","text":"<ul> <li>Concepts</li> </ul>"},{"location":"includes/tags/#context","title":"Context","text":"<ul> <li>Index</li> <li>Chunking</li> </ul>"},{"location":"includes/tags/#dask","title":"Dask","text":"<ul> <li>Chunking &amp; Dask</li> </ul>"},{"location":"includes/tags/#data","title":"Data","text":"<ul> <li>Index</li> </ul>"},{"location":"includes/tags/#debugging","title":"Debugging","text":"<ul> <li>Help</li> </ul>"},{"location":"includes/tags/#dependencies","title":"Dependencies","text":"<ul> <li>Index</li> </ul>"},{"location":"includes/tags/#design","title":"Design","text":"<ul> <li>To Do</li> </ul>"},{"location":"includes/tags/#development","title":"Development","text":"<ul> <li>To Do</li> </ul>"},{"location":"includes/tags/#diagnose","title":"Diagnose","text":"<ul> <li>Inspect</li> <li>Shapes</li> </ul>"},{"location":"includes/tags/#documentation","title":"Documentation","text":"<ul> <li>To Do</li> </ul>"},{"location":"includes/tags/#eupl-12","title":"EUPL-1.2","text":"<ul> <li>License</li> </ul>"},{"location":"includes/tags/#experimental","title":"Experimental","text":"<ul> <li>Suggest</li> <li>Massive JSON Kerchunking</li> </ul>"},{"location":"includes/tags/#frontend","title":"Frontend","text":"<ul> <li>Concepts</li> </ul>"},{"location":"includes/tags/#gnu-parallel","title":"GNU Parallel","text":"<ul> <li>Chunk appropriately ?</li> </ul>"},{"location":"includes/tags/#help","title":"Help","text":"<ul> <li>Help</li> </ul>"},{"location":"includes/tags/#how-to","title":"How-To","text":"<ul> <li>Index</li> <li>Chunk appropriately ?</li> <li>Help</li> <li>Inspect</li> <li>Kerchunk to JSON</li> <li>Kerchunk to Parquet</li> <li>Performance</li> <li>Rechunk</li> <li>Select</li> <li>Shapes</li> <li>Suggest</li> </ul>"},{"location":"includes/tags/#ideas","title":"Ideas","text":"<ul> <li>To Do</li> </ul>"},{"location":"includes/tags/#json","title":"JSON","text":"<ul> <li>Kerchunk to JSON</li> <li>Massive JSON Kerchunking</li> </ul>"},{"location":"includes/tags/#kerchunk","title":"Kerchunk","text":"<ul> <li>Kerchunk to JSON</li> <li>Kerchunk to Parquet</li> <li>Kerchunking</li> <li>Massive JSON Kerchunking</li> </ul>"},{"location":"includes/tags/#kerchunking","title":"Kerchunking","text":"<ul> <li>Massive JSON Kerchunking</li> </ul>"},{"location":"includes/tags/#license","title":"License","text":"<ul> <li>License</li> </ul>"},{"location":"includes/tags/#logging","title":"Logging","text":"<ul> <li>Logging &amp; Verbosity</li> <li>Help</li> </ul>"},{"location":"includes/tags/#metadata","title":"Metadata","text":"<ul> <li>Index</li> </ul>"},{"location":"includes/tags/#netcdf","title":"NetCDF","text":"<ul> <li>Chunk appropriately ?</li> <li>Rechunk</li> </ul>"},{"location":"includes/tags/#notes","title":"Notes","text":"<ul> <li>Notes</li> </ul>"},{"location":"includes/tags/#optimisation","title":"Optimisation","text":"<ul> <li>Xarray</li> </ul>"},{"location":"includes/tags/#other-tools","title":"Other Tools","text":"<ul> <li>See also</li> </ul>"},{"location":"includes/tags/#parallel","title":"Parallel","text":"<ul> <li>Concepts</li> </ul>"},{"location":"includes/tags/#parallel-processing","title":"Parallel processing","text":"<ul> <li>Chunk appropriately ?</li> </ul>"},{"location":"includes/tags/#parquet","title":"Parquet","text":"<ul> <li>Kerchunk to Parquet</li> </ul>"},{"location":"includes/tags/#performance","title":"Performance","text":"<ul> <li>Performance</li> <li>Xarray</li> </ul>"},{"location":"includes/tags/#programing","title":"Programing","text":"<ul> <li>To Do</li> </ul>"},{"location":"includes/tags/#python","title":"Python","text":"<ul> <li>Installation</li> </ul>"},{"location":"includes/tags/#rechunk","title":"Rechunk","text":"<ul> <li>Rechunk</li> <li>Rechunk</li> </ul>"},{"location":"includes/tags/#reference","title":"Reference","text":"<ul> <li>Index</li> <li>Inspect</li> <li>Command Line Interface</li> <li>Kerchunking</li> <li>Logging &amp; Verbosity</li> <li>Rechunk</li> <li>Select</li> <li>Shapes</li> <li>Suggest</li> </ul>"},{"location":"includes/tags/#relevant-projects","title":"Relevant-Projects","text":"<ul> <li>See also</li> </ul>"},{"location":"includes/tags/#revisit-me","title":"Revisit-Me","text":"<ul> <li>Massive JSON Kerchunking</li> </ul>"},{"location":"includes/tags/#sarah-3","title":"SARAH-3","text":"<ul> <li>Index</li> </ul>"},{"location":"includes/tags/#sarah3","title":"SARAH3","text":"<ul> <li>Kerchunk to JSON</li> <li>Kerchunk to Parquet</li> </ul>"},{"location":"includes/tags/#sis","title":"SIS","text":"<ul> <li>Kerchunk to JSON</li> <li>Kerchunk to Parquet</li> </ul>"},{"location":"includes/tags/#see-also","title":"See-Also","text":"<ul> <li>See also</li> </ul>"},{"location":"includes/tags/#serverless","title":"Serverless","text":"<ul> <li>Concepts</li> </ul>"},{"location":"includes/tags/#source-code","title":"Source Code","text":"<ul> <li>Index</li> </ul>"},{"location":"includes/tags/#time-series","title":"Time Series","text":""},{"location":"includes/tags/#to-do","title":"To Do","text":"<ul> <li>Kerchunk to Parquet</li> <li>Rechunk</li> <li>Suggest</li> <li>Index</li> <li>Chunking &amp; Dask</li> <li>Kerchunking</li> <li>Notes</li> <li>Xarray</li> </ul>"},{"location":"includes/tags/#tools","title":"Tools","text":"<ul> <li>Index</li> <li>Inspect</li> <li>Kerchunking</li> <li>Rechunk</li> <li>Select</li> <li>Shapes</li> <li>Suggest</li> </ul>"},{"location":"includes/tags/#tutorial","title":"Tutorial","text":"<ul> <li>Massive JSON Kerchunking</li> </ul>"},{"location":"includes/tags/#tutorials","title":"Tutorials","text":"<ul> <li>Tutorials</li> </ul>"},{"location":"includes/tags/#unsorted","title":"Unsorted","text":"<ul> <li>Notes</li> </ul>"},{"location":"includes/tags/#xarray","title":"Xarray","text":"<ul> <li>Xarray</li> </ul>"},{"location":"includes/tags/#direnv","title":"direnv","text":"<ul> <li>Installation</li> </ul>"},{"location":"includes/tags/#inspect","title":"inspect","text":"<ul> <li>Inspect</li> <li>Inspect</li> </ul>"},{"location":"includes/tags/#install","title":"install","text":"<ul> <li>Installation</li> </ul>"},{"location":"includes/tags/#nccopy","title":"nccopy","text":"<ul> <li>Chunk appropriately ?</li> <li>Rechunk</li> </ul>"},{"location":"includes/tags/#pip","title":"pip","text":"<ul> <li>Installation</li> </ul>"},{"location":"includes/tags/#read-performance","title":"read-performance","text":"<ul> <li>Kerchunk to Parquet</li> <li>Performance</li> </ul>"},{"location":"includes/tags/#rekx","title":"rekx","text":"<ul> <li>Index</li> <li>Inspect</li> <li>Command Line Interface</li> <li>Kerchunking</li> <li>Logging &amp; Verbosity</li> <li>Rechunk</li> <li>Select</li> <li>Shapes</li> <li>Suggest</li> <li>Chunk appropriately ?</li> <li>Help</li> <li>Inspect</li> <li>Kerchunk to JSON</li> <li>Kerchunk to Parquet</li> <li>Performance</li> <li>Rechunk</li> <li>Select</li> <li>Shapes</li> <li>Suggest</li> <li>Installation</li> </ul>"},{"location":"includes/tags/#select","title":"select","text":"<ul> <li>Select</li> <li>Select</li> </ul>"},{"location":"includes/tags/#shapes","title":"shapes","text":"<ul> <li>Shapes</li> <li>Shapes</li> </ul>"},{"location":"includes/tags/#statistics","title":"statistics","text":"<ul> <li>Select</li> </ul>"},{"location":"includes/tags/#suggest","title":"suggest","text":"<ul> <li>Suggest</li> </ul>"},{"location":"includes/tags/#virtual-environment","title":"virtual environment","text":"<ul> <li>Installation</li> </ul>"}]}